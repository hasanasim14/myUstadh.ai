## Chapter 7: Performance Assessment
From Chapter 7 of *Strategies Classroom Assessment for Student Learning: Doing It Right – Using It Well*, Second Edition. Jan Chappuis, Rick Stiggins, Steve Chappuis, Judith Arter. Copyright © 2012 by Pearson Education. All rights reserved.

*As we reflect on our most valued twenty-first century proficiencies, such as skill at collaboration, complex problem solving, or functioning effectively in the digital age, it's startling to notice how few can be translated into multiplechoice tests for evaluation. This is why methods such as performance assessment will be increasingly important as we look to the future of assessment in our schools.* 

*Performance assessment* , assessment based on observation and judgment, has been used for at least 3,000 years to evaluate mastery of essential competencies. To obtain a government position in ancient China, applicants had to demonstrate their skill at shooting with a bow and arrow and their proficiency at writing poetry. Although what we consider essential to measure may have changed over the centuries, performance assessment remains the same valuable process for collecting information about performances and products central to each academic discipline.

A performance assessment has two parts: the task to be completed by the respondent and the criteria for judging quality of the response. Students complete the task give a demonstration or create a product—and we evaluate it by judging the level of quality using a rubric.

Performance assessment is essentially subjective and can take more time to conduct and score than do the other methods we have studied so far. So we must ensure that our use of this form of assessment is as objective (free of bias and distortion) as possible and that we get maximum possible instructional value from time spent. Research and practice over the last several decades have strengthened both the accuracy and usefulness of our applications of this method. Specifically, this work has taught us how to create high-caliber rubrics for rating the quality of student work and how to help teachers learn to make consistent evaluations of those performances. It has enabled performance assessment to be used dependably outside the classroom as part of standardized, large-scale assessments. But more importantly, this work has made it possible for teachers to use performance assessments formatively to support learning. Good rubrics give us information we can use to differentiate instruction, direct students' attention to the features of work that constitute quality, allow us to give focused feedback, and enable students to self-assess.

As with any other method, performance assessment can be constructed and used well or poorly. In this chapter, we examine the characteristics of a good task, the characteristics of a good rubric, how to develop and evaluate each, and how to use performance assessment formatively, as assessment *for* learning. 1

*Even though it is called* performance *assessment, this method is used to judge both realtime performances, also called demonstrations, and products, or artifacts that students create.* 

### **Chapter 7 Learning Targets**

At the end of Chapter 7 , you will know how to do the following:

- Select, revise, and develop high-quality performance tasks.
- Select, revise, and develop high-quality rubrics.
- Use performance assessment formatively, as teaching tools.
- Structure performance assessments so that students can use the results to self-assess and set goals for further learning.

### **WHEN TO USE PERFORMANCE ASSESSMENT**

In Chapter 4 we advocated reserving performance assessment for those learning targets that really require it: skills, products, and some forms of reasoning. The most important determiner for using performance assessment is the nature of the learning target. For example, if the learning target states "Sets personal fitness goals," it is a *reasoning* target. The student could list personal fitness goals, which could be evaluated with a simple rubric or checklist, and we would consider this a written response assessment. If the learning target states "Creates a personal fitness plan," then the target calls for an *artifact* , a fitness plan, and it would be classified as a *product* target. The plan itself will be more extensive than a list of fitness goals—presumably it will include actions and timelines as well—and we would use a performance assessment to evaluate it because the task is more complex and the characteristics we are evaluating are also more complex than a list of personal fitness goals.

FIGURE 7.1   Keys to Quality Classroom Assessment 
![](_page_182_Figure_1.jpeg)

Two other conditions influence the selection of performance assessment. Use it when either or both of the following are true:

- You work with young primary students or students who cannot read or write in English. In this case, you will be using it to assess knowledge as well as reasoning, skill, and product targets.
- You can't get the information you need through written response assessment methodology.

### **ASSESSMENT DEVELOPMENT CYCLE FOR A PERFORMANCE ASSESSMENT**

Developing a performance assessment begins with the same four steps at the Planning Stage as do all other assessment methods: (1) determine who will use the assessment results and how they will use them; (2) identify the learning targets to be assessed; (3) verify that performance assessment is the best method for the purpose and the targets; and (4) attend to sampling issues. At the Development Stage, performance assessment requires four steps: (1) select, revise, or develop the task; (2) evaluate the task for quality; (3) select, revise, or develop the rubric; and (4) evaluate the rubric for quality. At the Use Stage, we follow the same two steps as with other methods: (1) conduct and score the assessment; and (2) revise as needed for future use ( Figure 7.2 ).

This is somewhat challenging work. So you should approach it from the perspective that, once developed, your performance assessments will be reusable (perhaps with ongoing improvements) well into the future. That is, don't think of this as oneshot development; this is not to use and toss. Take advantage of this valuable resource for as long as you can.

#### **Determining Users and Uses**

Again, we begin by answering these by now familiar questions: How do we want to use the information? Who else will use it? What decisions will they make? Typically,

 **FIGURE 7.2** Assessment Development Cycle for a Performance Assessment

*Planning*

- Determine who will use the assessment results and how they will use them.
- Identify the learning targets to be assessed.
- Select the appropriate assessment method or methods.
- Determine sample size.

*Development*

- Select or develop the task.
- Evaluate the task for quality.
- Select or develop the rubric.
- Evaluate the rubric for quality.

*Use*

- Conduct and score the assessment.
- Revise as needed for future use.

we will use performance assessment information for one or more of the following purposes:

- To plan instruction, as with a pretest
- To differentiate instruction, as with a mid-unit quiz
- To offer feedback to students so they can act on it during the learning
- To give students the opportunity to self-assess and set goals for further learning
- To measure level of achievement for a final grade, as with a post-test

Each of these purposes can be accomplished with performance assessment, as long as we keep the intended use in mind while making further planning and design decisions.

#### **Identifying Learning Targets**

At this step we simply list the learning target or targets the assessment is to measure. (If a target is complex or unclear, clarify it or deconstruct it first, following the processes outlined in Chapter 3 .)

#### **Selecting Assessment Method(s)**

Although we have already determined that we will use performance assessment, it's important to make sure we have identified targets that require it and also that the targets can be assessed accurately with this methodology. So review the list of learning targets with this in mind. (Refer to Chapter 4 for an in-depth discussion of which types of targets are best assessed with performance assessment.)

#### **Determining Sample Size**

As we have seen in previous chapters, the sampling challenge is gathering just enough evidence to make a relatively confident judgment of level of achievement. In the context of a performance assessment, sampling issues are addressed by the task. Does the task provide enough evidence to support a valid conclusion about level of mastery of the intended target or targets? The answer depends on a number of factors—the complexity of the learning target, the decision the evidence will inform, consistency of the student's performance, and proximity of the student's performance to an important cutoff mark. For each of these factors, we offer general guidelines, not rigid rules. Sampling well requires our professional judgment in the simultaneous weighing of all factors.

**COMPLEXITY OF THE TARGET.** The first sampling consideration is the complexity of the learning target to be assessed. If the target is a simple one, it may require fewer instances—a smaller sample—to judge level of achievement. If the target is a complex one, it may require more. For example, you may be able to judge a primary student's reading rate with one or two performances. This target is relatively narrow and sharply focused. However, if you want to judge proficiency at reading aloud with fluency, you may need several performances because the target is more complex—the definition of *reading fluency* often includes accuracy, phrasing, and expression. You may need more demonstrations to hear and evaluate all of the parts.

**DECISION THE EVIDENCE WILL INFORM.** How you intend to use the results also affects sampling decisions. If you will use the results in a low-stakes assessment *for* learning context, a smaller sample size will often do. If you are deciding how to regroup for instruction, one task may provide sufficient information to work from, because if the small sample size leads you to an incorrect judgment, you can easily see and correct it. In a high-stakes assessment *of* learning context, it is important to consider the consequences of an incorrect decision and enlarge the sample to minimize that possibility. If you are deciding, for example, who passes and who fails, you are going to want more rather than fewer samples to be sure you are making the correct inference about level of proficiency—an incorrect high-stakes decision is much harder to fix after the fact.

**CONSISTENCY OF PERFORMANCE.** A third key sampling variable, as described in Chapter 4 , is the consistency of the student's performance. If the student is clearly a master of the standard or clearly not, then we need few tasks to determine level of achievement. A student whose assessment results are consistently at one level of proficiency on a given learning target can with some confidence be said to be at that level. However, a student whose assessment results are all over the map for a given learning target does not engender the same level of confidence. If the student's performance fluctuates, it may be necessary to assign one or more subsequent tasks until we are able to make a stable estimate of level of achievement. Here is the rule of thumb: we have gathered enough evidence if we can guess with a degree of certainty how that student would do if we gave him one more task.

**PROXIMITY TO THE CUTOFF MARK.** In addition, when a student's level of achievement is very close to or on the dividing line between two levels of proficiency, such as very close to "meets the standard," or teetering between two grades, it is helpful to have more samples to justify assigning the level of grade. But when a student's performance is considerably above or below an important cutoff point, more data are not likely to change the outcome.

**FAQ 7.1 Difference Between Extended Written Response and Performance Assessment**

Question:

*If both extended written response and performance assessment are evaluated by rubrics, what is the difference between them?* 

Answer:

The differences lie in the nature of what is evaluated and what criteria are used to evaluate results. Written responses call for composition of original text. Performance assessments can call for an infinite array of products, performances, or demonstrations. Depending on the context, written responses can be evaluated using criteria reflective of content mastery or the quality of the reasoning used in formulating the answer. Performance assessments rely on criteria reflective of the quality of the particular product or performance called for by the context. It becomes difficult to differentiate written response and performance assessment when a composition is called for that is to be evaluated in terms of the form of the response; that is, the quality of the writing. We categorize that as a *product* target and assign it to the category of *performance assessment*.

### **SELECTING, REVISING, OR DEVELOPING THE TASK**

A performance task can be thought of as all the material you prepare to let students know what they are supposed to do to demonstrate their current level of proficiency. It can take one of several forms, depending on the learning target it is intended to assess: a physical demonstration of skill, a verbal presentation or dialogue, or the creation of a product. The purpose of the task is to elicit the correct demonstration or artifact from the student so that it can be assessed by the rubric.

Although much of the emphasis in performance assessment is on the rubric, the quality of the task must receive equal attention. Chances are you have had experience with a poorly designed task, either on the giving or receiving end. Here are just a few of the problems that can result:

- Student work doesn't provide evidence of the intended learning, even if the work itself is of high quality.
- Students aren't sure what to do and consequently don't produce what you expected or don't produce the level of quality they are capable of producing.
- You spend a great deal of time responding to "Is this what you want?" and "I don't get it" during task completion time.
- The task takes much longer to complete than expected.

**FIGURE 7.3** Characteristics of a Good Task
*Content of the Task* 

- Target Alignment: Aligns to the intended learning target and elicits the right performance or product
- Authenticity: Provides as realistic a context as possible
- Choice: If choice is offered, all options are equivalent
- Level of Scaffolding: Information points the way to success without "overhelping"
- Interference: Successful completion does not depend on unrelated skills or a specific linguistic or cultural background
- Availability of Resources: Necessary resources and materials are available to all

*Structure of the Task*

- Knowledge students are to use: What knowledge is to be applied?
- What students are to accomplish: What are students to do with the knowledge specified?
- Performance or product students are to create: What form will the finished performance or product take?
- Materials to be used: What materials should students use?
- Timeline for completion: How much time will students have?
- Conditions: If the task calls for a demonstration or performance, under what conditions will it take place?
- Help allowed: What assistance will be permitted? From whom?
- Criteria: What criteria will be the focus of the assessment?

*Sampling*

- Use of information: How many tasks will be assigned? Does this task sample adequately for the intended use?
- Coverage of the target: Does the breadth of the task or the number of tasks adequately cover the breadth of the target?

Figure 7.6 , the *Rubric for Tasks* , also found on the accompanying CD in the Chapter 7 file, can be used to evaluate tasks for each of these dimensions of quality.

- The resources necessary for completion turn out to be hard to acquire for some or all students.
- Students find it necessary to get outside help doing the work and therefore parts of the task are completed by well-meaning parents or other helpers.
- Students go into a tailspin and conclude they are not good at the subject.

In this section we will examine the characteristics of a good task—what needs to be in place to avoid these and other problems—by considering three dimensions: content, structure, and sampling. Figure 7.3 summarizes the criteria for a good task.

#### **The Content of the Task**

The ultimate goal of a performance task is to produce accurate evidence of the intended learning in a context that approximates realistic conditions, while avoiding sources of bias and distortion. Though the task need not be a complex assignment, it introduces a number of variables that must be managed thoughtfully for it to reflect student achievement accurately. Whether you are selecting, revising, or creating a task, the *content* of the task is the first aspect to consider. What learning will completion of this task demonstrate? What context will it be set in? Will the task allow for choice? Does the content set up a fair and unbiased assessment for all students?

**TARGET ALIGNMENT.** The requirements of the task should produce evidence of the learning target(s) it is intended to assess. This is known as *content validity* . For example, this writing task was intended to assess narrative writing: "You have all heard stories of how the elephant got its trunk or how the camel got its hump. Think of something in nature and make up a story about how it got that way." Instead, it elicited expository writing from a number of students and could not be used to judge their competence at narrative writing. If we don't elicit the right performance or product, we can't use the results as evidence of level of achievement.

In addition, requirements unrelated to the learning targets (e.g., neatness) should not be included. If you are going to require something and evaluate it, it should be part of the learning target you have been teaching to and students have been practicing.

**AUTHENTICITY.** The content of the task should provide as realistic a context as practical. It should model what application of the learning looks like in life beyond the classroom where possible. This helps students see the links among what they are learning, how they can use it, and why it might be valuable. This characteristic is often referred to as *authenticity* .

**CHOICE.** Student choice can increase student motivation. Yet, giving students a choice of tasks can cause the assessment to yield inaccurate results; with choice comes the chance that one or more options will include requirements unrelated to the learning target, or that they will represent different levels of difficulty. If the task allows students to select from a menu of tasks, all options should be equivalent. They should all provide evidence of achievement on the same learning targets at the same level of difficulty. If this requirement is not satisfied, students will select the option they feel most confident with and this can keep us (and them) from understanding and working on areas in need of improvement.

**LEVEL OF SCAFFOLDING.** The information in the task should be sufficient to let students know what they are to do without giving so much information that the task will no longer measure level of mastery of the intended learning target. The task should point the way to success without doing the thinking for the student.

**INTERFERENCE.** In addition, successful completion of the task cannot depend on unrelated skills, such as intensive reading in a task intended to evaluate a mathematics learning target or complex mathematical problem solving in a task intended to evaluate reading. Nor can the task depend on having had one particular cultural or linguistic background for successful completion. This can happen when a task uses a context such as sewing or baseball that is not familiar to some students. Including unrelated skills or a context relying on cultural or linguistic background can interfere with the accuracy of results for groups of students, due to conditions beyond their control.

**AVAILABILITY OF RESOURCES.** All resources and materials necessary for the completion of the task need to be available to all students. If, for example, some students have difficulty obtaining materials required for a display or cannot get to a library or an Internet connection for required research, their performance on the task will be compromised by factors other than the level of achievement the task was intended to assess. Again, these factors are out of some students' control.

#### **Structure of the Task**

After having thought through content considerations, we are ready to look at the structure of the task. Imagine students receive the following "Physics of Motion" science task to assess their comprehension of physics equations, understanding of the principles of experiment design, and ability to collect and explain experiment results:

Conduct an experiment to determine the percentage of vehicles that exceed the speed limit as they pass the school. Report your results.

Although the task represents a good portion of what students need to know to successfully complete it, if this is all the information they receive, many students may be unable to demonstrate their learning accurately. What kinds of information do students need in order to know what to do? What materials can or should they use? If the task calls for a demonstration or a performance, under what conditions will they be demonstrating or performing? What is the timeline for completion? How much and what kinds of help are allowed? How will success be judged?

**KNOWLEDGE STUDENTS ARE TO USE.** Good tasks remind students of the knowledge they are to use in completing them. Tasks that give students only partial content information cause teachers to spend an inordinate amount of time filling in the missing pieces. We cannot overstate the importance of avoiding this problem. When faced with incomplete information, some students conclude that because they don't understand what to do, the problem is with them and not the task. For the duration of the assignment, these students don't regain a sense of confidence that they can be successful, and some of them give up. Such tasks are not a fair measure of achievement.

Therefore, it is important to include in the task a statement of the knowledge to be applied. This reminds students of which part of all that they have been studying

they are to focus on. It might appear that this could go without saying, but many students are thinking about a number of different things besides our course at any given time, and it is helpful to give them a general cue to the knowledge they will apply in completing the task.

 In the "Physics of Motion" task, the knowledge students are to use is the knowledge of the physics of motion.

**WHAT STUDENTS ARE TO ACCOMPLISH.** Most tasks will ask students to reason or perform in some way with the knowledge specified. Often, as in the "Physics of Motion" task, this is the only information given.

 In this example, students are told to conduct an experiment to determine the percentage of vehicles that exceed the speed limit as they pass the school.

 If part of the learning to be assessed is ability to design an experiment, the task should include the instruction to design as well as conduct the experiment.

**PERFORMANCE OR PRODUCT STUDENTS ARE TO CREATE.** Most tasks include some version of describing the performance or product that will ultimately be evaluated, but the description may not include enough details. In the "Physics of Motion" task, students are told to report their results. Yet the intended learning targets relate to student ability to collect and explain experiment results. Without more information, students' reports will not likely include evidence of experimental design. Also, students could prepare a one-sentence summary and be at a loss as to how to turn it into a report.

For the "Physics of Motion" task, students would benefit from a statement such as "Write a report in which you explain your experimental design and share your results."

**MATERIALS TO BE USED.** If students are required to use certain materials, it is better to include this information on the written task than to rely on students to guess or remember what you said. However, if part of what you are assessing is the knowledge of what materials to use, then don't list them in the task.

 In the "Physics of Motion" task, it may be helpful to let students know they are to use a stopwatch and a measuring tape.

**TIMELINE FOR COMPLETION.** The time given for completion should be reasonable, given the task requirements. We often underestimate the amount of time a complex task will take, which can compromise the accuracy of its results and create stress for students and teachers alike. It is best to check this carefully before assigning a task. If the task is to be completed entirely in class, specify enough time so that almost all students can finish with no problem, and then offer options as needed for those who

 **FAQ 7.2 Individual or Group Work**
Question:

*To make tasks more like what occurs in the workplace, shouldn't students work in groups to complete them?* 

Answer:

In an attempt to make assessments as realistic as possible, teachers often have students work in a group. This is fine if your target is to determine how well students work in a group. But, if your target is to ascertain individual achievement, a group product or presentation will not yield accurate information. To get around this problem, some test developers recommend having students work in groups to make sure everyone is beginning the task with a similar knowledge base. Then have students create the product or perform the skill individually. In general, when assessing individual student reasoning proficiency, skill level, or product development capabilities, the task must be completed individually to give an accurate picture of each student's level of achievement.

don't. Or plan enough time for all to finish and have a followup activity available for those who finish early.

It is a good idea to include the timeline for completion in the task. If it has intermediate due dates, those should be included as well. Seeing the timeline in writing helps students get or stay organized and it helps you rethink whether the timeframe is reasonable.

In the "Physics of Motion" task, it might look like this: "You will have three days to design and conduct the experiment and two days to write your report. The report is due in one week."

**CONDITIONS.** If a task calls for a demonstration or performance, it should specify the conditions under which the demonstration or performance is to take place. For example, if students are to give a speech, you may want to specify the audience and the length of time required for the speech.

In the "Physics of Motion" task, there may be no need to specify conditions.

**HELP ALLOWED.** Accuracy of assessment requires that we assess each student's level of achievement and no one else's. For tasks accomplished in one day, you can control who does the work and what parts students get help with. When completion of a task will span more than one day, some students may have a great deal of parent help while others have none. If you want to increase the odds that the work is the student's alone, the task should explicitly state that as a requirement. If help is allowed, how much and what kind? If, for example the task requires a written product and you don't intend to evaluate it for spelling, capitalization, or punctuation, you could recommend that students use a peer or parent editor to help with that part.

In the "Physics of Motion" example, you might tell students that they can work with a partner for designing and conducting the experiment, but they will each have to submit a separate, individually written report.

**CRITERIA.** The task should include a reminder of the elements of quality you will use to assess it. You can either attach the rubric that you will use or remind students of the criteria that will be the focus of this assessment from a rubric you have already distributed.

In the "Physics of Motion" task, it might look like this: Your report will be scored on understanding of physics equations, experimental design, and collection and explanation of results. Rubrics are attached.

Students should have had prior experience with the standards of quality represented on your rubrics before being held accountable for demonstrating them. A process for familiarizing students with rubrics is explained later in this chapter.

So, if the "Physics of Motion" task had all of these components, it would look like this (see For Example 7.1 ): Using a stopwatch and a measuring tape, you are to use your knowledge of the physics of motion to design an experiment to determine the percentage of vehicles that exceed the speed limit as they pass the school. Then you are to conduct your experiment. Last, you are to write a report in which you explain your experimental design and share your results.

You will have three days to design and conduct your experiment and two days to write your report. The report is due in one week.

You may work with a partner or alone to design and conduct the experiment, but you must submit an independent report that you alone have written. Your report will be scored on understanding of physics equations, experimental design, and collection and explanation of results. Rubrics are attached.

#### **Sampling**

Last, in selecting, revising, or developing a task we check the sample size. The task should offer enough evidence to satisfy its intended purpose and to adequately represent the breadth of the learning target. If it is a summative assessment, you will be able to draw a confident conclusion about student mastery of the intended target(s). If it is a formative assessment, you and/or students will have enough evidence to guide further instruction and learning.

**TABLE-"Physics of Motion" Task**
| Knowledge students are to use    | Use your knowledge of the physics of motion.                       
| What students are to accomplish  | Design and conduct an experiment to determine the percentage of vehicles that exceed the speed limit as 
|                                  | they pass the school.      
| Performance or product to create | Write a report in which you explain your experimental design and share your results.           
| Materials to be used             | Use a stopwatch and a measuring tape.                                              
| Timeline for completion          | You will have three days to design and conduct the experiment and two days to write your report. The report is 
|                                  | due in one week.                             
| Conditions*                      | N/A      
| Help allowed                     | You can work with a partner or alone to design and conduct the experiment. You must each submit a separate 
|                                  | report that you alone have written.              
| Criteria                         | Your report will be scored on understanding of physics equations, experimental design, and collection and 
|                                  | explanation of results. Rubrics are attached.     |


**USE OF INFORMATION.** There are three ways that tasks can sample adequately for the intended use. The first is breadth of the task. This means the complexity and coverage of the task. Sometimes one sufficiently broad task can provide enough information for its intended use. The second is the number of tasks. For some purposes you may need one, while for others you may need several separate tasks to adequately sample for the intended use. The third is repeated instances of performance. In this case, you may be able to use one task and ask students to perform it as many times as needed to support the intended use of the information.

**COVERAGE OF TARGET.** The breadth of the task or the number of tasks should adequately cover the breadth of the target(s). Is the task broad enough to cover the important dimensions of the learning target? If the target is complex, you may need several tasks, each focusing on one part of the learning target. One way to gather samples that show student progress and achievement status is to have students collect work in a portfolio. If we carefully outline the types of entries along with the learning targets they represent, we can get a good sample. With a portfolio, we can gather evidence over time to make the sample adequately reflect the breadth and depth of the learning target being assessed. In Chapter 11 we describe how to assemble and use portfolios to accomplish this purpose.

#### **Creating Tasks to Elicit Good Writing**

In certain writing tasks, especially those in the English Language Arts classroom, students benefit from a more in-depth explanation of the knowledge they are to apply and the product they are to create. Writer Donald Murray tells us that "a principal cause of poor writing received from students is the assignment . . . they have to be well prepared so that the students know the purpose of the assignment and how to fulfill it. Far too many teachers blame the students for poor writing when the fault lies with the teacher's instructions—or lack of instructions" (Murray, 2004, p. 98 ). When a task is intended to measure writing learning targets, we can use a more explicit formula in its design, one that answers the questions that writers must ask to write well:

- What is my role?
- Who is my audience?
- What is the format?
- What is the topic?
- What is the purpose?

 These questions are represented by the acronym RAFTS and are illustrated in Figure 7.4 . (In the acronym, the "S" stands for "strong verb," which directs the student's attention to the purpose of the writing.)

**FIGURE 7.4** RAFTS Writing Task Design

**R ole:** Writers must imagine themselves as fulfilling specific roles—for example, as tour guides or scientists or critics—when they write.

**A udience:** Writers must always visualize their audiences clearly and consistently throughout the writing process. If they don't, the writing will fail.

**F ormat:** Writers must see clearly the format that the finished writing should have, whether brochure, memo, letter to the editor, or article in a magazine.

**T opic:** Writers have to select and narrow their topics to manageable proportions, given their audiences and formats.

**S trong verb:** Words like "cajole," "tempt," "discourage," when serving as definers of the predominant tone of a piece of writing, will guide writers in innumerable choices of words.

 *Source:* Reprinted from "Why Grade Student Writing?" by E. Smith, 1990, *Washington English Journal, 13* (1), p. 26 . Reprinted by permission.

**ROLE.** Sometimes we ask students simply to write as themselves, as students, but we can often increase their motivation and the relevance of a task if we ask them to assume a role. Think about these questions to devise student roles: Who might be writing about this topic? If it is a content-area topic (social studies, science, mathematics, health, art, and so on), who in the practice of this content area might be writing about this topic? What job might they have?

**AUDIENCE.** When we don't specify an audience, students are writing to us, their teachers, by default. Yet they don't often know how to envision their audience's needs when they write, even if it is us. For writers to make good decisions about what information to include, what terminology to use, and what tone to adopt, they need to think about who they are writing to. When we ask students to write thorough explanations, it is helpful if we specify an audience who is not familiar with the topic. If we are the audience, either stated or unstated, students often conclude that we know plenty about the topic already. It is hard to write about something to someone who knows more about it that you do, and that particular circumstance doesn't occur in life beyond school very often. In life beyond school, when we are writing to inform, generally, the audience doesn't have the same level of expertise as the writer does. So, in tasks calling for informational writing, consider specifying an audience who might need to know the information and who doesn't already know it.

**FORMAT.** This is a simple component of the task. If a beyond-school application of the writing would take the form of a report or an essay, then by all means specify that format. Decisions about format are driven by three considerations: audience, topic, and purpose. If our audience is primary students, our topic is insects, and our purpose is to inform, a report may not be the best format. We can convey interesting and important information about insects in an alphabet book, on a poster, in a sticker book, or on playing cards. If, on the other hand, our audience is a politician, our topic is water quality (as measured by the number and diversity of stream bugs found in water samples), and our purpose is to persuade the politician to take an action, a combination of a letter and a report will be more suited to the task.

**TOPIC.** We rarely leave this out of a task or assignment. However, even this aspect can cause student writing to be better or worse, depending on how it is crafted. When we specify the topic for students, we must exercise caution in how we state it. The question here is, are we going to narrow the topic for students or are we going to expect students to narrow it for themselves? If we have been studying the foundations of the U.S. economic system and we want students to write about the Industrial Revolution, we will have to narrow the topic considerably for them to handle it successfully. Or we can teach students how to narrow topics and let them determine the aspect they will focus on. Considerations in narrowing topics include who the

audience is and how much time the writer will have. Generally, the less time, the narrower the topic. We can write all about the topic of friendship if we have a year or two and want to produce a book, but if we have only a week or so, we may wish to write a simple set of instructions for how to be a friend.

**STRONG VERB.** This does not refer to strong verbs in the students' writing. Rather, in this context, *strong verb* refers to the verb we use in the task itself. What is the *purpose* for the writing? Most often writing tasks in school are set to accomplish one of three purposes—to narrate, to inform, or to persuade—and the forms of writing produced are often referred to as narrative, expository, and persuasive. In narrative writing, the purpose is to tell a story, either real (personal narrative or anecdote) or imagined (fictional narrative). In expository writing, the controlling purpose is to inform. In persuasive writing, we may have one of four purposes: to initiate thought, to change thought, to initiate action, or to change action. We may use both narrative and expository writing in service of persuasion, but the ultimate purpose for persuasive writing is to cause something different to happen. Figure 7.5 gives

**TABLE-Specifying the purpose in a Writing task: strong verbs**
| Purpose     | Sample Verbs and Phrases                         |
|-------------|--------------------------------------------------|
| To narrate  | Chronicle                                        |
|             | Depict                                           |
|             | Describe an experience                           |
|             | Give an account of                               |
|             | Recount                                          |
|             | Relate                                           |
|             | Set forth                                        |
|             | Tell the story of                                |
|             | Tell about a time when                           |
| To inform   | Brief                                             |
|             | Clarify                                           |
|             | Compare                                           |
|             | Define                                            |
|             | Discuss                                           |
|             | Describe                                          |
|             | Explain                                           |
|             | Familiarize                                       |
|             | Inform                                            |
|             | Teach                                             |
|             | Tell                                              |
|             | Update                                            |
| To persuade | Argue                                             |
|             | Challenge                                         |
|             | Compel                                            |
|             | Convert                                           |
|             | Convince                                          |
|             | Defend                                            |
|             | Enlist                                            |
|             | Exhort                                            |
|             | Impel                                             |
|             | Incite                                            |
|             | Induce                                            |
|             | Influence                                         |
|             | Inspire                                           |
|             | Justify                                           |
|             | Persuade                                          |
|             | Sway                                              |


**For Example 7.2 Using the RAFTS Formula**
**To Your Health**

Imagine that a fifth-grade teacher from the elementary school you attended has asked for your help. Because she knows that younger children look up to teenagers, she has asked you to help teach her students how healthy childhood habits lead to becoming healthy adults.

Your assignment:

In a report to be read by fifth graders, explain three or more habits they can establish now to help them become healthy adults.

In framing your report, consider the following questions:

- What are healthy childhood habits?
- What does good health involve beyond healthy eating habits?
- What should a child do, and what should a child avoid?

| Role     | Well-informed older student |
|----------|-----------------------------|
| Audience | Fifth graders               |
| Format   | Report                      |
| Topic    | Health habits               |
| Purpose  | To teach (inform)           |

Your report will be judged on the basis of the attached criteria.

examples of verbs that help students understand what kind of writing they are to produce.

For Example 7.2 shows how we might use these questions to plan the ingredients or a written task in a content area.

#### **Evaluating the Task for Quality**

We have developed the Rubric for Tasks to help you evaluate any performance task for the degree to which it meets the standards of quality on the three criteria described in this section: *Content* , *Structure of the Task* , and *Sampling* . The Rubric for Tasks, Figure 7.6 , also can be found in the Chapter 7 CD file.
**FIGURE 7.6 Rubrics for Tasks**
**TABLE-Content: What learning will the task demonstrate?**
| Indicator             |                                                                                                                   |
|-----------------------|-------------------------------------------------------------------------------------------------------------------|
| Target Alignment      | **Level 3: Ready to Use**                                                                                         |
|                       | All requirements of the task are directly aligned to the learning target(s) to be assessed.                       |
|                       | The task will elicit a performance that could be used to judge proficiency on the intended learning targets.      |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       | Some requirements of the task are not aligned to the learning target(s) to be assessed.                           |
|                       | There is extra work in this task not needed to assess the intended learning targets.                              |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       | Requirements of the task are not aligned to the learning target(s) to be assessed.                                |
|                       | The task will not elicit a performance that could be used to judge proficiency.                                   |
|                       |                                                                                                                   |
| Authenticity          | **Level 3: Ready to Use**                                                                                         | 
|                       | The task provides as realistic a context as possible, given the learning target and intended use of the results.  |
|                       | The conditions model application of the learning to a practical situation found in life beyond school.            |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       | The task provides an artificial context.                                                                          |
|                       | The conditions do not provide a clear link to application of learning to real-world situations.                   |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       | The task provides no context, or the context does not support application beyond school.                          |
|                       |                                                                                                                   |
| Choice                | **Level 3: Ready to Use**                                                                                         | 
|                       | All choices ask for the same performance or product, with similar difficulty, targeting the same learning goals.  |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       | Some choices may vary in difficulty or performance/product type or target different learning goals.               |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       | Choices vary greatly in performance type, difficulty level, or intended learning targets.                         |
|                       |                                                                                                                   |
| Interference          | **Level 3: Ready to Use**                                                                                         |
|                       | Task success does not depend on unrelated skills (e.g., intensive reading in a math task).                        |
|                       | Task is culturally robust—not dependent on one cultural or linguistic background.                                 |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       | Task may be slightly influenced by unrelated skills or cultural background.                                       |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       | Task depends on unrelated skills or a specific cultural background.                                               |
|                       |                                                                                                                   |
| Resources             | **Level 3: Ready to Use**                                                                                         | 
|                       |  All resources required to complete the task successfully are available to all students                           |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       | Some students may have difficulty obtaining resources needed to complete the task.                                |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       | Many students will have difficulty accessing necessary resources. successfully, or one                            |
|                       | or more of the resources required will be difficult for most students to obtain.                                  |

**TABLE-Information Provided: Are the directions and guidance given clear and sufficient?**

| Indicator             |                                                                                                                   |
|-----------------------|-------------------------------------------------------------------------------------------------------------------|
| Instructions          | **Level 3: Ready to Use**                                                                                         |
|                       | The instructions are clear and unambiguous.                                                                       |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  The instructions may leave room for erroneous interpretation of what is expected.                                |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The instructions may leave room for erroneous interpretation of what is expected.                                |
|                       |                                                                                                                   |
| Supplemental          | **Level 3: Ready to Use**                                                                                         |
| Information           |  The task includes the following information                                                                      |
|                       |     •    The knowledge students are to use in creating the task                                                   |
|                       |     •    The performance or product students are to create—what form it should take                               |
|                       |     •   The materials to be used, if any                                                                          |
|                       |     •   Timeline for completion                                                                                   |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  | 
|                       | Some of the following information is clear; some is unclear or missing:                                           |
|                       |     •    The knowledge students are to use in creating the task                                                   |
|                       |     •    The performance or product students are to create—what form it should take                               |
|                       |     •   The materials to be used, if any                                                                          |
|                       |     •   Timeline for completion                                                                                   | 
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The task does not include the following information:                                                             |
|                       |     •    The knowledge students are to use in creating the task                                                   |
|                       |     •    The performance or product students are to create—what form it should take                               |
|                       |     •   The materials to be used, if any                                                                          |
|                       |     •   Timeline for completion                                                                                   |
|                       |                                                                                                                   |
| Time Allowed          | **Level 3: Ready to Use**                                                                                         |
|                       |  The time allowed for the task is sufficient for successful completion.                                           | 
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  The time allowed is too long or too short, but either the timeline or the task can be adjusted                   |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The task will take considerably more time than is allowed and cannot be broken into shorter segments.            |
|                       |                                                                                                                   |
| Level of Scaffolding  | **Level 3: Ready to Use**                                                                                         |
|                       |  The task information is sufficient to let students know what they are to do without giving so much information   |
|                       |  that the task will no longer measure level of mastery of the intended learning target. The content points the way| 
|                       |  success without doing the thinking for the student.                                                              |
|                       |                                                                                                                   | 
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  Some parts of the task may give students too much help. In some places, the task does the thinking or the work   |
|                       |  for the student, compromising the results or the learning.                                                       |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |   The task is over-scaffolded. If used for summative purposes, the task cannot measure students’ ability to create| 
|                       |   product or performance independently, because the content is so explicit that students can follow it like a     |
|                       |   recipe. If used formatively, students can satisfactorily complete the task without having learned anything. The |
|                       |   task measures only students’ ability to follow directions.                                                      |
|                       |                                                                                                                   |
| Conditions            | **Level 3: Ready to Use**                                                                                         |
|                       |  If a task assesses a performance skill, it specifies the conditions under which the                              |
|                       |  performance or demonstration is to take place                                                                    | 
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  If a task assesses a performance skill, it does not sufficiently specify                                         |
|                       |  the conditions under which the performance or demonstration is to take place.                                    |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  If a task assesses a performance skill, it does not give any indication of the                                   |
|                       |  conditions under which the performance or demonstration is to take place.                                        |
|                       |                                                                                                                   |
| Help Allowed          | **Level 3: Ready to Use**                                                                                         |
|                       |  Multi-day tasks specify the help allowed.                                                                        |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  Although there is some reference to what kind of help is allowed for multi                                       |
|                       |  day tasks, it could be misinterpreted.                                                                           |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  Multi-day tasks do not specify the help allowed.                                                                 |
|                       |                                                                                                                   |
| Criteria              | **Level 3: Ready to Use**                                                                                         |
|                       |  The task includes a description of (or reference to) the criteria by which the performance or product will be    |
|                       |  judged.                                                                                                          | 
|                       |  Students are familiar with the criteria.                                                                         |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  Although described or referred to, the criteria by which the performance or                                      |
|                       |  product will be judged are vague or unclear (see  Rubric for Rubrics ).                                          |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The task includes no reference to the criteria by which the performance or product will be judged.               |
|                       |  or  The students are not familiar with the criteria to be used.                                                  | 

**TABLE-Sampling—Is there enough evidence?**

| Indicator             |                                                                                                                   |
|-----------------------|-------------------------------------------------------------------------------------------------------------------|
| Use of Information    | **Level 3: Ready to Use**                                                                                         |
|                       |  The breadth of the task or the number of tasks or repeated instances of                                          |
|                       |  performance is sufficient to support the intended use of the information.                                        |
|                       |                                                                                                                   |
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  The task is broader than needed to support the intended use of the information                                   | 
|                       |  There are more tasks or repeated instances of performance than are needed to support the intended use of the     |
|                       |  information.                                                                                                     | 
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The breadth of the task or the number of tasks or repeated instances of                                          |
|                       |  performance is not sufficient support the intended use of the information.                                       |
|                       |                                                                                                                   |
| Coverage of Target    | **Level 3: Ready to Use**                                                                                         |
|                       |  The breadth of the task or the number of tasks or repeated instances of                                          |
|                       |  performance is sufficient to cover the breadth of the intended learning target.                                  |
|                       |                                                                                                                   |                                 
|                       | **Level 2: Needs Some Revision**                                                                                  |
|                       |  The task is broader than needed to cover the breadth of the intended learning target.                            |
|                       |  There are more tasks or repeated instances of performance than are needed to cover the breadth of the intended   |
|                       |  learning target.                                                                                                 |
|                       |                                                                                                                   |
|                       | **Level 1: Completely Revise or Don’t Use**                                                                       |
|                       |  The breadth of the task or the number of tasks or repeated instances of                                          |
|                       |  performance is not sufficient to cover the breadth of the intended learning target.                              |
|-----------------------|-------------------------------------------------------------------------------------------------------------------|


### **SELECTING, REVISING, OR DEVELOPING RUBRICS**

In the context of performance assessment, rubrics represent the criteria for evaluating the quality of a reasoning process, a performance, or a product. You may be familiar with a variety of rubrics, some of which you may have seen used in large-scale assessments. Although rubrics used in large-scale assessments share many features of quality with rubrics designed for classroom use, those rubrics are generally designed to yield a quick, overall summary judgment of level of achievement. As a result, they don't tend to provide the level of descriptive detail about strengths and weaknesses in individual student work, and are therefore of limited usefulness in day-to-day instruction. To function effectively in the classroom, either formatively or summatively, rubrics must provide such detail.

A good classroom-level rubric serves multiple communicative and evaluative purposes:

- Defines quality
- Makes expectations clear and explicit for students
- Describes quality to parents
- Focuses teaching
- Guides interventions
- Promotes descriptive feedback to students
- Promotes student self-assessment and goal setting
- Tracks student achievement
- Makes judgments more objective, consistent, and accurate
- Improves grading consistency

If you search for rubrics on the Internet, you'll be confronted with hundreds of choices, some of which could accomplish all of the purposes listed here and many of which could not. How do you know which is which? Whether you plan to use an existing rubric or develop one of your own, it is helpful to understand the characteristics of rubrics that make them capable of serving all intended purposes. In this section, we'll introduce the terminology commonly used to describe rubrics and then examine the three dimensions of a high-quality rubric: *Content* , *Structure* , and *Descriptors* .

#### **Rubric Terminology**

You may have heard rubrics described as *holistic* or *analytic* . Those terms refer to how many scoring scales the rubric is comprised of—one or more than one. A *holistic* rubric has only one scale—all features of quality are considered together in determining a score. An *analytic* rubric has two or more scales—features of quality have been organized into separate categories and are rated separately from one another. These are sometimes known as *multi-trait rubrics* . The reasoning rubrics we examined in Chapter 6 are examples of holistic rubrics—one for each pattern of reasoning. The Oral Presentation Rubric shown in Figure 7.7 is an example of an analytic rubric.

The structure of each rubric takes the form of *criteria* , *indicators* , *levels* , and *descriptors* . See Figure 7.7 for illustrations of each of these terms.

**CRITERIA.** The categories of quality in an analytic rubric are known as *criteria* . Criteria represent key, independently varying dimensions of quality. Each criterion has its own rubric. You can teach each criterion separately, students can practice and receive feedback on each criterion separately, students can self-assess on each criterion separately, and you can assign grades to each criterion separately, if desired. Criteria are also sometimes called *traits.*

**INDICATORS.** Criteria for complex performances or products can be broken down further into subcategories called *indicators* . Indicators are the bulleted list of features assessed in each criterion. Occasionally indicators are represented on the rubric as subheads dividing the descriptors of quality. All criteria have indicators, but not all criteria call them out with structured subheads within a rubric.

**LEVELS.** *Levels* on a rubric are the points on a scale defining degrees of quality. They can be labeled with numbers (e.g., 1–5), phrases (e.g., "Just Beginning"; Halfway There"; "Success"), and/or symbols representing "basic" to "proficient" (e.g., parts of a hamburger, ice cream cone, or pizza). On an analytic rubric each criterion generally has the same number of levels.

**DESCRIPTORS.** *Descriptors* refer to the sentence or phrases representing each indicator at each level. Descriptors provide the details used to flesh out the indicators and differentiate the levels. In assessment *for* learning applications, the descriptors function as diagnosis and feedback about student strengths and weaknesses within the criterion.

#### **Content of the Rubric**

The *content* of the rubric defines the elements of quality essential to achieve the intended learning target. What does it assess? What are we looking for in a student's product or performance? What will "count?" We examine this characteristic first when selecting rubrics. If the rubric under consideration falls seriously short on content, there is no need to consider it further.

A good rubric defines the intended learning target by describing what is required to do it well. If the rubric misrepresents the intended learning, students will work toward producing evidence of something other than what is desired—what students see on the rubric is how they will define quality. To make sure a rubric's content is in good shape, we pay attention to two factors, target alignment and match to essential elements.

**TARGET ALIGNMENT.** Just as the task should align to the learning target(s) to be assessed, so should the rubric. The rubric's criteria and descriptors should not focus

**FIGURE 7.7** Structure of a Rubric

To illustrate the structure of a rubric, we will use an oral presentation rubric as our example.

**CRITERIA**

The oral presentation rubric has four *criteria* :

- **1.** Content
- **2.** Organization
    - **3.** Delivery
    - **4.** Language Use

**INDICATORS**

Each of the four criteria has several indicators:

- **1.** Content
    - Clear main topic
    - All information is important to understanding the topic
    - Facts, details, anecdotes, and/or examples make topic come alive for audience
- **2.** Organization
    - Opening introduces topic and catches audience's interest
    - Sequence of ideas supports meaning and is easy to follow
    - Transition words guide audience
    - Conclusion wraps up topic and leaves audience feeling satisfied
- **3.** Delivery
    - Maintains eye contact with audience throughout presentation
    - Voice is loud enough for audience to hear
    - Articulates clearly
    - Speaks at a pace that keeps audience engaged without racing
    - Avoids "filler" words ("and," "uh," "um," 'like," "you know")
    - Uses gestures and movement to enhance meaning
    - Uses notes only as reminders
    - Visual aids and props, if used, add to meaning
- **4.** Language Use
    - Chooses words and phrases to create a clear understanding of the message
    - Uses language techniques (e.g., humor, imagery, simile, and metaphor) effectively as appropriate to topic, purpose, and audience
    - Explains unfamiliar terminology, if used
    - Matches level of formality in language and tone to purpose and audience
    - Uses words and phrases accurately
    - Uses correct grammar

**LEVELS AND DESCRIPTORS**

Each criterion has a separate scoring scale, divided into *levels* . The oral presentation rubric has three levels. Each indicator for each criterion is fleshed out into one or more *descriptors* at each level. Here is what the rubric looks like for the criterion of *Content* . Each of the other three criteria also has a rubric organized the same way.

**ORAL PRESENTATION CRITERION 1: CONTENT**
**5:** Strong

- My presentation had a clear main topic.
- All of the information in my presentation related to and supported my topic.
- The information I included was important to understanding my topic.
- I chose facts, details, anecdotes, and/or examples to make my topic come alive for my audience.
**3:** Part-way There
- My topic was fairly broad, but the audience could tell where I was headed.
- Most of my details related to and supported my topic, but some might have been off-topic.
- Some of my information was important, but some details might have been too trivial to be included. Maybe I should have left some details out.
- Some of my information may not have been interesting or useful to my audience.
**1:** Just Beginning

- I wasn't sure what the focus of my presentation was, or I got mixed up and changed topics during my presentation. I think I wandered through a few topics.
- I didn't really know how to choose details to share, so I just used whatever came into my mind.
- I forgot to think about what information might be most interesting or useful to my audience.

*Source :* Adapted from *Seven Strategies of Assessment* for *Learning (p.* 194 ), by J. Chappuis, 2009, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

on features that do not contribute to doing well on the learning target. Sometimes rubrics stray in their focus, as when students are asked to produce a poster to demonstrate a reasoning learning target and the rubric includes features of the poster more closely related to direction following or art than to the reasoning target. "Home-

grown" rubrics, absent careful consideration of the intended learning, often suffer from this problem. If we are evaluating reasoning, the rubric should represent levels of quality for the reasoning target. Features unrelated to the learning targets should be left out or assessed separately for another purpose.

*Judge the performance (demonstration) or the product (artifact) only if the performance or product is specified in the learning target. If a performance or product is not called for, make sure the rubric measures the learning target.*

 **FAQ 7.3 Length of Rubrics**

Question:

*Aren't shorter rubrics better? Shouldn't the rubric fit on one page?* 

Answer:

Whether a rubric fits on one page depends on the use to which it will be put and the complexity of the learning target. For a rubric with more than one criterion, each criterion needs enough descriptive detail so that teachers can be consistent in judging student work and students can understand the strengths and/or weaknesses that underlie each score point.

With a lengthy rubric keep two points in mind. First, you don't have to evaluate all criteria for each piece of work. One of the strengths of multicriteria rubrics is that you can teach to, and assess, one aspect of quality at a time. Second, once students are familiar with a rubric, you can use a list of indicators for each criterion to remind them of the definition of quality, keeping the whole rubric handy for reference.

We would not advocate trading clarity for conciseness in either summative or formative applications.

**MATCH TO ESSENTIAL ELEMENTS.** The rubric's criteria and descriptors should also represent best thinking in the field about what it means to perform well on the intended learning target. Everything of importance for students at your level should be included. Three unfortunate things happen when important things are omitted: (1) we send the message that what is left out is unimportant; (2) we generate incomplete information on which to plan future instruction; and (3) we provide no feedback to students on the quality of valued elements.

By the same token, trivial features, those not important to success, *should* be left out. Some rubrics with a problem here require one way of demonstrating the learning, but the requirement may not be essential to demonstration of quality for the learning target. These rubrics inaccurately limit the definition of what it means to do well and penalize students who achieve the intended learning through a different path. If the feature is essential to the learning target, leave it in. If not, consider taking it out.

*Source:* Adapted with permission from *Creating and Recognizing Quality Rubrics* (p. 43 ), by J. A. Arter and J. Chappuis, 2006, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

**FIGURE 7.8** Characteristics of a Good Rubric

**Content of the Rubric** 

- Target Alignment: Focuses on features that contribute to doing well on the learning target.
- Focus on Essential Elements: Represents best thinking in the field about what it means to perform well on the intended learning target.

**Structure of the Rubric** 

- Number of Criteria: Sufficient to reflect the complexity of the learning target and its intended use.
- Independence of Criteria: If multiple criteria, they are independent of one another.
- Grouping of Descriptors: If multiple criteria, descriptors are grouped logically.
- Number of Levels: Fits the complexity of the target and intended use of the data.

**Descriptors in the Rubric** 

- Kind of Detail: Wording is descriptive of the work and can be used diagnostically in describing strengths and weaknesses.
- Content of Performance Levels: Levels of performance quality are parallel in content.
- Formative Usefulness: Language can function as effective feedback to the student and the teacher.

### **Structure of the Rubric**

*Structure* refers to how the rubric is organized: criteria are defined that represent important dimensions of quality. A good rubric organizes the criteria and its associated descriptors in ways that make it possible for the user to create an accurate picture of strengths and weaknesses. Good clear structure contributes to ease of use. To maximize a rubric's structure, we pay attention to four factors: number of criteria, independence of criteria, grouping of descriptors, and number of levels.

**NUMBER OF CRITERIA.** The number of criteria should be sufficient to reflect the complexity of the learning target and its intended use. If the rubric is holistic, the single scale needs to sufficiently represent all important parts of the target in one scale. If the target is complex, the rubric needs to include whatever number of criteria is needed appropriately define all important categories of proficiency.

**INDEPENDENCE OF CRITERIA.** If there are multiple criteria, they should be independent of one another. The same or similar descriptors should appear in only one criterion. When the same feature is rated in more than one criterion, it may indicate that the criteria aren't separable. If they are separable, they can be rated independently and each feature should appear in only one criterion.

**GROUPING OF DESCRIPTORS.** If there are multiple criteria, all descriptors should fit under the criterion they are assigned to. In other words, the categories defined by the criteria should suit the descriptors contained within them. Grouping of descriptors is a classification challenge. If descriptors don't fit where they're placed, they should be moved, or the categories should be redefined.

**NUMBER OF LEVELS.** The number of levels of proficiency defined within each criterion should fit the complexity of the target and intended use of the data as well. The levels should be useful in diagnosing student strengths and next steps: there should be enough of them to reflect typical stages of student understanding, capabilities, or progress. However, there should not be so many levels that it is difficult or impossible to define each or to distinguish among them.

When a scoring guide is comprised of several categories, each of which is assigned a number of points, it is sometimes referred to as a scoring rubric. In truth, it is often a list of criteria with few or no descriptors and lots of levels. Here is an example:

Ideas and Content: 10 points Organization: 20 points Word Choice and Sentence Structure: 10 points Conventions: 20 points

When a scoring guide looks like this, it's not a rubric because it's not a description of levels of quality. It is just a way to assign points, and not a very reliable one. If there are 20 points possible for Organization, there are 20 levels of quality with no guidance on how to differentiate among them—what's the difference between a 13 and a 14?

#### **Descriptors in the Rubric**

The *descriptors* are the "goes-unders"—the detail that fleshes out each level. A highquality rubric includes descriptors that accurately represent the criteria, are complete, and are clear enough so that teachers and students are likely to interpret them the same way. To evaluate a rubric's descriptors we pay attention to three factors: kind of detail, content of the levels, and formative usefulness.

**KIND OF DETAIL.** The wording should be descriptive of the work. To be used formatively, a rubric's descriptors should be helpful in defining levels in ways that diagnose student strengths and weaknesses. Evaluative language should not be used in the descriptors. When a rubric uses only these kinds of evaluative terms to differentiate levels, it offers no insight into why something is good or not; it just repeats the judgment of the level.

Also, if counting the number or frequency of something is included in a descriptor, we need to ensure that changes in such counts *are* indicators of changes in quality. Under

**FAQ 7.4 Number of Levels**

Question:

*Shouldn't we always use an even number of levels? With an odd number, won't it be too easy to gravitate toward the middle?* 

Answer:

The number of levels depends on the learning target being assessed and the intended use of the information the rubric yields. Raters can easily be trained to avoid the problem of over-assigning the middle score.

Some simpler learning targets can truly only be divided into three levels of proficiency, so it makes sense to have only three levels. Others, such as the criterion *Content* in the Oral Presentation example in Figure 7.7 , can be divided into five levels. This rubric has three defined levels, but it is easy to see that a student's performance might contain some descriptors from Level 5 and some descriptors for Level 3, or some descriptors from Level 1 and some from Level 3. With such rubrics, when a performance falls between two score points, highlight the phrases that describe it from each of the defined levels and then assign it the intermediate score, e.g., 4 or 2.

*Source:* Adapted with permission from *Creating and Recognizing Quality Rubrics* (p. 36 ), by J. A. Arter and J. Chappuis, 2006, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

the guise of increasing the objectivity of scoring guides, it is tempting to count things the number of sentences, the number of pieces of information, the number of topics covered, and so on. But this backfires when quality is not primarily defined by quantity. See For Example 7.3 for a sample of descriptive, evaluative, and quantitative language.

**CONTENT OF LEVELS.** The levels of the rubric should be parallel in their references to keys to quality. If an indicator of quality is referred to in one level, it should be mentioned in all levels. If an indicator is missing at one or more levels, there should be a logical rationale. For example, in a writing rubric, if "focus" is described at the "strong" level, it should also be described at all of the other levels. Or in a mathematics problem-solving rubric, one indicator may be "reasonableness of the solution." In this case, even though you may have a three-level rubric, you may only describe reasonableness of the solution at the "strong" and "weak" levels, because the solution is either within a reasonable range or it isn't.

**Descriptive, Evaluative, and Quantitative Language**

One criterion of a rubric for a science report may include descriptors for display of information. These three examples show the differences among descriptive language, evaluative language, and quantitative language. We recommend whenever possible to use descriptive language.

**Descriptive language:**

- **4:** Display of information is accurate, complete, and organized so that it is easy to interpret.
- **3:** Display of information is accurate, mostly complete, and is mostly organized so that it is easy to interpret. It may have one or two small omissions.
- **2:** Display of information is partially accurate, partially complete, and may have some organization problems.
- **1:** Display of information is inaccurate, incomplete, and not well organized.

**Evaluative language:**

- **4:** Excellent display of information
- **3:** Good display of information
- **2:** Fair display of information
- **1:** Poor display of information

**Quantitative language:**

- **4:** Displays four pieces of information
- **3:** Displays three pieces of information
- **2:** Displays two pieces of information
- **1:** Displays one piece of information

**FORMATIVE USEFULNESS.** If the rubric is intended for formative use, its levels and descriptors should function as effective feedback to the student and the teacher, leading to clear conclusions about strengths and areas needing work that provide sufficient detail to guide further learning. Students should be able to use the ingredients of the rubric to self-assess, to revise their own work, and to plan their own next steps in learning. They should be able to use the criteria, levels, and descriptors to

*Source:* Adapted from *Seven Strategies of Assessment* for *Learning* (p. 39 ), by J. Chappuis, 2009, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

 **FAQ 7.5 Including Task-specific Requirements in the Rubric** 

Question:

*The tasks I assign have very specific requirements and I include them in the rubric to make sure students do them and get credit for them. Can I still do this?* 

Answer:

It depends. If the requirements relate directly to a definition of quality regarding the learning target, they may legitimately belong in the rubric. However, if the requirements have to do with following directions—e.g., includes three characters, has five sentences—unless they are integral to the successful demonstration of the intended learning, they don't belong in the rubric and may not belong in the task.

offer one another feedback. Teachers should be able to use the rubric to determine what to teach next, to identify needs for differentiated instruction, or to identify topics for whole-group reteaching. All of the student uses require a version of the rubric in language they can understand—a student-friendly rubric.

#### **Process for Developing Rubrics**

Often existing rubrics can be revised to meet standards of quality. However, when a survey of available rubrics yields no promising candidates, you will need to start from scratch. Because you will want to use the rubric both formatively and summatively in the classroom, we recommend that you develop *general* rather than *task- specific* rubrics.

The process of rubric development involves collecting samples of existing rubrics, brainstorming features of quality, creating a draft, examining and scoring student work with the draft, and revising it. We have organized this path into six steps which we strongly recommend be carried out by teachers *working as a team* ( Figure 7.9 ):

- **1.** Establish a knowledge base.
- **2.** Gather samples of student performances or products.
- **3.** Sort student work by level of quality.
- **4.** Cluster the descriptors into traits.
- **5.** Identify samples that illustrate each level.
- **6.** Test the rubric and revise as needed.

##### **STEP 1: ESTABLISH A KNOWLEDGE BASE.** 
To create a rubric that meets standards of quality, we have to be clear ourselves about what the performance or product looks like when it is done well. If you're an expert in creating the product or performance

**FIGURE 7.9** Steps in Rubric Development

- **1.** Establish a knowledge base.
- **2.** Gather samples of student performances or products.
- **3.** Sort student work by level of quality.
- **4.** Cluster the descriptors into traits.
- **5.** Identify samples that illustrate each level.
- **6.** Test the rubric and revise as needed.

yourself, you may be able to work alone, but we recommend going through this process with a team as mentioned previously. A rubric development team should include some level of expertise. If you are not an expert at it, make sure you are working with someone who has experience with creating the performance or product.

As a team, begin by listing what you believe to be the characteristics of a highquality performance or product, as called for by the learning target the rubric is intended to assess.

Next, collect as many existing rubrics as you can. These documents may provide you with inspiration and rubric language. Review the rubrics and add to your list any characteristics that you believe should be added.

##### **STEP 2: GATHER SAMPLES OF STUDENT PERFORMANCES OR PRODUCTS.** 
Gather a range of student performances or products that illustrate different levels of quality on the intended learning target. Good sources include your own students' work, your colleagues' students' work, books on teaching your subject, your State Department of Education's website, and other Internet sites. If the learning target requires a performance, this will require gathering audio or videotaped examples.

In general, try to gather at least 20 samples representing more than one topic or task. Using samples from only one topic or task may lead you to develop a rubric that is too task-specific to be useful across tasks. A variety of samples helps ensure that all important criteria are included on the final rubric.

*A note of caution* : If the samples come from your own school, once you are finished with this process, don't publish them as "anchors" for the rubric. Any published anchors require permission from the student and more anonymity than can be guaranteed in one school.

##### **STEP 3: SORT STUDENT WORK BY LEVEL OF QUALITY.** 
Begin by examining the samples of student work and sorting them into three stacks representing your evaluation of them as Strong, Medium, or Weak. Write down your reasons for placing each sample in each stack as you go. Have each member of the team do this independently. The goal of sorting is not to get every sample in exactly the correct stack. The goal is to develop a complete list of the *reasons why* a sample should be placed in a particular stack. Eventually, the rubric you develop will be capable of judging samples accurately and consistently, but for now, focus on being descriptive in your reasons.

For each sample, write down exactly what you are saying to yourself as you place it into the stack. Don't wait until you have the samples sorted—it's harder later to remember all of the details of your reasons. Include as much detail as you can. Dig below general statements to a description of the evidence that leads to the general conclusion. For example, "logical" might be on your list of desired characteristics, but that is a general statement and does not describe the features present that lead you to determine whether or not a solution or argument is logical.

As another example, if "fluency" is on your list, students may not understand "lacks fluency," but they will understand "speaks slowly with hesitation." To generate detail, ask yourself questions such as these: "What specific features made me judge that the speech lacks fluency?" "What am I saying to myself as I categorize the performance?" "What descriptive feedback might I give to this student?" If you want the rubric to provide descriptive feedback and to function as a self-assessment and goal-setting tool for students, it is to your advantage to include those descriptive phrases from the outset. The descriptive phrases that you create now will form the core of your rubric descriptors.

Then, as a team, compile all of your descriptions of the samples in each stack. Ask yourself, "What makes the Strong stack different from the Middle stack? What makes it different from the Weak stack?" Use the samples to create a list of descriptors of quality at each level. We recommend trying to come up with as broad and long a list as possible.

As people sort samples, they sometimes discover that three levels is too few. Their eye develops to the point that they find work that is between two of the levels and they want to have four to six stacks. This is the beginning of determining your final number of levels. If three levels seem adequate, that's your number. If you can distinguish more than three independent levels, sort student work into the number of stacks you think you need and create the list of descriptors for each level. That is fine as long as you can find descriptors and/or sample performances that differentiate the levels. As long as you and students can differentiate performance levels, identify however many performance levels you need.

##### **STEP 4: CLUSTER THE DESCRIPTORS INTO TRAITS.** 
Your sorting and describing will result in a hodgepodge of descriptors at each level of performance. Some descriptors will be closely linked and can be assigned to a category: someone will say "Wait a minute. We have a whole lot of statements that refer to fluency. Why not group them together?" Some descriptors will overlap: you may hear, "Wait a minute. 'Speaks in paragraphs' is the same as 'Combines several sentences.' Why not delete one?"

 **FAQ 7.6 Student-developed Rubrics**

Question:

*To maximize student motivation, shouldn't we develop our rubrics with them?* 

Answer:

Involving students in developing criteria has several advantages and some drawbacks. It helps them internalize the standards of quality and it helps us refine our thinking. But most often we as teachers know more about the criteria for quality than students do, so developing criteria with students requires a robust sample set to help them refine their vision. For example, if students come up with criteria along the lines of "three colors" for a quality poster, we need to be prepared to broaden their thinking by showing them examples of effective posters with fewer (or more) than three colors. If they say that work has to be two pages long, we need to be ready to show them effective work that is only one page long, or that is six pages long.

A teacher should not begin instruction aimed at helping students perform well on a performance assessment without a clear vision of what the rubric should contain. The role of instruction is to bring students to a sufficient understanding of the keys to quality to be able to perform well. If you want to develop criteria with students, be prepared to assist them in their discovery of good criteria through use of thoughtfully chosen examples. Leading students through a process of devising a student-friendly version of the already-created rubric is one excellent instructional strategy. But the original articulation of keys to quality is the responsibility of the teacher.

Other descriptors may need to be separated into two categories: "I had trouble placing a student performance in a single category because it was strong in fluency but weak in pronunciation. Let's score those two dimensions separately."

This is the beginning of an analytical structure for the rubric—when you can sort out broad, independently varying categories of strengths and weaknesses, these indicate separate criteria. Once you have drafted the categories, it's time to refine them. You might decide that two criteria really refer to the same thing, or that one criterion should be divided into two or more criteria because of their independence from one another and the importance of being able to assess each separately. Most rubrics go through several stages of criteria definition and organization.

##### **STEP 5: IDENTIFY SAMPLES THAT ILLUSTRATE EACH LEVEL.** 
Return to the samples categorized as Strong, Middle, and Weak and select examples that illustrate well what is meant by each trait at each achievement level. These samples—also called *models, exemplars, examples,* and *anchors* —help teachers attain consistency with each other and within their own scoring across time, students, and assignments. Samples also help students understand what each achievement level looks like in concrete terms. Be sure to have more than one sample to illustrate each level. If you show students only one example of good performance, all performances might come out looking the same. Rather, show several performances that illustrate each level and trait.

Here are rules of thumb for selecting samples to illustrate criteria and levels:

- **1.** *Start with the extremes* . Identify what you consider to be classic examples of strong and weak performances or products—ones that match a good number of the descriptors in the highest and lowest categories. Choose samples that everyone on the development team can agree on. When students are first learning to analyze samples for quality, they need examples that are fairly straightforward. Leave the more ambiguous examples for later, when students have more fully developed their sense of quality.
- **2.** *Find examples for the middle level(s).* If you are using an odd number of levels, find samples that represent the approximate midpoint in the progression to excellence for each criterion. These samples will demonstrate the partial understanding or developing proficiency that is described by the phrases attached to your Middle level. If you are using an even number of levels, you will need to find two or four sets of midrange examples. For example, if your rubric has four levels, you will select samples for each criterion that typify the descriptors at level 2 (weaker, rather than stronger) and samples for each criterion that typify the descriptors at level 3 (stronger, rather than weaker).
- **3.** *Find several different examples across assignments that illustrate each level.* The purpose of the samples is to help with training raters to apply the criteria consistently, whether those raters are teachers or students. Teachers and students need to be able to apply the rubric across tasks, so the samples should help in learning how to do that.
- **4.** *Keep your eye out for examples illustrating typical problems* . Select examples that illustrate common errors students make, misconceptions they are likely to have, and flaws in reasoning. Carefully selected examples of typical problems can function well as teaching tools if students will be using them to practice scoring with the rubric.

The process of finding examples of performances or products at each level for each criterion usually results in revisions to the descriptors and criteria.

##### **STEP 6: TEST THE RUBRIC AND REVISE AS NEEDED.** 
Now is the time to test the rubric and note how you might improve it. Score student samples with your draft rubric and ask students to score anonymous samples as well. Unless you're spectacularly good at rubric development or spectacularly lucky, you'll identify some combination of the following problems:

- **1.** Some student performances or products include features not mentioned in the rubric. If the features are indeed salient to quality, add descriptors and perhaps, indicators. Especially try to add descriptors that clarify general features or concepts. Don't despair that your rubric is becoming unwieldy. It's part of the revision process: we expand text to include all possible options before paring it down to the most-needed elements.
- **2.** Features of student work seem to be rated in more than one criterion. Note this when it occurs. It might be that some descriptors are repeated across criteria. You will need to decide in which criterion the feature fits best. In some cases you may need to merge two criteria into one because they overlap to the extent that they can't be scored separately.
- **3.** Criteria seem too broad. Sometimes there are a number of indicators in a criterion that could be separated out to form two or more separate criteria. Do this if you would like to teach, diagnose, and assess the parts separately. Make sure that the new criteria structure does represent independently varying features of quality.
- **4.** The content of some levels is not parallel. You find that some descriptors at one level are not represented at other levels. In this case, write the descriptors for the other levels.

#### **Evaluating the Rubric for Quality**

We have developed the Rubric for Rubrics to help you evaluate any performance rubric for the degree to which it meets the standards of quality on the three criteria described in this section: *Content* , *Structure* , and *Descriptors* . The Rubric for Rubrics, Figure 7.10 , can also be found in the Chapter 7 CD file.

### **USE STAGE**

At this stage we conduct and score the assessment and revise it as needed for future use. As we noted previously, problems can still crop up even with the best planning. It is a good idea to keep notes of any potential sources of mismeasurement that may have compromised students' ability to show what they know and can do. If something appears to have gone awry and you can't identify the problem, use the Rubric for Tasks and the Rubric for Rubrics to troubleshoot the performance assessment.

![](_page_218_Picture_1.jpeg)
![](_page_219_Picture_1.jpeg)
![](_page_220_Picture_1.jpeg)
![](_page_221_Picture_1.jpeg)

### **SEVEN STRATEGIES FOR USING RUBRICS AS INSTRUCTIONAL TOOLS IN THE CLASSROOM**

You're all set with a great task and rubric. But how do you get students to understand and internalize your standards of quality? Performance assessment is a prime context for using assessment to help students learn. The instructional power here resides in using high-quality performance criteria to help students answer the three questions introduced in Chapter 2 to define assessment *for* learning: "Where am I going?"; "Where am I now?"; and "How can I close the gap?" Try out these seven strategies for using a rubric as a teaching tool to help students become competent, confident self-assessors and improve their performance in any subject.

#### **Where Am I Going?**

##### **STRATEGY 1: PROVIDE STUDENTS WITH A CLEAR AND UNDERSTANDABLE VISION OF THE LEARNING TARGET.** 
Motivation and achievement both increase when instruction is guided by clearly defined learning targets. Activities that help students answer the questions, "What's the learning? What am I responsible for accomplishing?" set the stage for all further formative assessment actions.

*In the context of performance assessment:*

 Teach students the concepts underpinning quality in your scoring rubric by asking them what they already know ("What makes a good _____________?"), then show how their prior knowledge links to your definition of quality.

*Rationale:*

 Showing the connection between new information and knowledge students already have helps it all make sense and provides a link to long-term memory. It also lays the foundation for students understanding upcoming learning.

##### **STRATEGY 2: USE EXAMPLES AND MODELS OF STRONG AND WEAK WORK.** 
Carefully chosen examples of the range of quality can create and refine students' understanding of the learning goal by helping students answer the questions, "What defines quality work? What are some problems to avoid?"

*In the context of performance assessment:*

- Use models of strong and weak work.
- Share anonymous strong and weak student work. Have students use the scoring rubric to evaluate the samples, then share their reasons, using the language of the scoring rubric.
- Share published strong (and weak, if available) work. Let students comment on the quality of published examples and your own work, using the language of the scoring rubric.
- Share your own work. Model the "messy underside" of creating the performance or product for students.

*Rationale:*

 Student performances improve when they understand the meaning of quality. This strategy teaches students to distinguish between strong and weak products or performances, and to articulate the differences. It also encourages teachers to share different aspects of the beauty of their discipline. What does it look/sound/feel like when it's done especially well? Modeling the messy underside for students reassures them that high-quality work doesn't always start out looking like high-quality work. As teachers, we tend to smooth over this part, so when the going gets messy for students, they may infer that they are "doing it wrong." What does high-quality work look like at its beginning stages? Model it.

#### **Where Am I Now?**

##### **STRATEGY 3: OFFER REGULAR DESCRIPTIVE FEEDBACK.** 
Effective feedback shows students where they are on their path to attaining the intended learning. It answers for students the questions, "What are my strengths?" "What do I need to work on?" "Where did I go wrong and what can I do about it?"

 *In the context of performance assessment:* 

 If students have become familiar with the language of the rubric, we can use that language as the basis for descriptive feedback. If we are focusing on one trait at a time, we only need give descriptive feedback on that one trait. This has the effect of narrowing the scope of work for both the teacher and the student. With struggling students, we can show them they do indeed know some things and we can limit the things they need to work on at one time to less daunting, more manageable number. Our feedback may be verbal, such as that given in a brief student-teacher conference, or we may choose to offer written feedback.

*Rationale:*

 Students need descriptive feedback while they're learning. It tells them how close they are to reaching the target and it models the kind of thinking we want them to be able to do, ultimately, when self-assessing.

##### **STRATEGY 4: TEACH STUDENTS TO SELF-ASSESS AND SET GOALS.** 
Strategy 4 teaches students to identify their strengths and weaknesses and to set goals for further learning. It helps them answer the questions, "What am I good at?"; "What do I need to work on?"; and "What should I do next?"

 *In the context of performance assessment:* 

 Strategy 4 includes anything students do to identify where they are with respect to mastery of the desired learning and to set goals for improvement. Black and Wiliam (1998a) assert that for assessment to bring about learning gains, it has to include student self-assessment: "self-assessment by the students is not an interesting option or luxury; it has to be seen as essential" (pp. 54 – 55 ). In performance assessments, you can ask students to use the rubric to identify their own strengths and areas for improvement. If you have given them descriptive feedback based on the rubric, you have modeled for them the kind of thinking they are to do when selfassessing. You can teach them to set specific, achievement-related goals and to make a plan to accomplish them.

 *Rationale:* 

 Periodic articulation about their understanding of quality and about their own strengths and weaknesses is essential to students' ability to improve.

#### **How Can I Close the Gap?**

##### **STRATEGY 5: DESIGN LESSONS TO FOCUS ON ONE LEARNING TARGET OR ASPECT OF QUALITY AT A TIME.** 
When assessment information identifies a need, we can adjust instruction to target that need. In this strategy, we scaffold learning by narrowing the focus of a lesson to help students master a specific learning goal or to address specific misconceptions or problems.

 *In the context of performance assessment:* 

 Students who are not yet proficient at creating a complex performance or product have a difficult time improving simultaneously on all elements of quality. This strategy suggests that you may want to teach lessons that address your rubric one criterion at a time. In some instances, you may need to focus on only one *part* of one criterion at a time. For example, a writing rubric might have a criterion called *Organization.* Within that criterion are descriptors about the quality of the introduction, the sequencing of ideas, transitions between ideas, pacing, and the conclusion. If your students are not writing effective and inviting introductions to their papers, give them some practice with that single aspect of the whole rubric. You could use strategies 1 through 3: Ask "What makes a good introduction?"; share examples of strong and weak introductions; have students write an introduction to something they are working on; and offer descriptive feedback based on strengths and weaknesses of introductions as described in your rubric.

*Rationale:*

 Novice learners cannot improve simultaneously all elements of quality of a complex skill or product. If your scoring rubric represents a complex skill or product, students will benefit from a "mini-lesson" approach, wherein they are allowed to learn and master a portion at a time.

##### **STRATEGY 6: TEACH STUDENTS FOCUSED REVISION.** 
When a concept, skill, or competence proves difficult for students, we can let them practice it in smaller segments and give feedback on just the aspects they are practicing.

*In the context of performance assessment:*

Any activity that allows students to revise their initial work with a focus on a manageable number of aspects of quality, problems, or learning targets is a logical next step after teaching focused lessons. Alternatively, let them create a revision plan, detailing the steps they would take to improve their product or performance, and let that stand in for the actual revision. This is especially useful in an assessment *for* learning context; students can think about revision more frequently, because each instance takes less time. Strategy 6 gives students practice using the rubric to selfassess and to guide their revisions.

*Rationale:*

Students need the opportunity to practice using the scoring guide as a guide to revision. When they do this, it is the students and not you who are doing the thinking about and the work of revision; this translates into deeper learning.

##### **STRATEGY 7: ENGAGE STUDENTS IN SELF-REFLECTION AND LET THEM KEEP TRACK OF AND SHARE THEIR LEARNING.** 
Long-term retention and motivation increase when students track, reflect on, and communicate about their learning. In this strategy, students look back on their journey, reflecting on their learning and sharing their achievement with others.

*In the context of performance assessment:*

 Set up a system, such as a portfolio, that lets students track their learning along the way. Along with the artifacts, include the scoring rubrics you and they have used for feedback, self-assessment, and goal setting. Ask them to reflect on the contents of their portfolio, summarize their progress, and to comment on it: What changes have they noticed? What is easy that used to be hard? Where have they been surprised? Disappointed? Excited? What insights into themselves as learners have they discovered?

*Rationale:*

 Any activity that requires students to reflect on what they are learning and to share their progress with an audience both reinforces the learning and helps them develop insights into themselves as learners. By reflecting on their learning, students are learning more deeply and will remember it longer.

 (See Chapter 9 for information about how students can track their learning, Chapter 11 for how to set up and use portfolios for tracking and reflecting on progress, and Chapter 12 for the kinds of conferences students can participate in to share their learning.)

 For a more detailed treatment of how each of these strategies can be used with performance assessment across grade levels and subject areas, refer to Chappuis (2009).

 **My Classroom Then and Now 7.1**
 **Bruce Herzog**
I used to …

I have been an elementary school teacher for over twenty-five years. For the first fifteen years of my career I ended every year feeling disheartened by the fact that most of the students who were not meeting grade-level standards when they entered my room the previous fall were still below grade-level standards when the year ended. So each new year I worked even harder to improve my instruction because, throughout my career, I had been told over and over that good instruction would lead to good learning. I embraced new strategies and new programs of instruction that promised to lead to high achievement for all students, yet success still remained out of reach for far too many of my students.

Now I …

When I first heard about formative assessment I was skeptical, but when I read the research that showed that effective assessment practices could lead to unprecedented achievement gains it convinced me to give it a try. I began by having my students set goals and reflect on their learning. It became immediately apparent that most students weren't exactly sure about what they were supposed to be learning. This led me to focus on making sure that the learning targets I expected students to meet were clear. I did this through the use of rubrics, study guides, and continued student goal setting and reflection. Having clear learning targets helped me focus my instruction on those clearly identified learning targets and enabled me to refine my assessments to reflect exactly what students were learning.

What I notice as a result …

I saw almost immediate results. Within the first month that I began using formative assessment practices in my classroom I saw achievement gains from virtually all students with the greatest gains being made by the lowest-achieving students. Using these practices has completely changed the learning environment in my classroom. Good instruction is still important, but the focus has shifted from teaching to learning and from the teacher to the student. Now that learning targets are clear to students they are able to take responsibility for meeting those learning targets. By regularly reflecting on where they are in relation to the targets they are able to decide what action they need to take or what help they need in moving toward meeting those targets. Formative assessment has removed the barriers to learning in my classroom.

Do all students reach grade level standards in my room now? No, but many more do now than before I began using formative assessment practices and I am seeing dramatic improvement in those students who still have a way to go. More importantly, all students now know that, when they take responsibility for their own learning, they can be successful. One of my students probably put it best when he wrote in one of his reflections, "Now I know what I need to know and I know when I know it."

*Source:* Used with permission from 5th-grade teacher Bruce Herzog, Nooksack Valley School District, Nooksack Valley, WA, 2011.

### **USING A PERFORMANCE TASK AS AN ASSESSMENT**  *FOR* **LEARNING**

In the context of performance assessment, opportunity to practice and improve can be configured several ways, all variations of assessment *for* learning strategies 3, 4, 5, and 6 described in the previous section. These three approaches are summarized in Figure 7.11 , "Performance Assessment Tasks As Opportunities for Practice."

**1.** You can set up one or more practice tasks, use the rubric to give students feedback on their strengths and areas to work on, and let them revise their performance or product before assigning the task you will grade. This often works well with performance tasks that don't take days or weeks to complete. For example, in mathematics you may use several tasks, all assessing the same learning target, as practice. On the first task, use the rubric to offer students feedback, showing them what they are doing well and what they still need to master. Have them revise their work based on the feedback. On the second task, let them use the rubric to give each other feedback, again having them revise their work based on the feedback. On the third task, have them use the rubric to self-assess and then revise as needed. Use the fourth task as the graded event. If you have access to multiple tasks addressing the same learning target, consider a progression such as this rather than assigning them all for a grade.

**FIGURE 7.11** Performance Assessment Tasks As Opportunities for Practice

- **1.** Schedule feedback, self-assessment, and revision on short practice tasks before assigning a task to be graded.
- **2.** Break complex tasks into parts and schedule feedback, self-assessment, and revision on each part before students put the pieces together and turn the product in for a grade.
- **3.** Schedule feedback, self-assessment, and revision multiple times while students are developing a complex performance or product that will ultimately be graded.

*Source:* Adapted from *Creating and Recognizing Quality Rubrics* (p. 134 ), by J. A. Arter and J. Chappuis, 2006, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

- **2.** On a more lengthy assignment, such as a science laboratory experiment, you may want to break the task into smaller pieces. Students can write just the hypothesis and receive feedback on it from the portion of the rubric describing levels of quality for the hypothesis. Or you may give them practice at writing hypotheses by offering an experiment context and having them draft a hypothesis and then revise it using rubric-based feedback from you, from other students, or from their own self-assessment. You can follow the same procedure to practice any other parts of the experiment process or report that students have not yet mastered.
- **3.** Another approach, often used in English language arts classes, is to use the same task as assessment *for* and *of* learning through the writing process, as illustrated in Figure 7.12 . Here, the teacher assigns a writing task and gives students time to assemble their ideas and organize them into a draft, with instruction as needed. Students then share their drafts with peers and receive feedback focused on one or more criteria from the rubric. For example, if the instruction focused on the two criteria of *Ideas and Content* and *Voice* , students' feedback to each other

![](_page_227_Figure_3.jpeg)

**FIGURE 7.12** The Writing Process As Assessment *for* Learning

*Source:* Adapted with permission from *Creating and Recognizing Quality Rubrics* (p. 136 ), by J. A. Arter and J. Chappuis, 2006, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

would also focus on *Ideas and Content* and *Voice* . Students then use this feedback as well as their own thoughts to revise the draft. They may repeat the draft/ receive feedback/self-assess/revise cycle one or more times before submitting the paper to the teacher for feedback. Last, students revise their paper based on the teacher's (and perhaps others') feedback and turn it in for a grade. This process approach can also be used for shorter practice pieces, where students work on improving a paper for a selected number of criteria and then put the paper into a working folder. At a later date, students choose one piece from their writing folder to develop further through the whole writing process and then turn it in for a grade.

### **Summary**

Performance assessment, described by some as the most authentic of assessment methods and yet viewed with suspicion by others due to its inherently subjective nature, has evolved into one of our most valuable ways to collect information about student achievement and to involve students in assessing, reflecting on, and sharing their own learning.

This is assessment based on observation and judgment—we observe or review a performance or product and make a judgment about its quality. Performance assessments consist of two parts: a task what we ask the students to do—and criteria—the basis for judging quality. Performance assessment is well suited to evaluating reasoning, skill, and product learning targets.

To select high-quality performance tasks, examine them for three dimensions of quality: *Content* , *Evidence Provided,* and *Sampling* . Does the content of the task match our learning targets and performance criteria? Is it clear to students what they are supposed to do? Can the task be carried out within the time allowed given the materials at hand? Is there anything in the task that might disadvantage any particular student or group of students? Do we have enough tasks that cover enough dimensions of the targets to ensure that we will be able to infer overall level of student mastery of the target?

To develop high-quality tasks, we follow the same procedure for developing extended written response items:

- Specify the learning to be demonstrated.
- Specify the materials and constraints within which achievement is to be demonstrated.
- Remind students of the criteria that will be used to evaluate their performance or product.
To structure writing tasks, we can use the RAFTS formula: specify role, audience, format, topic and purpose (strong verb).

To evaluate tasks for quality, we can use the Rubric for Tasks (see Figure 7.6 ), found on the CD in the Chapter 7 file.

To select high-quality performance rubrics, we look at three dimensions of quality: *Content* , *Structure,* and *Descriptors.* Do the criteria cover features of work that really define quality? Are the criteria and descriptors organized to portray an accurate picture of strengths and weaknesses? Do the descriptors accurately represent the criteria? Are they complete? Are they clear enough so that teachers and students are likely to interpret them the same way?

To develop rubrics, we follow six steps: (1) establish our knowledge base, (2) gather samples of student performance, (3) sort the samples by level of quality and describe the features of the work at each level, (4) cluster the features into traits, (5) identify samples to illustrate each level, and (6) test the rubric and revise as needed.

To evaluate rubrics for quality, we can use the Rubric for Rubrics (see Figure 7.10 ), found on the CD in the Chapter 7 file.

The rubrics associated with performance assessments provide the classic example of how to involve students in assessment. Rubrics can be used to help students understand where they are going, where they are now, and how to close the gap. With respect to understanding where they are going, good rubrics define quality so that students can see it. They provide a vocabulary for talking about features of quality work. Using models of anonymous strong and weak performance not only helps students deepen their understanding of the features of a quality performance or product, but also allows students to become accurate raters of performance. This accuracy is essential before students begin to self-assess.

Rubrics also help students to know where they are now in their learning and how to improve. Teachers can use them to provide descriptive feedback to students and students can use them to self-assess, set goals for further learning. Rubric information can form the basis of students' tracking, reflecting on, and sharing their progress.

**NOTES** 

 1. Portions of this chapter have been reprinted and adapted from J. A. Arter and J. Chappuis, *Creating and Recognizing Quality Rubrics* , 2006, Upper Saddle River, NJ: Pearson Education. Copyright © 2006 by Pearson Education, Inc. Reprinted and adapted by permission of Pearson Education, Inc.

### **CHAPTER 7 ACTIVITIES**

End-of-chapter activities are intended to help you master the chapter's learning targets. They are designed to deepen your understanding of the chapter content, provide discussion topics for learning team meetings, and guide implementation of the practices taught in the chapter.

Forms for completing each activity appear in editable Microsoft Word format in the Chapter 7 CD file. Documents on the CD are marked with this symbol:

#### **Chapter 7 Learning Targets**

At the end of Chapter 7 , you will know how to do the following:

- **1.** Select, revise, and develop high-quality performance tasks.
    - **2.** Select, revise, and develop high-quality rubrics.
    - **3.** Use performance assessment formatively, as teaching tools.
    - **4.** Structure performance assessments so that students can use the results to selfassess and set goals for further learning.

| Activity 7.1 | Keep a Reflective Journal                     |
|--------------|-----------------------------------------------|
| Activity 7.2 | Evaluate a Performance Task for Quality       |
| Activity 7.3 | Create a Performance Task                     |
| Activity 7.4 | Create a Writing Task Using the RAFTS Format  |
| Activity 7.5 | Evaluate a Rubric for Quality                 |
| Activity 7.6 | Create a Rubric                               |
| Activity 7.7 | Create a Student-friendly Version of a Rubric |
| Activity 7.8 | Use Rubrics as Assessment for Learning        |
| Activity 7.9 | Structure a Task for Formative Use            |
|              | Activity 7.10 Reflect on Your Own Learning    |
|              | Activity 7.11 Select Portfolio Artifacts      |

##### **Activity 7.1 Keep a Reflective Journal**

Keep a record of your thoughts, questions, and any implementation activities you tried while reading Chapter 7 .

![](_page_231_Picture_4.jpeg)

Reflective Journal Form

##### **Activity 7.2 Evaluate a Performance Task for Quality**

Work with a partner or a team to carry out the following activity.

- **1.** After reading the section titled "Selecting, Developing, or Revising the Task," find a performance task you have used or will use to assess student learning.
- **2.** Make a copy of the task you have selected and the Rubric for Tasks for each person.
- **3.** Individually, evaluate the task for each criterion on the Rubric for Tasks. Refer to the text in Chapter 7for clarification of the terms used on the Rubric for Tasks. Note any areas needing improvement and suggestions for revision.
- **4.** Share your evaluation and notes with your partner or team.
- **5.** Together, revise the task so that it meets all applicable standards of quality as described in the Rubric for Tasks.

![](_page_231_Picture_14.jpeg)

##### **Activity 7.3 Create a Performance Task**

Work independently, with a partner, or a team to carry out the following activity.

- **1.** Select a learning target for which performance assessment is a good match.
- **2.** Identify the intended use and intended users of the assessment information the task will provide.
- **3.** Create the major components of the task by answering these four questions:
    - What knowledge are students to use?
    - What are they to perform or create?
    - What conditions are they to adhere to?
    - How much time will they have?
- **4.** Working with the information from Step 3, draft the task. Use the criteria from the Rubric for Tasks as a guide.
- **5.** Consider the intended use and the learning target to answer the following sampling questions:
    - How many tasks will be needed to sample well?
    - How should these tasks differ to cover the breadth and depth of the learning to be assessed?
- **6.** Identify the criteria that will be used to judge the performance or product. If you do not have a high-quality rubric available, revise or create one following the steps in Activity 7.5 or Activity 7.6.
- **7.** Evaluate your task using the Rubric for Tasks. Note any areas needing improvement and suggestions for revision.
- **8.** Revise your task so that it meets all *applicable* standards of quality as described in the Rubric for Tasks.

Task Development Template Rubric for Tasks

##### ** Activity 7.4 Create a Writing Task Using the RAFTS Format**

If the learning targets you are teaching specify that students will write to inform or explain, persuade or express and defend an opinion, or narrate real or imagined experiences or events, you may want to create a writing assignment using the RAFTS format. If so, use the text in the section titled "Creating Tasks to Elicit Good Writing" as a guide to complete the following activity.

- **1.** Brainstorm possibilities for *role, audience, format, topic*, and *strong verb* ( purpose). Refer to Figure 7.5for *strong verb* suggestions.
- **2.** Considering your students, the context (subject area content? literature content? personal experience content?), and the length of time you want the task to take, select one option for *role, audience, format, topic*, and *strong verb*.
- **3.** Assemble the ingredients into a draft-writing task, following the format illustrated in Example 7.2 .
- **4.** Share your draft task with a partner or your team. Together, use the Rubric for Tasks to evaluate it for quality. Note any areas needing improvement and suggestions for revision.
- **5.** Revise your task so that it meets all *applicable* standards of quality as described in the Rubric for Tasks.

![](_page_233_Picture_9.jpeg)

RAFTS Format Template Rubric for Tasks

##### **Activity 7.5 Evaluate a Rubric for Quality**

Work with a partner or a team to carry out the following activity.

- **1.** After reading the section titled "Selecting, Revising, or Developing Rubrics," find a rubric you have used or will use to assess student learning.
- **2.** Make a copy of the rubric, the Rubric Evaluation Form, and the Rubric for Rubrics for each person.
- **3.** Individually, evaluate the rubric for each criterion on the Rubric for Rubrics. Refer to the text in Chapter 7for clarification of the terms used on the Rubric for Rubrics. Note any areas needing improvement and suggestions for revision.
- **4.** Share your evaluation and notes with your partner or team.
- **5.** Together, revise the rubric so that it meets all applicable standards of quality as described in the Rubric for Rubrics.

Rubric Evaluation Form Rubric for Rubrics

##### **Activity 7.6 Create a Rubric**

If you have decided that no suitable rubric exists to evaluate the form of reasoning, skill, or product called for in a learning target, you will need to create one. This work is ideally done with a team, but can be completed with a partner, if more people are not available.

- **1.** Reread carefully the section titled "Selecting, Devising, or Developing Rubrics." Also read the Rubric for Rubrics carefully. Work with your team to discuss and answer any questions you may have about aspects of quality rubrics.
- **2.** Work through the six steps explained in the section titled "Process for Developing Rubrics" to create a draft rubric.
- **3.** Make notes of what needs revising after carrying out Step 6 and make the revisions.
- **4.** Audit your sample collection from Step 5 against the revised rubric. Note which work well and which don't. Revise your sample selection, adding and deleting as needed.
- **5.** Using the Rubric for Rubrics, evaluate your revised rubric for quality. Refer to the text in Chapter 7for clarification of the terms used on the Rubric for Rubrics. Note any areas needing improvement and suggestions for revision.
- **6.** Share your evaluation and notes with your partner or team.
- **7.** Together, revise the rubric so that it meets all applicable standards of quality as described in the Rubric for Rubrics.

Rubric Development Template Rubric Evaluation Form

Rubric for Rubrics

##### **Activity 7.7 Create a Student-friendly Version of a Rubric**

Once you have found or created a rubric that meets standards of quality, create a student-friendly version by following these steps. We recommend that you work with a partner or a team to complete this activity. In some cases, students can also be enlisted to work with you on this.

- **1.** Examine each phrase and decide if you want to leave it out, leave it as is, or convert it to student-friendly language. (Only leave phrases out if you do not need to evaluate the feature they represent.)
- **2.** To convert phrases to student-friendly language, look words up in the dictionary or in textbooks as needed. Discuss with colleagues the best phrasing choices for your students.
- **3.** Convert the definitions into wording your students will understand. Sometimes you need to convert one word into one or more phrases or sentences.
- **4.** Phrase the student-friendly version in the first person.
- **5.** Try the rubric out with students. Ask for their feedback.
- **6.** Revise as needed.

*Source:* Adapted from *Creating and Recognizing Quality Rubrics* (p. 83 ), by J. A. Arter and J. Chappuis, 2006, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

![](_page_236_Picture_11.jpeg)

Create a Student-friendly Version of a Rubric

##### **Activity 7.8 Use Rubrics As Assessment** *for* **Learning**

After reading the section titled "Seven Strategies for Using Rubrics As Instructional Tools in the Classroom," complete one or more of the following activities.

- **1.** Select a rubric that you plan to use with students, but have not yet introduced to them. Follow the instructions in Strategies 1 and 2 to help students understand the concepts represented by the rubric and to improve their ability to use the rubric to determine levels of quality. (For a more detailed explanation of how to implement Strategies 1 and 2 with rubrics, refer to Chapter 2of *Seven Strategies of Assessment* for *Learning* [Chappuis, 2009].) Have students keep the rubric for future use when engaging in the reasoning or creating the performance or product assessed by the rubric.
- **2.** After having introduced the language of a rubric to students, offer them feedback on their work by highlighting phrases on the rubric that match the level of quality they have produced. Ask them to revise their work based on the feedback before turning it in for a grade.
- **3.** After having offered feedback with the rubric, ask students to revise their work and then self-assess using the same rubric. If you are highlighting phrases on the rubric, have them use a different color highlighter to self-assess their revised work.
- **4.** Select one portion of the rubric and plan a series of lessons to help students get better at just that one part. You may want to include Strategy 2 activities as a part of the focused instruction. Ask them to practice on just that one part, then give them feedback on it. Let them revise their work, just for the characteristics that were the focus of instruction.
- **5.** Have students keep evidence of their growing proficiency—the tasks they have completed along with the formative and summative information from the rubrics. Refer to Chapters 11and 12for more information on how to help students track, reflect on, and share their growth and achievement using these artifacts.
- **6.** Reflect on the quality of the final performances or products your students are creating after experiencing these activities: how does it compare to the quality of their previous work (or to that of classes in the past)?
- **7.** Discuss with a partner or your team any impact the strategies might have had on students' motivation and achievement. Also discuss how you might modify the activities for future use.

![](_page_237_Picture_11.jpeg)

Debrief AFL Use of Rubrics

##### **Activity 7.9 Structure a Task for Formative Use**

After reading the section titled "Using a Performance Task as an Assessment *for* Learning," select a performance assessment task that you use to structure for formative use.

- **1.** Choose one of the three options best suited to your students, intended learning target(s), and task(s). Adapt it as needed.
- **2.** Find or create the materials required to carry out the option.
- **3.** Try it out with students, noting their responses to the activities.
- **4.** Reflect on the quality of their final performance or product: how does it compare to the quality of their previous work (or to that of classes in the past)?
- **5.** Discuss with a partner or your team any impact this activity might have had on students' motivation and achievement. Also discuss how you might modify the process for future use.

Debrief Structuring a Task for Formative Use

##### **Activity 7.10 Reflect on Your Own Learning**

Review the Chapter 7learning targets and select one or more that represented new learning for you or struck you as most significant from this chapter. If you are working individually, write a short reflection that captures your current understanding. If you are working with a partner or a team, either discuss what you have written or use this as a discussion prompt for a team meeting.

Reflect on Chapter 7 Learning

##### **Activity 7.11 Select Portfolio Artifacts**

Any of the activities from this chapter can be used as portfolio entries. Select activities you have completed or artifacts you have created that will illustrate your competence at the Chapter 7learning targets:

- **1.** Select, revise, and develop high-quality performance tasks.
- **2.** Select, revise, and develop high-quality rubrics.
- **3.** Use performance assessment formatively, as teaching tools.
- **4.** Structure performance assessments so that students can use the results to selfassess and set goals for further learning.

If you are keeping a reflective journal, you may want to include Chapter 7 's entry in your portfolio.

![](_page_239_Picture_9.jpeg)

Chapter 7 Portfolio Entry Cover Sheet

### **CD RESOURCES**

- 1. Activity 7.1 Reflective Journal Form
- 2. Activity 7.2 Performance Task Evaluation Form
- 3. Activity 7.2 Rubric for Tasks
- 4. Activity 7.3 Task Development Template
- 5. Activity 7.3 Rubric for Tasks
- 6. Activity 7.4 RAFTS Format Template
- 7. Activity 7.4 Rubric for Tasks
- 8. Activity 7.5 Rubric Evaluation Form
- 9. Activity 7.5 Rubric for Rubrics
- 10. Activity 7.6 Rubric Development Template
- 11. Activity 7.6 Rubric Evaluation Form
- 12. Activity 7.6 Rubric for Rubrics
- 13. Activity 7.7 Create a Student-friendly Version of a Rubric
- 14. Activity 7.8 Debrief AFL Use of Rubrics
- 15. Activity 7.9 Debrief Structuring a Task for Formative Use
- 16. Activity 7.10 Reflect on Chapter 7 Learning
- 17. Activity 7.11 Chapter 7 Portfolio Entry Cover Sheet