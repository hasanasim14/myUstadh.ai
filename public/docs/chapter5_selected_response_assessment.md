## Chapter 5:  Selected Response Assessment

From Chapter 5 of *Strategies Classroom Assessment for Student Learning: Doing It Right – Using It Well*, Second Edition. Jan Chappuis, Rick Stiggins, Steve Chappuis, Judith Arter. Copyright © 2012 by Pearson Education. All rights reserved.


*Surprisingly, some of our most powerful formative assessment practices involve diagnostic uses of selected response items, quizzes, and tests, by both teachers and students.* 

Over the years selected response assessment has become almost synonymous with the concept of testing. In the 1920s and 1930s, however, it was known as the new, scientific assessment method, welcomed by those at the forefront of education innovation because it was considered objective—that is, free of teacher judgment. Although its luster dimmed over the years, it has reemerged as a predominant assessment method, fueled by the intersection of testing and technology. Computer adaptive testing, other forms of online testing, and software programs rely almost exclusively on selected response methodology. Two major reasons for its dominance, especially in accountability testing, are that it is easy to administer and cheap to score.

However, as we have seen in Chapter 4 , if we use only selected response assessments, we are gathering data about only a portion of the important learning targets we teach. And teaching to the domain of what is covered on the accountability tests often requires that we give short shrift to learning targets not assessed with this method. For these reasons, many view selected response assessment in the classroom with distrust, even though it is still a valuable tool in our assessment repertoire and worthy of regular use as both assessment *for* learning and assessment *of* learning.

Selected response assessments can include one or more of four different item types: multiple choice, true/false, matching, and fill in the blank. In this chapter, we examine formative and summative uses for selected response assessments, how to choose from among selected response formats, how to create quality items, and how to use selected response assessments formatively, as assessment *for* learning.

### **Chapter 5 Learning Targets**

At the end of this chapter you will know how to do the following:

- Make a test blueprint for a selected response assessment.
- Choose from among selected response formats.
- Create high-quality items.
- Audit any selected response test for quality.
- Use selected response assessments to plan further instruction.
- Use selected response assessments as feedback to students and for student self-assessment and goal setting.

**FIGURE 5.1** Keys to Quality Classroom Assessment

**Key 1: Clear Purpose** Who will use the information? How will they use it? What information, in what detail, is required?

**Key 2: Clear Targets** Are learning targets clear to teachers? What kinds of achievement are to be assessed? Are these learning targets the focus of instruction?

**Key 3: Sound Design**

Do assessment methods match learning targets? Does the sample represent learning appropriately? Are items, tasks, and scoring rubrics of high quality? Does the assessment control for bias?

**Key 4: Effective Communication**

Can assessment results be used to guide instruction? Do formative assessments function as effective feedback? Is achievement tracked by learning target and reported by standard?

Do grades communicate achievement accurately?

**Key 5: Student Involvement**

Do assessment practices meet students' information needs? Are learning targets clear to students? Will the assessment yield information that students can use to self-assess and set goals? Are students tracking and communicating their evolving learning?

![](_page_134_Picture_1.jpeg)

### **WHEN TO USE SELECTED RESPONSE ASSESSMENT**

The first condition for using selected response is that it must be capable of reflecting the type of learning target to be assessed. Selected response formats are ideal for assessing knowledge-level learning targets, some patterns of reasoning, and a very few number of skill targets, as described in Chapter 4 .

Several other key conditions influence choosing the selected response method of assessment. Use it when

- The content to be assessed is broad, requiring wide-ranging coverage. Since the response time to one item is so short, you can include lots of items per unit of testing time and thus sample student achievement thoroughly.
- You want to diagnose student misconceptions and flaws in reasoning.
- Students can read English well enough to understand what each test item is asking of them.

 **FAQ 5.1 Misconceptions about Selected Response Assessment**

Question:

*Shouldn ' t we be using mostly multiple-choice tests because all the high-stakes tests use them?* 

Answer:

No. Although high-stakes tests use this format extensively, the reason for that choice is not because it is a better method. Large-scale tests usually need to be administered and scored in as little time as possible, as inexpensively as possible. These requirements lead to the use of selected response formats such as multiple choice. The obvious problem is that, unless other formats are also part of the assessment, learning targets representing important patterns of reasoning, skills, and products are going unmeasured.

Giving students practice on answering large-scale test items is one thing, but mirroring characteristics of high-stakes tests that are not instructionally useful or that do not provide accurate results in the classroom is not a good idea.

Question:

*Shouldn ' t we minimize the use of selected response assessments because they are not authentic?* 

Answer:

First, let's define "authentic." The *New American Oxford Dictionary* offers this as one definition: "made or done in a way that faithfully resembles the original" (p. 107 ). In the usual application to assessment, *authentic* refers to the context of the assessment mirroring the use or application of the learning in a situation that would require it in life. (We prefer to call this "life beyond school" rather than "real-world" because school can and should be part of the real world for students.)

By that definition, selected response methodology is not "inauthentic." Life beyond school often calls for correct answers and solutions chosen from a variety of options. We believe it is more helpful to think of authenticity as a *dimension* of assessments, not as a label given to some forms rather than others. We can keep it as a consideration when writing assessments of any sort, as long as the application or context doesn't interfere with the accuracy of the item, task, or scoring guide.

### **DEVELOPING A SELECTED RESPONSE TEST**

We will follow the steps in the Assessment Development Cycle described in Chapter 4 .

**Planning**
- **1.** Determine who will use the assessment results and how they will use them.
- **2.** Identify the learning targets to be assessed.
- **3.** Select the appropriate assessment method or methods.
- **4.** Determine sample size.
**Development**
- **5.** Develop or select items, exercises, tasks, and scoring procedures.
- **6.** Review and critique the overall assessment for quality before use.
 **Use**

- **7.** Conduct and score the assessment.
- **8.** Revise as needed for future use.

### **PLANNING STEPS**

As we saw in Chapter 4 , careful attention to each of the four planning steps is essential to ensuring that the resulting assessment will do what you want it to.

#### **Step 1: Determine Users and Uses**

We begin planning by answering these questions: How do we want to use the information? Who else will use it? What decisions will they make? Typically, we will use assessment information for one or more of the following purposes:

- To plan instruction, as with a pretest
- To offer feedback to students so they can self-assess and set goals for further learning
- To differentiate instruction according to student needs, as with a mid-unit quiz or an interim assessment
- To measure level of achievement to inform grading decisions, as with a post-test

Each one of these purposes can be accomplished with selected response formats, as long as we keep the intended use in mind while making further planning and design decisions.

#### **Step 2: Identify Learning Targets**

At this step we simply list the specific learning targets we have identified for the assessment. If one or more targets on the list are complex or unclear, clarify them or deconstruct them first, following the processes outlined in Chapter 3 .

#### **Step 3: Select Assessment Method(s)**

Although we have already determined that we will use selected response, we must make sure the list of clarified targets only includes knowledge and reasoning learning targets and also that those targets can be assessed well with selected response methodology. So, review the list of learning targets to verify that they are knowledge and reasoning targets and that selected response items can capture an accurate picture of achievement.

#### **Step 4: Determine Sample Size**

This step requires that we assign a relative importance to each learning target. One simple way to do this with selected response questions is to decide how many points the test will be worth and then divide the points according to relative importance

*When using a test that you didn't develop, check carefully that it matches the learning targets you taught and that the amount of emphasis each receives is appropriate.* 

of each learning target. The number of points we assign to each learning target outlines our sample, which should represent the breadth of the learning targets and their importance relative to each other in the instructional period the test is to cover. At this step, you may want to review the sampling considerations described in Chapter 4 .

Remember, when identifying the relative importance of each learning target, we consciously match our emphasis in assessment to our emphasis in the classroom. If, say, we spend 50 percent of the time learning how to read maps, then roughly 50 percent of the assessment should focus on map reading. If only 5 percent of the

course deals with reading maps, then in most cases it would misrepresent learning to devote 50 percent of the final assessment to map reading.

If, on the other hand, the results are to be reported by individual learning target, or if the test only measures one single target, the sample must be sufficient to defend an inference about mastery of that individual target.

**COMBINING PLANNING DECISIONS INTO AN ASSESSMENT BLUEPRINT.** For selected response assessments, we offer two useful types of blueprints. One is a list of the learning targets and the other is a table crossing content with knowledge and
**TABLE-Blueprints for Third-Grade Mathematics and Reading TestS**
| FIGURE 5.2 Blueprints for Third-Grade Mathematics and Reading Tests              |                  |
|----------------------------------------------------------------------------------|------------------|
| Mathematics                                                                      |                  |
| Learning Targets                                                                 | Number of Points |
| Identify place value to thousands                                                | 6                |
| Read, write, order, and compare numbers through four digits                      | 10               |
| Use place value understanding to round whole numbers to the nearest 10 or 100    | 4                |
|                                                                                  |                  |
| Reading                                                                          |                  |
| Learning Targets                                                                 | Number of Points |
| Determine the lesson of a fable                                                  | 1                |
| Identify key supporting details                                                  | 2                |
| Infer a character's feelings                                                     | 2                |
| Distinguish literal from nonliteral language                                     | 2                |

pattern(s) of reasoning to be assessed. Each is suited to different types of content, but both are equally effective as test planning instruments.

Figure 5.2 shows plans for a third-grade mathematics test and a third-grading reading test consisting of a list of learning targets and how many points each will be worth. Note that on the reading test, only one to three points are assigned to each learning target. That is because, for any one reading passage, it can be difficult to develop more than one or two items to assess targets such as "Infer a character's feelings." So, especially with the shorter passages at the lower grades, you would want to construct similar items for a variety of reading passages at the same level of difficulty to obtain a sufficient sample size from which to draw conclusions about students' level of mastery.

Figure 5.3 shows a list of learning targets for a fifth-grade social studies unit on westward expansion. The test blueprint consists of a list of the content embedded in the learning targets in the left-hand column labeled "Content Categories." Each category represents many facts and concepts, some of which will be sufficiently important to test. The blueprint also includes columns labeled for the cognitive action to be carried out: know outright and reason comparatively. These patterns will be emphasized during the unit of study. The numbers in each cell represent its relative importance in the unit as planned. This kind of test plan is especially useful if we want to ensure that the test covers both recall of important information and reasoning processes we have taught. (Remember that there could be other learning targets taught during the unit—this blueprint represents only those covered by the selected response portion of the test.)

**FIGURE 5.3 Learning Targets and Blueprint for a Fifth-Grade Social Studies Unit**

- **1.** Explain the concept of Manifest Destiny and its contribution to the migration of people in the development of the United States.
- **2.** Compare the motives of the different groups who participated in the westward expansion by leaving the eastern United States and heading west.
- **3.** Compare the lives of different Native American groups before and after westward expansion.
- **4.** Identify significant individuals who took part in the westward expansion.
- **5.** Explain how the westward migration led to conflict between Native Americans and settlers and between Mexicans and settlers.
| Content Category                            | Knowledge | Compare/ Contrast | Totals |
|---------------------------------------------|-----------|-------------------|--------|
| 1. Manifest Destiny                         | 2         |                   | 2      |
| 2. Reasons settlers went west               |           | 6                 | 6      |
| 3. Life of Native American groups           | 4         | 2                 | 6      |
| 4. Significant individuals                  | 6         |                   | 6      |
| 5. Effects on Native Americans and Mexicans | 6         | 4                 | 10     |
| TOTALS                                      | 28        | 8                 | 36     |

### **DEVELOPMENT AND USE STEPS**

The remaining four steps of the Assessment Development Cycle focus on developing the items and scoring procedures, critiquing the assessment for quality, administering the assessment, and revising it as needed.

#### **Step 5: Develop or Select Items, Exercises, Tasks, and Scoring Procedures**

The process for developing selected response items is as follows: identify the specific content to include, choose which kinds of items to write, write the items, and assemble the test.

**IDENTIFYING SPECIFIC CONTENT TO INCLUDE.** Even though people often think of selected response tests as objective measures of learning, selecting the content for the test is itself a subjective exercise. The test developer—you yourself, a textbook author, or a test publisher—chooses what will be on the test from a vast array of possible considerations. It is a matter of professional judgment, just as is determining how to teach the material in the first place. This element of subjectivity does not compromise the test's validity if the learning targets that underpin the content standards have been clearly and accurately identified.

Now that you have numbers on the test plan to indicate the relative importance of each learning target or content category, the next step is to identify what content you will test for each cell. In most cases, you won't test everything students are to have learned. Instead, you will select or create questions that cover as much of the important content as possible given the amount of testing time available, and that are prudent for the age of your students. You use the results to make an inference: a student who has scored 75 percent on the test has mastered about 75 percent of the material that was intended to be learned. You must carefully select the sample of all possible important aspects of knowledge and reasoning so that it allows you to accurately estimate level of achievement.

 **My Classroom Then and Now 5.1**
**Myron Dueck** 
I used to . . .

For the first decade of my teaching career, I constructed tests by section based on the type or style of the questions. For instance, my unit tests would have a structure such as this:

| Section 1: True & False      | (10 pts) |
|------------------------------|----------|
| Section 2: Multiple Choice   | (15 pts) |
| Section 3: Short Answers     | (10 pts) |
| Section 4: Long Answer/Essay | (20 pts) |

In each of the first three sections, I took a somewhat random sampling of concepts and facts from throughout the unit being tested. Therefore, from each section I had a basic idea of what the student knew of the unit as a whole. The last section could be general or specific, depending on the unit of study.

These tests were fairly easy to make and even easier to mark. As well, I thought that this was the best type of test to produce, but this is probably based on years of seeing this as 'the standard' way to construct a test.

Now I . . .

I divide my test by the learning outcomes or standards I want to evaluate. Now a test on the USA in the 1920s is structured like this:

| Section 1: USA in the 1920s                    | (11 pts) |
|------------------------------------------------|----------|
| Section 2: Causes of the Great Depression      | (8 pts)  |
| Section 3: FDR's Efforts to End the Depression | (6 pts)  |
| Section 4: Reactions to FDR's Actions          | (7 pts)  |
| Section 5: FDR's Overall Impact on the USA     | (11 pts) |
|                                                |          |

Once a student takes the test and I have evaluated it, the student notes his/her section scores on a custom tracking sheet. Each section is allotted a percentage score and this score is compared to the student's overall test score as well as his or her academic goal. Taking all of these numbers into account, the student determines which section(s) to retest. The student also has an opportunity to plan what he/she might do differently in preparation for the subsequent evaluation.

Why I changed . . .

- **1.** Students are able to be evaluated according to each learning outcome and this has obvious benefits:
	- **a.** I can reteach sections (learning outcomes) upon which the entire class scored poorly or below competency.
	- **b.** An individual student can retest a single section or multiple sections depending on how he or she performed.
	- **c.** A student can easily identify and focus on the areas in which they know they can improve.
	- **d.** As the facilitator, I can effectively and efficiently administer retests as I am only retesting and remarking those sections that have been identified.
- **2.** Struggling learners usually start with the section they know best, and research shows that success breeds success.
- **3.** I am able to quickly evaluate if there is a strong correlation between the value of each section and the amount of time allotted to it in class.
- **4.** I have constructed retests that have the same sections and values, but different questions or question formats. It is very easy to administer these retests and to determine if authentic learning has occurred.
- **5.** This structure is a very good way to use both formative and summative assessments in the same way and at the same time.
- **6.** Students feel a sense of ownership and control not present in conventional testing formats.

What I notice as a result . . .

- **1.** Very positive student reactions to this system.
- **2.** Incredible parent support and encouragement.
- **3.** Increased student participation in 're-learning' activities.
- **4.** Less stress and pressure at the time of the first evaluation.

**WRITING PROPOSITIONS.** *Propositions* are statements of important facts, concepts, or understandings that students will be held accountable for learning. Writing propositions is an efficient way to (1) identify the content that will be on the test and then (2) generate any type of selected response item you choose to reflect that content.

*Source*: Used with permission from Myron Dueck, high school social studies teacher, SD 67 (Okanagan-Skaha), Penticton, BC, Canada, 2011.

To write propositions, first review the material you have taught. For every cell in the test plan, note in writing the most important facts, concepts, or understandings you think every student should have at the end of instruction. State each as a clear sentence. *These sentences are propositions* . As you will see, they form a quick and easy basis for writing quality test items. Many teachers find it helpful to collect propositions over time while instruction is unfolding. This facilitates later test development. We recommend that you write more propositions than you will need. Additional propositions serve two purposes: (1) they allow you to create parallel forms of the test if you like and, (2) they provide a convenient supply of replacement propositions or test items if you find a weak item during test development. You will find that it is a time-saver to write propositions as you teach the material or to collect them afterwards.

**Knowledge Propositions.** Let's say we are writing propositions for the test planned in Figure 5.3 . We will need a total of 30 knowledge items, two of which will relate to Manifest Destiny. As we read through the material we have taught, we identify and write down three or four statements that reflect important knowledge about the concept of Manifest Destiny. These are our propositions. They might include the following:

- Manifest Destiny represents a belief that it was natural and right to expand the territory of the United States westward.
- Manifest Destiny represents a mission to impart the government and way of life of United States citizens to people living in the land west of the United States during the 1800s.
- Manifest Destiny was first used as a justification for annexing Texas to the United States.
- Manifest Destiny represents a belief that was used to justify the taking of Native American lands.

The test plan also requires six items in the cell that crosses *Know* with *Effects on Native Americans and Mexicans* . Here are two sample propositions:

- Three effects of westward expansion on Plains Indians were increased disease, removal to reservation lands, and loss of food sources.
- Mexico lost the territory of Texas.

**Reasoning Propositions.** To write one of these, identify the knowledge to be applied, apply the pattern of reasoning, and state the result as a declarative sentence. In essence, you state a correct application of the pattern of reasoning to the content. To write propositions for the cell in Figure 5.3 that crosses *Reasons settlers went west* with *Compare/Contrast,* we would identify two groups of settlers to focus on and then state a comparison of reasons each went west.

Examples of the resulting propositions might read like this:

- The Mormons went west to practice their religion without persecution, whereas the settlers in Texas went west because land was cheap or free and they wanted a place to start over.
- Both Mormons and settlers in Texas were searching for a better life.
- Settlers were encouraged to move to Texas by the Mexican government, while Mormons were led to settle in Utah by their religious leaders.

Remember that when we intend to evaluate students' ability to reason, we must provide them with a context different than that in which they practiced their reasoning. If we don't, as we saw in Chapter 3 , we will not be capturing real evidence of their reasoning (ability to figure things out). Instead, we will assess what they remember from previous reasoning. If we want to assess the learning target as a reasoning proficiency, "Compare the motives of the different groups who participated in the westward expansion by leaving the eastern United States and heading west," we cannot have reasoned that through during instruction. It must appear on the test as a novel (that is, new to the student) item. If we did figure it out in class, then we certainly can assess student mastery of it, but it becomes a knowledge question.

**Interpretive Exercises.** This is a label used for those instances when we present students with a table of information, a diagram, or some other source of information and then ask them to use that information to figure out answers to reasoning questions. The most common version of this is found in reading comprehension tests, where a passage is accompanied by test items that ask for inferences based on the content of the passage. In this case, the propositions you write will be specific to the content of the information provided. For example, for the reading targets from the test planned in Figure 5.2 , students might read a short passage recounting Odysseus's experience with the Cyclops.

Propositions for the learning target "infer a character's feelings" might look like this:

- Odysseus felt afraid in the Cyclops' cave.
- Polyphemus felt angry towards the sailors.

You might consider using the interpretive format in content areas other than English language arts when you are not sure that some (or all) of your students have

mastered some body of basic knowledge, but nevertheless, you want to assess their reasoning proficiency. In this case, present the information and construct reasoning propositions. Or, if you simply want to assess reasoning and don't need to assess content knowledge, just give the content knowledge and write propositions to structure questions asking them to apply the pattern of reasoning.

**When Writing Propositions Is Unnecessary.** For some knowledge targets such as those identified in the third-grade mathematics test plan, writing propositions is unnecessary.

**CHOOSING ITEM TYPES.** Selected response methodology offers four choices of item types: multiple choice, true/false, match, and fill in the blank. Each has strengths and weaknesses. You can use Figure 5.4 to decide when to choose which format.
**TABLE-Comparison of Selected Response Item Types**
| Item              | Used When                           | Advantage                               | Limitations                            |
|-------------------|-------------------------------------|-----------------------------------------|----------------------------------------|
| Multiple Choice   | There is only one right answer.      | Can measure a variety of objectives.   | Guessing can skew score (up to 33%     |
|                   | There are several plausible          | Easy to score.                         | chance, depending on number of         |
|                   | alternatives to the correct answer.  | Can cover lots of material efficiently.| distracters). Can be hard to identify  |
|                   |                                      | Carefully crafted distracters can      | plausible distracters.                 |
|                   |                                      | provide diagnostic information.        |                                        |
| True/False        | A large body of content is to be     | Can ask many questions in a short time.| Can be trivial or misleading if not    |
|                   | tested, requiring the use of many    | Easy to score.                         | written carefully. Guessing can skew   |
|                   | test items.                          |                                        | score (50% chance).                    |
| Matching          | There are many related thoughts or   | Can cover lots of material efficiently.| Process of elimination can skew score  |
|                   | facts; you want to measure           | Easy to score.                         | if not written carefully.              |
|                   | association of information.          | Can serve as several multiple choice   |                                        |
|                   |                                      | items in one (each response is a       |                                        |
|                   |                                      | distracter for the others).            |                                        |
| Fill in the Blank | A clear, short answer is required.   | Assesses production of a response.     | Takes longer to score.                 |
|                   | You want to determine if students    | Reduces the possibility of getting the |                                        |
|                   | know the answer, rather than if they | right answer by guessing.              |                                        |
|                   | can select it from a list.           | Can cover lots of material efficiently.|                                        |


**WRITING ITEMS.** Now here is the power of propositions: Once you have written them to reflect your important learnings, you can use them to almost instantly write whatever kind of selected response item you might want to use. Here's how it works with the following proposition from the Manifest Destiny example:

Manifest Destiny represents a mission to impart the government and way of life of United States ' citizens to people living in the land west of the United States during the 1800s.

**Multiple-Choice Items.** To create a multiple-choice item, turn the basic focus of the proposition into a question. Then turn the other part of the proposition into the correct answer. Add a number of plausible but incorrect answers and you have a multiple-choice item.

**What was the mission of Manifest Destiny in the United States in the 1800s?**

- **a.** To have Lewis and Clark make friends with the Native Americans they met.
- **b.** To move the U.S. form of government and way of life west.
- **c.** To defeat General Santa Anna in the battle of the Alamo.
- **d.** To establish religious freedom for all who lived in the west.

**True/False Items.** To create a true/false item that is true, include the proposition on the test as stated. In this example, for fifth graders, you may want to simplify the proposition so that it reads as follows:

Manifest Destiny represents a mission the U.S. had in the 1800s to move its government and way of life westward .

To create a false true/false item, make one part false:

Manifest Destiny represents a mission the United States had in the 1800s to guarantee religious freedom to all settlers.

**Matching Items.** This is a way to sample several interrelated propositions at once with great efficiency. Think of each entry in a matching exercise as a multiple-choice item, in that the task is to combine the trigger item (or "stem") with its proper match. You simply take your proposition and separate it into its subject (stem) and predicate (match) parts. Do this with several propositions at once, list the stems in order and then scramble the matches and you have a matching exercise.

 The key to writing good matching items is to use them when they make sense: where the learning targets can be thought of as a series of closely linked propositions, such as states and their capitals or items to be categorized and their categories. Any individual match (stem and response) would state a single proposition. Matching items generally test knowledge propositions, but they can also be used to assess reasoning propositions.

**Fill-in-the-blank Items.** To create a fill-in item from a proposition, leave out the phrase defining the concept or dealing with the effect and ask a question:

What was the mission of Manifest Destiny in the United States in the 1800s?

**For Example 5.1 Turning a Social Studies Proposition into Different Item Types**

**Proposition:**

Three effects of westward expansion on Plains Indians in the 1800s were increased disease, removal to reservation lands, and loss of food sources.

**Multiple-choice Item:**

What were three effects of westward expansion on Plains Indians in the 1800s?

- **a.** Access to health care, removal to reservation lands, and loss of food sources
- **b.** Access to health care, population growth, and opportunities for better jobs
- **c.** Increased disease, removal to reservation lands, and loss of food sources
- **d.** Loss of their schools, removal to reservation lands, and private ownership of land

**True/False Item:**

(True) Three effects of westward expansion on Plains Indians were increased disease, removal to reservation lands, and loss of food sources.

(False) One effect of westward expansion on Plains Indians was access to better health care.

**Matching Item:**

Not applicable

 **Fill-in-the-blank Item:**

What were three effects of westward expansion on Plains Indians in the 1800s?

**Turning a Reading Proposition into Different Item Types** 

**Proposition:**

Odysseus felt afraid in the Cyclops' cave.

**Multiple-choice Item:**

How did Odysseus feel in the Cyclops cave?

- **a.** Envious because Polyphemus was strong and fierce.
- **b.** Ashamed because he lied to Polyphemus.
- **c.** Worried that Polyphemus was going to hurt the goats.
- **d.** Afraid that Polyphemus would eat him.

**True/False Item:**

(True) Odysseus felt afraid in the Cyclops' cave. (False) Odysseus felt ashamed in the Cyclops' cave.

**Matching Item:**

Not applicable

 **Fill-in-the-blank Item:**

How did Odysseus feel in the Cyclops' cave?

Here is one final quality-control idea linked to propositions. Before you do any item writing, list all of the propositions that are going to make up your test. Then review the list from top to bottom in one reading, asking yourself this question: Does this set of ideas really reflect the essential learnings of this unit of study? Remove and replace weak entries and fill any gaps you discover.

**GUIDELINES FOR WRITING QUALITY ITEMS.** We offer here the commonsense guidelines that test developers use to ensure item quality. 1 The first set of guidelines applies to all item types, and the rest are specific to each particular format.

**General Guidelines.** 

**1.** *Keep wording simple and focused. Aim for the lowest possible reading level.* Good item writing first and foremost represents an exercise in effective written communication.

*Right:*

What are the poles of a magnet called?

- **a.** Anode and cathode
- **b.** North and south
- **c.** Strong and weak
- **d.** Attract and repel

*Wrong:*

When scientists rely on magnets in the development of electric motors, they need to know about poles, which are?

**2.** *Ask a full question in the stem.* This forces you to express a complete thought in the stem or trigger part of the question, which usually promotes students' understanding.

*Right:*

What was the trend in interest rates between 1950 and 1965?

- **a.** Increased only
- **b.** Decreased only
- **c.** Increased, then decreased
- **d.** Remained unchanged

*Wrong:*

Between 1950 and 1965

- **a.** Interest rates increased.
- **b.** Interest rates decreased.
- **c.** Interest rates fluctuated greatly.
- **d.** Interest rates did not change.
- **3.** *Eliminate clues to the correct answer either within the question or across questions within a test.* When grammatical clues within items or material presented in other items give away the correct answer, students get items right for the wrong reasons.

*Wrong:* 

All of these are an example of a bird that flies, except an

- **a.** Ostrich
- **b.** Falcon
- **c.** Cormorant
- **d.** Robin

(The article *an* at the end of the stem requires a response beginning with a vowel. As only one is offered, it must be correct.)

*Also wrong:*

Which of the following are examples of birds that do not fly?

- **a.** Falcon
- **b.** Ostrich and penguin
- **c.** Cormorant
- **d.** Robin

(The question calls for a plural response. As only one is offered, it must be correct.)

- **4.** *Do not make the correct answer obvious to students who have not studied the material.*
- **5.** *Highlight critical, easily overlooked words* (e.g., NOT, MOST, LEAST, EXCEPT).
- **6.** *Have a qualified colleague read your items to ensure their appropriateness.* This is especially true of relatively more important tests, such as big unit tests and final exams.
- **7.** *Double-check the scoring key for accuracy before scoring.*

**Guidelines for Multiple-Choice Items.** The following guidelines for writing multiplechoice test items allow students to answer questions more quickly without wasting time trying to determine what the question is saying. (The item *stem* refers to the part of the question that comes before the choices. The *distracters* are the incorrect choices.)

- **1.** *Ask a complete question to get the item started, if you can.* This is important for clarity. It has the effect of directing the item's focus first to the stem, rather than the response options, to guide students' choices.
- **2.** *Don* ' *t repeat the same words within each response option; rather, reword the item stem to remove the repetitive material from below.* This will clarify and focus the problem, not to mention making it more efficient for respondents to read.
- **3.** *Be sure there is only one correct or best answer.* This is where that colleague's independent review can help. Remember, it is acceptable to ask respondents to select a "best answer" from among a set of correct answers. Just be sure to word the question so as to make it clear that they are to find the best answer and be sure there is one clearly best answer.
- **4.** *Choose distracters carefully* . All distracters must be plausible—they must be choices that can't be ruled out without having the knowledge or reasoning proficiency being assessed. Carefully chosen distracters can function as formative assessment tools when they represent typical misunderstandings or flaws in reasoning. Student responses then become diagnostic. For example, when writing an item testing students' ability to generalize, typical errors include not generalizing at all (making a statement that is only true for the evidence at hand) and over-generalizing (making a statement that casts too wide a net). If you know

which kind of error each distracter represents, you can disaggregate the test results by answer choice (which most scoring machines and software can do) to diagnose common misconceptions or specific reasoning problems students are experiencing. Then you can plan instruction to address them. Figure 5.5 shows item formulas for creating distracters that can reveal difficulties with selected patterns of reasoning.

**TABLE-Item Formulas for Selected Patterns of Reasoning**
| Reasoning Learning Target         | Item Formula                                                  |
|-----------------------------------|---------------------------------------------------------------|
| Makes inferences based on text    | Question: Which sentence tells an idea you can get from       |
|                                   | this (selection)?                                             |
|                                   | The right answer — A guess based on clues you can find in     |
|                                   | the text                                                      |
|                                   | A wrong answer — A guess that seems reasonable, but that      |
|                                   | evidence in the text does not support                         |
|                                   | A wrong answer — Not a guess, just a detail recopied          |
|                                   | verbatim from the text                                        |
| Summarizes information in text    | Question: Which sentence best summarizes what this            |
|                                   | (selection) is about?                                         |
|                                   | The right answer — A brief statement of the main point(s)     |
|                                   | or idea(s)                                                    |
|                                   | A wrong answer — A statement including an idea not found      |
|                                   | in the passage                                                |
|                                   | A wrong answer — A statement including an idea from the       |
|                                   | passage that is too narrow to be acceptable as a summary      |
| Compares and contrasts            | Question: Which sentence tells how (two or more items)        |
| elements of text                  | are alike?                                                    |
|                                   | The right answer — A statement of an appropriate similarity   |
|                                   | A wrong answer — A statement that does not identify a         |
|                                   | similarity                                                    |
|                                   | A wrong answer — A statement that is true of one or the       |
|                                   | other but not both                                            |
|                                   | Or                                                            |
|                                   | Which sentence tells how (two or more items) differ?          |
|                                   | The right answer — A statement of an appropriate difference   |
|                                   | A wrong answer — A statement that does not identify a         |
|                                   | difference                                                    |
|                                   | A wrong answer — A statement that claims an inaccurate        |
|                                   | difference                                                    |
| Makes generalizations based       | Question: Which statement can you support after reading       |
| on information found in text      | this selection?                                               |
|                                   | The right answer — A statement that is true for the           |
|                                   | evidence presented and extends the application logically to   |
|                                   | a broader array of instances                                  |
|                                   | A wrong answer — A statement that is true for the evidence    |
|                                   | presented, but includes too broad an array of instances       |
|                                   | to be supported by evidence                                   |
|                                   | A wrong answer — A statement that is true for the evidence    |
|                                   | presented, but does not include an extension to other         |
|                                   | instances                                                     |
|                                   | A wrong answer — A statement that is not true for the         |
|                                   | evidence presented                                            |


*Source:* Adapted from *Washington Assessment of Student Learning 4th-Grade Reading Test and Item Specifications* , 1998, Olympia, WA: Office of the Superintendent of Public Instruction. Adapted with permission.

If you find that you cannot think of enough plausible distracters, include the item on a test the first time as a fill-in-the-blank question. As your students respond, common wrong answers will provide you with a good variety of viable distracters for future use.

**5.** *Word response options as briefly as possible and be sure they are grammatically parallel.* This makes items easier to read and eliminates cues to the right answer.

*Right:*

Why did colonists migrate to the United States?

- **a.** To escape taxation
- **b.** For religious freedom
- **c.** For adventure
- **d.** More than one of the above

*Wrong:*

Why did colonists come to the United States?

- **a.** To escape heavy taxation by their native governments
- **b.** Religion
- **c.** They sought the adventure of living among native Americans in the new land
- **d.** There was the promise of great wealth in the New World
- **e.** More than one of the above answers
- **6.** *Make all response options the same length.* Testwise students know that the correct answer may be the longest one because writers frequently need to add qualifiers to make it the best choice. If you need to do this, do it to all response options.
- **7.** *Don* ' *t use* " *all of the above* " *or* " *none of the above* " *merely to fill space;* use them only when they fit comfortably into the context of the question. In general, test writers avoid using "all of the above" because if a student can determine that two responses are correct, then the answer must be "all of the above."
- **8.** *Use* " *always* " *or* " *never* " *in your answer choices with caution.* Rarely are things always or never true. Absolutes are frequently incorrect; a student who knows this but is not sure of the correct answer can automatically eliminate those choices.
- **9.** *It* ' *s okay to vary the number of response options presented as appropriate to pose the problem you want your students to solve.* While four or five response options are most common, it is permissible to vary the number of response options offered across items within the same test. It is more important to have plausible distracters than a set number of them.

**Guidelines for True/False Exercises.** You have only one simple guideline to follow here: Make the item entirely true or false as stated. Complex "idea salads" including

some truth and some falsehood just confuse the issue. Precisely what is the proposition you are testing? State it and move on to the next one.

*Right:*

The Continental Divide is located in the Appalachian Mountains.

*Wrong:*

From the Continental Divide, located in the Appalachian Mountains, water flows into either the Pacific Ocean or the Mississippi River.

**Guidelines for Matching Items.** When developing matching exercises, follow all of the multiple-choice guidelines offered previously. In addition, observe the following guidelines:

- **1.** *Provide clear directions for making the match.*
- **2.** *Keep the list of things to be matched short.* The maximum number of options is 10. Shorter is better.
- **3.** *Keep the list of things to be matched homogeneous.* Don't mix events with dates or with names.

*Right:*

Directions: New England states are listed in the left-hand column and capital cities in the right-hand column. Place the letter for the capital city in the space next to the state in which it is located. Responses may be used only once.

*Wrong:*
|    | States        | Capital Cities |
|----|---------------|----------------|
| 1. | Rhode Island  | A. Concord     |
| 2. | Maine         | B. Boston      |
| 3. | Massachusetts | C. Providence  |
| 4. | New Hampshire | D. Albany      |
| 5. | Vermont       | E. Augusta     |
|    |               | F. Montpelier  |
|    |               |                |
|    |               |                |
| 1. | Texas         | A. \$7,200,000 |
| 2. | Hawaii        | B. Chicago     |
| 3. | New York      | C. Mardi Gras  |
| 4. | Illinois      | D. Austin      |
| 5. | Alaska        | E. 50th state  |
|    |               |                |

- **4.** *Keep the list of response options brief in their wording and parallel in construction.*
- **5.** *Include more response options than stems and permit students to use response options more than once when appropriate.* This has the effect of making it impossible for students to arrive at the correct response purely through a process of elimination.

**Guidelines for Fill-in-the-blank Items.** Here are three simple guidelines to follow:

- **1.** *Ask respondents a question and provide space for an answer.* This forces you to express a complete thought.
- **2.** *Try to stick to one blank per item.* Come to the point. Ask one question, get one answer, and move on to the next question.
- **3.** *Don* ' *t let the length of the line to be filled in be a clue as to the length or nature of the correct response.* This may seem elementary, but it happens. Again, this can misinform you about students' real levels of achievement.

*Right:*  In what section of the orchestra is the kettle drum found?  
*Wrong:*  In the percussion section of the orchestra are located , , , and .

**4.** *Put the blank toward the end of the sentence.*

**ASSEMBLING THE TEST.** Begin the test with relatively easier items to maximize students' opportunity to start on a note of confidence. Consider arranging the items on your test according to the learning target each represents, especially if the results will be used formatively. If that presents a challenge in direction writing because you would be mixing item formats, consider indicating in some other fashion which learning target each addresses. However, do so only if this information will not give students unfair clues regarding the correct answer. Make sure your students know how many points each question is worth, so they can learn to prioritize their testing time. Put all parts of an item on the same page.

 **My Classroom Then and Now 5.2**
**Laura Anderson**
We used to . . .

Our fourth-grade team had always grouped for math, written our own unit assessments, and given tests within a similar time frame. Our test questions were not grouped and often either did not have enough or had too many questions. Teachers only received an overall test score and not enough specific data about which skills were not mastered. We also did not collect the data. Even when questions obviously need revisions, it was easier to put the test away until next year. Unfortunately, we did not remember the changes a year later and gave the same test over again. Our old team discussions were based on the logistics of the school day (i.e., schedules, upcoming events, and running of materials).

Now we . . .

New to us was the test format that challenged us to identify our learning targets and group "like questions" together by targets. This new format with a percent score by target really helps teachers and students quickly see which targets are met. Students indicate whether they feel they made a simple mistake or if they need more study.

Why we changed . . .

This feature really holds the kids accountable for their own learning. We saw signs of students becoming more invested in the test-taking process and taking more ownership of their learning. Also, our instruction has become more focused as we started posting learning targets in our classrooms. Our students are now clearer about the learning targets of our daily lessons.

What we notice as a result . . .

The most powerful change we have seen since starting the common assessment process is the team discussion piece after the test data collection. Now, we meet as a team and examine our student data and test questions. When many kids miss a particular test question, we discuss whether we feel it was a test question issue or a teaching issue. If a question needs revising, it is done right away. We have continued to improve our tests over the last three years rather than using the same flawed tests over again. These collegial discussions focusing on test data are like nothing we have ever been a part of. Now, we focus much more of our team time on instructional strategies, remediation ideas, and curriculum/test development. It wasn't always easy to make the time or feel comfortable sharing our weaknesses. But, we have grown so much as teachers and have developed a professional community centered around improving our instruction.

Our standardized math test scores (both state and district level) have shown dramatic improvement after our implementation of the common formative assessment process. We have felt such success that this school year we began designing and using common formative assessments in reading. We are now hopeful and excited to see the same improvement in our reading standardized scores.

*Source:* Used with permission from Laura Anderson, 4th-grade teacher, District 196, Rosemount, MN, 2011.

#### **Step 6: Review and Critique the Overall Assessment for Quality Before Use**

At this step, we determine how much time to allow, adjust the time or the test as needed, and review the assessment for quality.

**DETERMINING TESTING TIME.** We advocate the development and use of "power" tests—this, as opposed to timed tests. We want each student to have time to address each item. If they don't, they will be marked wrong and you will infer nonmastery. But you may well be incorrect—you may be mismeasuring achievement. To prevent this, estimate time required to give the test as planned. Decide if this amount of testing is acceptable—if students in your grade level can reasonably be expected to maintain concentration for that long. If the testing time as planned is unreasonable, modify your test by one of the following means:

- Convert some of the more time-consuming item formats to true/false items.
- Test only some of the learning targets and hold others for a separate assessment later.

**REVIEWING THE ASSESSMENT FOR QUALITY.** There are three aspects of test quality to evaluate: how well it matches the assessment blueprint, how well the items are written, and how well the test avoids other sources of bias.

**Matching the Assessment Blueprint.** To check for the match between your test and its blueprint, you can use the following process:

- **1.** Make a numbered list of the learning targets the test is to cover.
- **2.** Next to each item on your test, write the number of the learning target it measures. Also indicate how many points the item is worth. For example, item number one on your test might measure learning target number five and be worth two points, so next to item number one, you would write 5/2.
- **3.** On the list of learning targets, tally the points each receives on the test.
- **4.** Compare these numbers to the blueprint.

**Ensuring Item Quality.** To evaluate the quality of the items you must do two things: check that each item tests what you intended and verify that each item is wellwritten.

To check that your items test what you intended, you can work backwards to turn them into the propositions you began with. Here's how:

- Combine the multiple-choice item stem with the correct response.
- True true/false items already are propositions.
- Make false true/false items true to derive the proposition.
- Match up elements in matching exercises.
- Fill in the blanks of short answer items.

If the result is a match with the proposition you wrote, then the item will measure what you intended it to.

You can verify that each item is well-written by using the Guidelines for Writing Quality Items and also by auditing each against the requirements of the first five sections of the checklist in Figure 5.6 .

**Avoiding Other Sources of Bias.** Even with a match to learning targets, appropriate sampling, and high-quality items, there are a few things that can still go wrong that will compromise your results. The format of the test may be confusing or misleading, the directions may not be clear, or students may not have enough time to answer all questions. Example 5.3 shows two formats for the same item: Version 1 illustrates a problem with the layout that has been corrected in Version 2. You can check for sources of bias in your test by auditing it against the requirements in the last three sections of Figure 5.6 : "Format," "Directions," and "Time."

![](_page_155_Figure_11.jpeg)

**FIGURE 5.6** Selected Response Test Quality Checklist

**1. General guidelines for all formats**

- Keep wording simple and focused. Aim for lowest possible reading level.
- Ask a question.
- Avoid providing clues within and between items.
- Correct answer should not be obvious without mastering material tested.
- Highlight critical words (e.g., most, least, except, not).

**2. Guidelines for multiple-choice items**

- State whole question in item stem.
- Eliminate repetition of material in response options.
- Be sure there is only one correct or best answer.
- Keep response options brief and parallel.
- Make all response options the same length.
- Limit use of "all" or "none of the above."
- Use "always" and "never" with caution.

**3. Guideline for true/false items**

Make them entirely true or entirely false as stated.

**4. Guidelines for matching items**

- Provide clear directions for the match to be made.
- Keep list of trigger items brief (maximum length is 10).
- Include only homogeneous items.
- Keep wording of response options brief and parallel.
- Provide more responses than trigger items.

**5. Guidelines for fill-in items**

- Ask a question.
- Provide one blank per item.
- Do not make length of blank a clue.
- Put blank toward the end.

**6. Formatting test items**

- Be consistent in the presentation of an item type.
- Keep all parts of a test question on one page.
- Avoid crowding too many questions on one page.
- Arrange items from easy to hard.
- Try to group similar formats together.
- Avoid misleading layout.

**7. Writing directions**

- Write clear, explicit directions for each item type.
- State the point value of each item type.
- Indicate how the answer should be expressed (e.g., should the word true or false be written, or T or F? Should numbers be rounded to the nearest tenth? Should units such as months, meters, or grams be included in the answer?)
- **8. Time**

Make sure the test is not too long for the time allowed.

*Source:* Adapted from *Student-Involved Assessment* for *Learning* , 5th ed. (p. 122 ), by R. J. Stiggins, 2008, Upper Saddle River, NJ: Pearson Education. Copyright © 2008 by Pearson Education, Inc. Adapted by permission of Pearson Education, Inc.

#### **Step 7: Conduct and Score the Assessment 
#### Step 8: Revise as Needed for Future Use**

 Even the best planning can't catch all problems with an assessment. Here are two things to watch for as you administer the test:

- *Students don* ' *t have enough time to complete all test items* . If students don't have the opportunity to attempt each item, their scores will not reflect what they have learned. Watch for students frantically marking items toward the end of the time allowed. Also look for large numbers of incomplete tests.
- *Students are confused by an item* . Make notes on the questions for which students ask clarifying questions.

While correcting the tests, make note of items that caused unexpected difficulties. Use the Guidelines for Writing Quality Items and the checklist in Figure 5.6 to troubleshoot these items as well as the items for which students required clarification and make any revisions as needed for future use.

 **My Classroom Then and Now 5.3**
**Shannon Braun**

I used to …

Testing in my classroom has always followed the traditional model. I would spend a couple of weeks teaching a unit, give a quiz partway through the chapter, and end the unit with a test. I rarely analyzed the results of my quizzes and tests. Instead, I corrected the summative assessments, entered them in my gradebook, and shared the results and class averages with the students. Were there questions that many students missed? What types of errors were students making? By not analyzing my assessments with any detail, I did not always have answers for parents or tutors wondering how they could help their children.

Now I …

After doing some research on formative assessment and gathering ideas from others, I have started to implement some ideas in my own classroom. I now begin with essential targets or goals and guide students towards them. Students use responders to assess progress on certain targets. I am easily able to collect data from the students during the chapter and make spreadsheets to analyze the results. I return corrected summative unit tests with a slip of paper indicating the targets measured on the test. As we go through the test as a class, I match targets with test questions and students record tally marks for each target they missed. I post additional review worksheets for each target on my webpage and give students a week to work on them. I then allow retakes on these targets for students who have completed the extra practice. I also convey information to parents and tutors about the areas in which students need help.

Why I changed …

These practices help both students and me decide what areas they are doing well on and what areas they need to improve. Besides helping students understand their tests better, it helps me improve instruction. Prior to using formative assessment strategies I would give parents a very vague response when they asked what to do to help their children, but now I am able to give parents very detailed guidance.

What I notice as a result …

Using formative assessment in a structured way has transformed the way I look at instruction. I found improvements in student achievement in almost all cases. Communication between myself and students and parents has improved. Other members of my department are also using these strategies and we are now able to compare results and share insight on how we might improve certain targets. We all have different techniques to share!

*Source*:Used with permission from Shannon Braun, high school mathematics teacher, District 196, Rosemount, MN, 2011.

### **SELECTED RESPONSE ASSESSMENT** *FOR* **LEARNING**

As we saw in Chapter 2 , formative assessment can be thought of as actions that involve both the teacher and the student as decision makers. We also saw that motivation to learn and level of achievement both rise when students are engaged in the assessment process. The following suggestions represent ways selected response tests can serve to help students answer the three essential questions at the heart of assessment *for* learning: "Where am I going?"; "Where am I now?"; and "How can I close the gap?" These seven strategies as applied to selected response tests are summarized in Figure 5.7 .

#### **Where Am I Going?**

##### **STRATEGY 1: PROVIDE STUDENTS WITH A CLEAR AND UNDERSTANDABLE VISION OF THE LEARNING TARGET.** O
nce you have an assessment blueprint, you can use it, not just for test development, but also as assessment *for* learning. Give students a list of the learning targets, in terms they understand, at the outset of instruction, along with your assessment blueprint. You can also use the blueprint as a way to summarize the day's or week's learning by asking students to identify which cells the instruction has focused on.

**TABLE- Assessment for Learning with Selected Response Methodology**
| Section              | Description                                                                  |
|----------------------|------------------------------------------------------------------------------|
| Where am I going?    | 1. Provide students with a clear and understandable vision of the learning   |
|                      | target. Write targets in student-friendly language. Share assessment         |
|                      | blueprints at the outset. Have students match propositions with blueprint    |
|                      | cells. Have students generate propositions for each blueprint cell.          |
|                      | 2. Use examples and models of strong and weak work.                          |
|                      | Students identify wrong multiple-choice and fill-in answers and say why.     |
| Where am I now?      | 3. Offer regular descriptive feedback. Use distracters to intentionally      |
|                      | frame corrective feedback. Use the blueprint to provide feedback target by   |
|                      | target on a test.                                                            |
|                      | 4. Teach students to self-assess and set goals.                              |
|                      | Students "traffic light" the blueprint. Students use assessment blueprints   |
|                      | as a basis for evaluation of strengths and areas for study.                  |
| How can I close the  | 5. Design lessons to focus on one learning target or aspect of quality at    |
| gap?                 | a time. Use student-generated information from Strategy 4 to differentiate   |
|                      | instruction.                                                                 |
|                      | Students use item formulas to write items. Students answer question:         |
|                      | How do you know your answer is correct?                                      |
|                      | Students turn propositions into items and practice answering the items.      |
|                      | Students create test items for each cell and quiz each other.                |
|                      | Students use graphic organizers to practice patterns of reasoning.           |
|                      | 6. Teach students focused revision.                                          |
|                      | Students answer the question: How do I make this answer better?              |
|                      | 7. Engage students in self-reflection and let them keep track of and share   |
|                      | what they know. Students track their progress and use that information to    |
|                      | engage in self-reflection:                                                   |
|                      | I have become better at ____. I used to ____, but now I ____.                |


Propositions play a role in assessment *for* learning. Explain to students that a proposition is a statement of important learning and then do one or more of the following:

- Ask students to note what they understand to be the proposition(s) at the center of the day's instruction. Have them keep a list and add to it each day. Help students see how well their lists match your own.
- Give groups of students the assessment blueprint and sample propositions representing the learning thus far. Have them match the propositions to the correct cell in the blueprint.
- Have students create propositions for each cell in the blueprint. Check these for misunderstandings and reteach as needed.

##### **STRATEGY 2: USE EXAMPLES AND MODELS OF STRONG AND WEAK WORK.** 
Give students a reasoning item formula, such as the ones in Figure 5.6 . (For younger students, you will need to translate the item formula into student-friendly language.) Show them a test item created with the formula. Ask them to identify which answers are wrong and which one is right by identifying the response pattern each follows. See the example "Using Item Formulas as Strong and Weak Examples" for an illustration of how this can work when teaching fourth-grade students to infer.

**For Example 5.4**
**Using Item Formulas as Strong and Weak Examples**

Which one of these answers is a good inference, based on the reading selection from *The BFG*? Mark the good inference with a star. The right answer is a good inference because it is a guess based on clues from the story.

- **a.** The BFG could hear extremely well because he could not see very well.
- **b.** The author loved his father very much.
- **c.** The father did not finish high school.
- **d.** The father had a good imagination.
- **e.** The father wanted people to think he was a serious man.
- **f.** The father had a well-developed sense of hearing.
- **g.** The father was a funny person.

Some of these answers are wrong because they are not inferences at all! They are just facts that the story tells you outright. Write the letters of those wrong answers here:

Some of the answers are wrong because, even though they are guesses, there are no clues in the story to support them. Write the letters of those wrong answers here:

Be careful—you might think there is evidence for them, so look closely!

#### **Where Am I Now?**

##### **STRATEGY 3: OFFER REGULAR DESCRIPTIVE FEEDBACK.** 
To offer effective feedback with selected response assessments, you must know what learning target each item addresses. Your students must also understand the targets they are responsible for learning. If you are using multiple-choice items, the distracters students choose can pinpoint problems to address. After taking a test, students can use the assessment blueprint to figure out which learning targets they have mastered and which ones they still need to work on.

##### **STRATEGY 4: TEACH STUDENTS TO SELF-ASSESS AND SET GOALS.** 
Hand your assessment blueprint out at the beginning of instruction. Have students self-assess on the learning targets or concepts as you teach them, using "traffic light" icons. Students mark the learning target or concept with a large dot—green to indicate confidence in having mastered it ("I've got it"), yellow to indicate a judgment of partial mastery ("I understand part of it, but not all of it"), or red to indicate little or no understanding ("I don't get it at all"). Then let the "greens" and "yellows" partner to fine-tune their understanding while you work with the "reds." (Black, Harrison, Lee, Marshall, & Wiliam, 2002).

You can structure a quiz or test to function as both effective feedback and a means for self-assessment and goal setting. This works best if there is opportunity for students to improve their achievement before the assessment that counts for the mark or grade. Here is the process (adapted from Chappuis, 2009, pp. 111–112 ):

- **1.** Identify what each item on the quiz or test measures by filling out the first two columns of the form "Reviewing My Results" shown in Figure 5.8 .
- **2.** Administer the quiz or test, correct it, and hand it back to students, along with the form, "Reviewing My Results."
- **3.** Have students review their corrected quizzes or tests and mark the appropriate column "Right" or "Wrong" for each item.
- **4.** Then have students review the items they got wrong and ask themselves, "Do I know what I did wrong? Could I correct this myself?" If the answer is "Yes," they mark the "Simple Mistake" column. If the answer is "No," they mark the "Don't Get It" column.
- **5.** Hand out the form "Analyzing My Results" shown in Figure 5.9 and have students transfer their results to one (or more) of three categories: "I am good at these"; "I am pretty good at these, but need to do a little review"; and "I need to keep learning these."
- **6.** Last, students make a plan to improve. Figure 5.10 offers two examples of goalsetting frames you might have them use.

Figure 5.11 shows an example of a form designed for secondary students. To use it, you make a numbered list of the learning targets to be assessed and then put only the learning target number on the form. Students use the form while they are taking the quiz or test to mark "Confident" or "Unsure" as they answer each item. After you have corrected the quizzes or tests, students fill in the "Right," "Wrong," "Simple Mistake," and "Don't Get It" columns. Then they combine that information

**FIGURE 5.8** Reviewing My Results
**TABLE-Reviewing my results**
| Name:                                                                                                                                     |
|-------------------------------------------------------------------------------------------------------------------------------------------|
| Please look at your corrected test and mark whether each problem is right or wrong. Then look at the problems you got wrong and decide    |
| if you made a simple mistake. If you did, mark the "Simple Mistake" column. For all the remaining problems you got wrong, mark the        |
| "Don't Get It" column.                                                                                                                    |
| Assignment: _____________ | Date: _____________ |                                                                                         | 
| Problem | Learning Target | Right | Wrong | Simple Mistake | Don't Get It |
|---------|------------------|-------|--------|----------------|---------------|
| 1       |                  |       |        |                |               |
| 2       |                  |       |        |                |               |
| 3       |                  |       |        |                |               |
| 4       |                  |       |        |                |               |
| 5       |                  |       |        |                |               |
| 6       |                  |       |        |                |               |
| 7       |                  |       |        |                |               |
| 8       |                  |       |        |                |               |
| 9       |                  |       |        |                |               |
| 10      |                  |       |        |                |               |

*Source:* Reprinted from *Seven Strategies of Assessment* for *Learning* (p. 112 ), by J. Chappuis, 2009, Upper Saddle River, NJ: Pearson Education. Reprinted by permission.

**FIGURE 5.9** Analyzing My Results
**Analyzing My Results**
**TABLE-Analyzing My Results**
| I AM GOOD AT THESE! Learning targets I got right:                                                                   |
|---------------------------------------------------------------------------------------------------------------------|
| I AM PRETTY GOOD AT THESE, BUT NEED TO DO A LITTLE REVIEW Learning targets I got wrong because of a simple mistake: |
| What I can do to keep this from happening again:                                                                    |
| I NEED TO KEEP LEARNING THESE Learning targets I got wrong and I'm not sure what to do to correct them:             |
| What I can do to get better at them:                                                                                |
|                                                                                                                     |

*Source:* Reprinted from *Seven Strategies of Assessment* for *Learning* (p. 113 ), by J. Chappuis, 2009, Upper Saddle River, NJ: Pearson Education. Reprinted by permission.

**TABLE-Student Goal-Setting Frames**
| FIGURE 5.10 Student Goal-Setting Frames                                                                                             |
|-------------------------------------------------------------------------------------------------------------------------------------|
| To get better at ___________________________________, I could …                                                                     |
| •                                                                                                                                   |
| •                                                                                                                                   |
| •                                                                                                                                   |
| •                                                                                                                                   |
| Some things I am going to start doing are …                                                                                         |
| •                                                                                                                                   |
| •                                                                                                                                   |
| •                                                                                                                                   |
| I'll start doing this on _________________ and work on it until _________________.                                                  |
| (date)                                                                                                                              |
|                                                                                                                                     |
| One way I'll know I'm getting better is …                                                                                           |
| ___________________________________________________________________________________________                                         |
| ___________________________________________________________________________________________                                         |
|_____________________________________________________________________________________________________________________________________|
| Goal                                 | Steps                                 | Evidence                                |
|--------------------------------------|----------------------------------------|----------------------------------------|
| What do I need to get better at?     | How do I plan to do this?              | What evidence will show I've           |
|                                      |                                        | achieved my goal?                      |
| Time Frame: Begin ________________   | End ________________                                                        |
| Date ________________               | Signed ________________                                                      |
| Source: From *Self-Assessment and Goal-Setting*, (p. 45) by K. Gregory, C. Cameron, and A. Davies, 2000, Merville, BC: Connections. |
| Reprinted by permission.                                                                                                            |


along with the "Confident" or "Unsure" information to fill out the form "Analyzing My Results."

#### **How Can I Close the Gap?**

##### **STRATEGY 5: DESIGN LESSONS TO FOCUS ON ONE LEARNING TARGET OR ASPECT OF QUALITY AT A TIME.** 
You can use the information provided by students in reviewing and analyzing their quiz or test results to differentiate instruction, to have students form study groups, or as the basis for general review.

Other focused activities include the following:

- Let students create test items using an item formula. See the example "Studentgenerated Inference Questions" for an illustration of how this might work as a followup to the activity described in Strategy 2.
- Follow each selected response item with the question, "How do you know your answer is correct?," providing several lines for a response. Discuss common reasons for right and wrong choices when you pass back the test.


 **FIGURE 5.11** Reviewing and Analyzing My Results—Secondary Version
**TABLE-Reviewing and Analyzing My Results—Secondary Version**
| Name: ____________________________     | Assignment: ___________________________        | Date: ________________ |
|----------------------------------------|-----------------------------------------------|------------------------|
As you answer each question, decide whether you feel **confident** in your answer or **unsure**, and mark the corresponding box.
| Problem # | Learning Target # | Confident | Unsure | Right | Wrong | Simple Mistake | Don't Get It |
|-----------|-------------------|-----------|--------|-------|--------|----------------|---------------|
| 1         |                   |           |        |       |        |                |               |
| 2         |                   |           |        |       |        |                |               |
| 3         |                   |           |        |       |        |                |               |
| 4         |                   |           |        |       |        |                |               |
| 5         |                   |           |        |       |        |                |               |
| 6         |                   |           |        |       |        |                |               |
My Strengths (Learning Target #)
To identify your areas of strength, write down the learning targets for problems you felt **confident about and got right**.
| Learning Target # | Learning Target or Problem Description |
|-------------------|----------------------------------------|
|                   |                                        |
|                   |                                        |
|                   |                                        |
My Highest Priority for Studying
To determine what you need to study most, write down the learning targets for problems you marked "**Don't Get It**" (problems you got wrong, NOT because of a simple mistake).
| Learning Target # | Learning Target or Problem Description |
|-------------------|----------------------------------------|
|                   |                                        |
|                   |                                        |
|                   |                                        |

What I Need to Review
To determine what you need to review, write down the learning targets for problems you were **unsure of** and for problems on which you made **simple mistakes**.
| Learning Target # | Learning Target or Problem Description |
|-------------------|----------------------------------------|
|                   |                                        |
|                   |                                        |
|                   |                                        |


*Source:* Adapted from *Assessment FOR Learning: An Action Guide for School Leaders* (pp. 198 , 199), by S. Chappuis, R. Stiggins, J. Arter, and J. Chappuis, 2004, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

 **My Classroom Then and Now 5.4**
**Kim Urban**
I used to …

To prepare for tests or quizzes, I used to do a whole-class review (either worksheets or a game). This would mean that students who have mastered a concept reviewed the material for the same amount of time as a student struggling. I also did not separate concepts (learning targets) for the chapter. Rather, students would study for the chapter as a whole. The review was completed in class the day before the test so there was limited opportunity for students to ask for help or identify the areas they needed help with.

Now I …

Two days before the scheduled test (summative assessment), I separate learning targets into "I can" statements and give students a formative assessment. Students must answer the questions (1–2 per learning target) and bring their paper to me. If there are mistakes, I provide immediate feedback and have students correct and bring their paper back to me. When students have shown through the formative assessment that they have mastered a concept, they are able to receive enrichment on those learning targets. Students who have made corrections on a learning target may benefit from further review. Additional practice materials are made available to those students. Students who are unable to make corrections can then sit with me and review in a small group. The next day, students who have mastered the learning targets and feel that they are ready may begin their test. Students who worked with me the previous day and students who are still working on learning targets will be offered additional review individually or with me. They are given the test the next day.

Why I changed …

All students do not need to review the same targets for the same amount of time. The use of formative assessments and separating review into learning targets allows me to differentiate instruction for the students. With 30 or more students in a class, formative assessments provide a quick way to assess the level of understanding of all students before testing, and provide for each student's individual needs.

What I notice as a result …

Students used to be overwhelmed by studying for a "chapter" in math class. Now, I show students the learning targets that they have mastered so they can focus on studying the learning targets that they are struggling with. Even students unwilling to ask for help benefit from formative assessments because they receive the feedback from me and are forced to correct their mistakes. Test scores have improved and the majority of students are willing to ask for help on targets they do not understand.

*Source:* Used with permission from Kim Urban, 6th-grade mathematics teacher, Olmsted Falls School District, Olmsted Falls, OH, 2011.

- Assign groups of students to each cell of your test plan. Have them create questions that might be on the test, based on the propositions they generated for each cell during instruction. Have the groups take each others' practice tests.
- Teach students to use graphic organizers as a means to understand the specific kind of reasoning called for. After doing activities from Strategies 1 and 2, let students create their own graphic organizer for a specific pattern of reasoning.

**Student-generated Inference Questions**
How to Write an Inference Question
Here is a recipe that test writers use to create multiple-choice inference questions and answers.
Question:

*Which idea does this selection suggest?* 

Possible responses

*(these include the right answer and several wrong answers):* 

- The correct response is a guess that is supported by clues in the text.
- One incorrect response is a guess that a quick glance might suggest, but there really isn't enough evidence to support it.
- One incorrect response is a wild guess, because there aren't any clues in the text to support it.

Now it's your turn! First, read the assigned passage in the text. Then, work with a partner to create the right and wrong answers to the inference question below.

Here ' s the inference question:

Which idea does this selection suggest?

You write a correct answer and two incorrect answers. You can mix up the order—the correct answer doesn't have to come first.

- **a.**
- **b.**
- **c.**

##### **STRATEGY 6: TEACH STUDENTS FOCUSED REVISION.** 
Anything we do to give students practice with applying what they know about quality or correctness to rework their own answers or to offer suggestions on someone else's work causes them to engage in revision. Consider letting them first practice on anonymous responses by answering one or both of the following questions: "What is wrong with this answer?" "What would make this answer better?"

##### **STRATEGY 7: ENGAGE STUDENTS IN SELF-REFLECTION AND LET THEM KEEP TRACK OF AND SHARE THEIR LEARNING.** 
We may think of this strategy as best suited to performance assessment, but it is equally effective with selected response assessment. Students should be thinking about their achievement with respect to the knowledge and reasoning targets measured by this method, both because they play an important role in their education, and also because it is these fundamental targets that struggling students often have not mastered. As we stated in Chapter 2 , any activity that requires students to reflect on what they are learning and to share their progress both reinforces the learning and helps them develop insight into themselves as learners. These are keys to enhancing student motivation.

Some software programs have built-in mechanisms for students to track their progress and communicate their results. Students answer a variety of questions and get immediate feedback on how they did. They are able to monitor their own progress and experience the satisfaction of watching themselves improve. Many students come to enjoy the assessment experience, even if they are not wildly successful at first, and are motivated by seeing progress to continue trying.

We share more ways for students to track their learning in Chapter 9 and to share it in Chapters 11 and 12 .

For specific examples of how each of these strategies can be used with selected response assessment across grade levels and subject areas, see Chappuis (2009) *.*

### **Summary**

In this chapter we revisited the idea that selected response items are a good way to measure knowledge and reasoning learning targets, as long as students can read at the needed level to understand the questions. Selected response is an efficient way to cover a lot of content in a short period of time.

Although we reviewed all steps in test development—from planning to critiquing the final product—we focused on creating assessment blueprints, generating propositions to identify important content, and adhering to guidelines for writing high-quality selected response items of all types.

Finally, we offered concrete examples of how to use selected response assessment as assessment *for* learning, meeting both the teacher's and the students' information needs. These strategies focused on how to use items, quizzes, and tests to answer the three questions that define assessment *for* learning: "Where am I going?"; "Where am I now?"; and "How can I close the gap?"

### **NOTES**

 1. Portions of these writing guidelines have been reprinted and adapted from Chapter 5 , pp. 91-119 , of R. J. Stiggins, and J. Chappuis*, Introduction to Student-involved Assessment FOR Learning,* 6th ed., 2011, Upper Saddle River, NJ: Pearson Education. Copyright © 2011 by Pearson Education, Inc. Reprinted and adapted by permission of Pearson Education, Inc.

### **CHAPTER 5 ACTIVITIES**

End-of-chapter activities are intended to help you master the chapter's learning targets. They are designed to deepen your understanding of the chapter content, provide discussion topics for learning team meetings, and guide implementation of the practices taught in the chapter.

Forms for completing each activity appear in editable Microsoft Word format in the Chapter 5 CD file. Documents on the CD are marked with this symbol:

#### **Chapter 5 Learning Targets**

At the end of this chapter, you will know how to do the following:

- **1.** Make a test blueprint for a selected response assessment.
- **2.** Choose from among selected response formats.
- **3.** Create high-quality items.
- **4.** Audit any selected response test for quality.
- **5.** Use selected response assessments to plan further instruction.
- **6.** Use selected response assessments as feedback to students and for student selfassessment and goal setting.

Activity 5.1 Keep a Reflective Journal 
Activity 5.2 Audit Items for Quality 
Activity 5.3 Create a Test 
Activity 5.4 Develop an Assessment *for* Learning Activity Activity 5.5 Prepare Quiz or Test for Formative Use 
Activity 5.6 Reflect on Your Own Learning
Activity 5.7 Select Portfolio Artifacts

##### **Activity 5.1 Keep a Reflective Journal**

Keep a record of your thoughts, questions, and any implementation activities you tried while reading Chapter 5 .

Reflective Journal Form

##### **Activity 5.2 Audit Items for Quality**

This activity has two parts. The first part is an exercise in identifying item problems and the second part is an exercise in applying the Selected Response Test Quality Checklist in Figure 5.6to an existing selected response quiz or test.

**Part 1: Test of Franzipanics**

After reading the section, "Step 5: Develop or Select Items, Exercises, Tasks, and Scoring Procedures," work independently, with a partner, or with a team to review this test of an imaginary subject, "Franzipanics." The correct answers can be guessed at without any knowledge of Franzipanics.

**Directions:** Circle the correct answer for each question. Then identify the item flaw that allowed you to obtain the correct answer without knowing the subject.

- **1.** The purpose of the cluss in furmpaling is to remove
	- **a.** cluss-prags **c.** cloughs
	- **b.** tremalis **d.** plumots
- **2.** Trassig is true when
	- **a.** lusp trasses the vom **c.** the belgo frulls
	- **b.** the viskal fans, if the viskal is donwil or zortil **d.** dissles lisk easily
- **3.** The sigla frequently overfesks the trelsum because
	- **a.** all siglas are mellious **c.** the trelsum is usually tarious
	- **b.** siglas are always votial **d.** no trelsa are feskable
- **4.** The fribbled breg will minter best with an
	- **a.** derst **c.** sorter
	- **b.** morst **d.** ignu
- **5.** Among the reasons for tristal doss are
	- **a.** the sabs foped and the foths tinzed **c.** few rakobs were accepted in sluth
	- **b.** the kredges roted with the orots **d.** most of the polats were thonced
- **6.** The mintering function of the ignu is most effectively carried out in connection with
	- **b.** the groshing stantol **d.** a frally sush
	- **a.** a raxma tol **c.** the fribbled breg
		

![](_page_171_Picture_27.jpeg)

![](_page_171_Picture_29.jpeg)

Test of Franzipanics Answers to Franzipanics Test

**Part 2: Critique a Selected Response Test**

Now select a short selected response quiz or test from your resources. Work independently, with a partner, or with a team to evaluate its quality by following the steps described in the section, "Reviewing the Assessment for Quality."

- **1.** Check for the match between your test and its blueprint. If it doesn't have a blueprint, make one by completing Activity 4.5. You can use one of the blueprint forms from Activity 5.3, also.
- **2.** Evaluate the quality of each item by checking that it tests what you intend it to and verifying that each item is well written. Use the Selected Response Test Quality Checklist ( Figure 5.6 ) to help you with this step.
- **3.** Audit the test against the sources of bias described in the text.
- **4.** Make a revision plan for the test, if needed.

![](_page_172_Picture_8.jpeg)

![](_page_172_Picture_9.jpeg)

Selected Response Test Quality Checklist

##### ** Activity 5.3 Create a Test**

Select a short unit that you are currently teaching or will teach this year. Work independently, with a partner, or with a team to carry out this activity.
**Planning Stage**

**1.** Follow the steps described in the Planning Stage. Use either "Test Blueprint A" or "Test Blueprint B," or modify one to make your own test blueprint.

**Development Stage**

- **2.** Identify the content to be tested by writing propositions following the suggestions in Chapter 5 .
- **3.** Next, determine which item type(s) you will use for each target. Refer to Figure 5.4 "Comparison of Selected Response Item Types" for guidance.
- **4.** Follow the Guidelines for Writing Quality Items to create each type of item you have specified.
- **5.** Assemble the items following the guidelines in the text.
- **6.** Review and critique the test for quality following the instructions in the text. Revise it as needed before use. You can also use the Selected Response Test Quality Checklist to help with this.
- **7.** If you created the test for formative use, use the blueprint to create a second version of it for summative use. If you created the test for summative use, use the blueprint to create a second version for formative use.

![](_page_173_Picture_14.jpeg)

Selected Response Test Quality Checklist

![](_page_173_Picture_16.jpeg)

Test Blueprint Form A Test Blueprint Form B

##### **Activity 5.4 Develop an Assessment** *for* **Learning Activity** 

- **1.** After reading the section, "Selected Response Assessment *for* Learning," select one formative application to try.
- **2.** Develop the materials, use them with students, and note any changes in students' motivation, interest, or achievement.
- **3.** Share the materials you developed and your observations about the effects the activity had on students with a colleague or your learning team.

(Note that directions for the application described in Strategy 4 and illustrated in Figures 5.8 to 5.11 are provided in Activity 5.5.)

![](_page_174_Picture_7.jpeg)

Debrief the AFL Activity You Tried

##### ** Activity 5.5 Prepare a Quiz or Test for Formative Use**

This activity is set up so that both teachers and students can use quiz or test results formatively: teachers to plan further instruction and students to self-assess and set goals for further learning. It has two parts—preparing for formative use and creating the forms students will use.

Begin by choosing a selected response quiz or test you intend to give to students. Work independently or with one or more colleagues who will also give the quiz or test to carry out the two parts of this activity.

**Part 1: Preparing for Formative Use**

At this point, you need to ensure two things. The first is that the items on the quiz or test match exactly to what you have been teaching. The second is that you have planned instructional interventions and built in time for them after the quiz or test is given.

For the first consideration, making sure that each item on the quiz or test matches what you are teaching, if you have developed the test or quiz yourself following Chapter 5guidelines, you will have taken care of the match. If you are using a quiz or test that you did not develop, use Activity 4.4, "Audit an Assessment for Clear Learning Targets" to ensure the match.

For the second consideration, think about how many learning targets you will be assessing as well as their complexity. How much remediation time do you anticipate students will need after the assessment? What kinds of activities will you prepare for review and reteaching? Identify specific resources and activities that could be used for further instruction on each learning target you are assessing. Plan for a mix of independent work and direct instruction to meet different student needs.

**Part 2: Creating the Forms**

You can use one of two versions. Option A, illustrated in Figures 5.8 and 5.9, is the simpler version and Option B, illustrated in Figure 5.11 , is more detailed.

When working with a cumulative-type test, this activity is most powerful as a learning experience if students will have an opportunity to take some version of the test again because it can direct their studying for a retake of the test. Students can follow up their self-analysis with a specific study plan, using a form such as one of those shown in Figure 5.10 .

**Option A**

- **1.** Identify which learning target each item on the quiz or test represents.
- **2.** Fill out the first two columns of the form "Reviewing My Results": *Problem Number* and *Learning Target*. In this version, you write the actual learning target (in student-friendly language) on the form.
- **3.** Give the quiz or test. Correct it as usual and hand it back to students, along with the forms "Reviewing My Results" and "Analyzing My Results."
- **4.** Have students mark whether each problem is right or wrong on the form "Reviewing My Results." Then have them revisit the problems they got wrong to determine the cause. They mark "Simple Mistake" if they know what they did wrong and can correct it without help. They mark "Don't Get It" if they do not understand how to answer the problem correctly.
- **5.** Now have students fill out the form "Analyzing My Results." They begin by listing learning targets they got right. Then they list those with simple mistakes and make a plan to keep from making those mistakes again. Last they list the learning targets that they don't get and write down options for how to get better at them. It's good to have students think about this, but you can also give them suggestions, such as an activity to complete, a section in the text to re-read, or a series of problems that build understanding to work.
- **6.** You can use the students' self-identified "I need to keep learning these" lists of targets to group students for focused instruction.
- **7.** You can also ask students to use the results to complete a goal-setting form.

**Option B**

- **1.** Make a numbered list of the learning targets represented on the test. Write the learning targets in the same student-friendly language you have used when sharing them with students. Save this list.
- **2.** Fill out the first two columns of the form, "Reviewing My Results": *Problem Number* and *Learning Target Number*. For this version, you write the learning target number only, not the learning target itself, on the form, because you will be handing the form out with the test.
- **3.** Copy the form, "Reviewing and Analyzing Results" for each student and hand it out with the quiz or test.
- **4.** As students take the quiz or test, they note on the form whether they feel confident or unsure of the correct response to each item.
- **5.** Correct the tests as usual and hand them back, along with the numbered list of learning targets (from Step One), and the form "Reviewing and Analyzing My Results."
- **6.** Have students mark whether each problem is right or wrong on the first portion of the form. Then have them revisit the problems they got wrong to determine the cause. They mark "Simple Mistake" if they know what they did wrong and can correct it without help. They mark "Don't Get It" if they do not understand how to answer the problem correctly.
- **7.** Now have students fill out the second half of the form to analyze their results. They begin by listing learning targets they got right as their strengths. Then they list the learning targets they don't get as their highest priority for study. Last

they list those with simple mistakes and those they got right, but were unsure of what they need to review.

- **8.** You can use the students' self-identified "I need to keep learning these" lists of targets to group students for focused instruction, such as an activity to complete, a section in the text to re-read, or a series of problems that build understanding.
- **9.** If this is a summative assessment and you are going to offer students a chance to retake it, you may want to have them complete a goal-setting form based on the results of this first assessment.

![](_page_177_Picture_5.jpeg)

Reviewing and Analyzing My Results, Option A Reviewing and Analyzing My Results, Option B

Goal-Setting Form

##### **Activity 5.6 Reflect on Your Own Learning**

Review the Chapter 5learning targets and select one or more that represented new learning for you or struck you as most significant from this chapter. If you are working individually, write a short reflection that captures your current understanding. If you are working with a partner or a team, either discuss what you have written or use this as a discussion prompt for a team meeting.

![](_page_177_Picture_11.jpeg)

Reflect on Chapter 5 Learning

##### **Activity 5.7 Select Portfolio Artifacts**

Any of the activities from this chapter can be used as portfolio entries. Select any activity you have completed or artifacts you have created that will illustrate your competence at the Chapter 5learning targets:

- **1.** Make an assessment blueprint for a selected response assessment.
- **2.** Choose from among selected response formats.
- **3.** Create high-quality items.
- **4.** Audit any selected response test for quality.
- **5.** Use selected response assessments to plan further instruction.
- **6.** Use selected response assessments as feedback to students and for student selfassessment and goal setting.

If you are keeping a reflective journal, you may want to include Chapter 5 's entry in your portfolio.

![](_page_178_Picture_11.jpeg)

Chapter 5 Portfolio Entry Cover Sheet

### **CD RESOURCES**

- 1. Activity 5.1 Reflective Journal Form
- 2. Activity 5.2 Test of Franzipanics
- 3. Activity 5.2 Answers to Franzipanics Test
- 4. Activity 5.2 Test Blueprint Form A
- 5. Activity 5.2 Test Blueprint Form B
- 6. Activity 5.2 Selected Response Test Quality Checklist
- 7. Activity 5.3 Test Blueprint Form A
- 8. Activity 5.3 Test Blueprint Form B
- 9. Activity 5.3 Selected Response Test Quality Checklist
- 10. Activity 5.4 Debrief the AFL Activity You Tried
- 11. Activity 5.5 Reviewing and Analyzing My Results, Option A
- 12. Activity 5.5 Reviewing and Analyzing My Results, Option B
- 13. Activity 5.5 Goal Setting Form
- 14. Activity 5.6 Reflect on Chapter 5 Learning
- 15. Activity 5.7 Chapter 5 Portfolio Entry Cover Sheet
