## Chapter 6:  Written Response Assessment

 *A farmer lost his entire crop. Why might this have happened?*

Student 1: Drought
Student 2: Floods and heavy rains destroyed them.

Drought destroyed them.

The birds ate all the seeds.

The crop was demolished for business construction.

He didn't take proper care of his crops.

He went bankrupt, so was unable to look after his crop.

The soil was unsuitable for growing his crops.

(Arter & Busick, 2001, p. 137 )

Written response assessment acts as a window into students' knowledge, conceptual understanding, and reasoning abilities, but it only provides a clear view if it is well done. At first glance a written response assessment may seem fairly easy to create—what's so hard about writing a question?—yet as the preceding student responses show, we may not get what we expected or hoped for without carefully thought-out questions and plans for scoring the answers.

Written response assessments include *short answer* items and *extended written response* items. Short answer items call for a very brief response having one or a limited range of possible right answers. Extended written response items require a response that is at least several sentences in length, and generally have a greater number of possible correct or acceptable answers. In this chapter, we examine how to develop items of both kinds, how to score them accurately, and how to use written response assessments formatively, as assessments *for* learning.

From Chapter 6 of *Strategies Classroom Assessment for Student Learning: Doing It Right – Using It Well*, Second Edition. Jan Chappuis, Rick Stiggins, Steve Chappuis, Judith Arter. Copyright © 2012 by Pearson Education. All rights reserved.

### **Chapter 6 Learning Targets**

At the end of Chapter 6 , you will know how to do the following:

- Develop short answer items and scoring guides.
- Develop extended written response items and scoring guides.
- Use written response assessments formatively, as teaching tools.
- Structure written response assessments so that students can use the results to self-assess and set goals for further learning.

**FIGURE 6.1** Keys to Quality Classroom Assessment

**Key 1: Clear Purpose** Who will use the information? How will they use it? What information, in what detail, is required?

**Key 2: Clear Targets** Are learning targets clear to teachers? What kinds of achievement are to be assessed? Are these learning targets the focus of instruction?

 **Key 3: Sound Design**

Do assessment methods match learning targets? Does the sample represent learning appropriately? Are items, tasks, and scoring rubrics of high quality? Does the assessment control for bias?

 **Key 4: Effective Communication**

Can assessment results be used to guide instruction? Do formative assessments function as effective feedback? Is achievement tracked by learning target and reported by standard? Do grades communicate achievement accurately?

 **Key 5: Student Involvement**

Do assessment practices meet students' information needs? Are learning targets clear to students? Will the assessment yield information that students can use to self-assess and set goals? Are students tracking and communicating their evolving learning?

![](_page_243_Picture_1.jpeg)

### **WHEN TO USE WRITTEN RESPONSE ASSESSMENT**

Not surprisingly, the first consideration for using written response assessment is the type of learning target to be assessed. Written response assessment is a strong match for knowledge targets, especially when assessing mastery of chunks of knowledge that interrelate, rather than individual pieces of knowledge assessed separately. (Selected response is more efficient for individual pieces of knowledge.) For example, in science, we might want students to explain how atoms combine to form other substances. In social studies, we might want students to describe the factors that led to development of centers of habitation, and why each factor is important. In English, we might want students to explain the difference between connotation and denotation.

Written response is also a strong match for reasoning targets. Reasoning occurs in the mind—we can't "see" it as it's happening, but we can ask students to describe their thinking using a written form of communication. In doing so, we can discover not only what they know, but also how well they understand it. For example, in mathematics we might ask students to explain the process they used to arrive at an answer. In science, we might ask students to explain their rationale for an experiment design.

Through written response, students can show their ability to infer, analyze, compare, determine cause and effect, and evaluate information. In a unit on pollution, for example, we might ask students to determine which solution to a problem is most likely to have the greatest benefit and to explain the reasons for their choice. In social studies, we might ask students to trace the line of an argument in a political speech. In English, we might ask students to interpret the meaning of a metaphor in a poem.

Several other conditions influence the selection of the written response method of assessment. Use it when

- Your students are capable of writing in English. Written response, especially extended written response, probably won't work well for primary students, English language learners, and students with special needs involving English or writing.
- You can't get the information you need through the less time-consuming selected response method.
- You know that the scoring guides are of high quality and that scorers will apply them consistently.

### **THE PLANNING STAGE FOR A WRITTEN RESPONSE ASSESSMENT**

The Planning Stage has four steps: determine who will use the assessment results and how they will use them, identify the learning targets to be assessed, select the appropriate assessment method or methods, and determine sample size.

#### **Determining Users and Uses**

We begin planning by answering these questions: How do we want to use the information? Who else will use it? What decisions will they make? Typically, we will use assessment information for one or more of the following purposes:

- To plan instruction, as with a pretest
- To differentiate instruction based on student needs derived from the assessment
- To offer feedback to students so they can act on it during the learning
- To give students the opportunity to self-assess and set goals for further learning
- To measure level of achievement for a final grade, as with a post-test

Each one of these purposes can be accomplished with written response formats, as long as we keep the intended use in mind while making further planning and design decisions.

#### **Identifying Learning Targets**

At this step we simply list the specific learning targets the assessment is to measure. (If a target is complex or unclear, clarify it or deconstruct it first, following the processes outlined in Chapter 3 .)

#### **Selecting Assessment Method(s)**

Although we have already determined that we will use written response, it's important to make sure we have identified only knowledge and reasoning learning targets as the subject of this assessment and also that the targets can be assessed well with written response methodology. So review the list of learning targets (from Step 2) with this in mind. (Refer to Chapter 4 for an in-depth discussion of which types of targets are best assessed with written response methodology.)

#### **Determining Sample Size**

At this step, we establish priorities. Which of the learning targets or topics are most important, next most important, and so on? This will serve as the basis for the distribution of points or ratings in the overall assessment plan. The prioritization should parallel the amount of time and emphasis given the various targets or topics in teaching.

Remember, when identifying the relative importance of each learning target, we consciously match our emphasis in assessment to our emphasis in the classroom. If, say, we spend 20 percent of the time learning how to trace a line of argument in a political speech, then roughly 20 percent of the assessment points should focus on tracing a line of argument. If only 5 percent of the course deals with tracing lines of argument, then in most cases it would misrepresent learning to devote 20 percent of the final assessment to it.

However, if it's a standards referenced assessment in which we are documenting how students have done in mastering one or more standards of equal importance, then we simply apportion the points to adequately sample each standard.

**COMBINING PLANNING DECISIONS INTO AN ASSESSMENT BLUEPRINT.** We can create a blueprint for a written response assessment in one of two ways—a list of learning targets or a table—as described in Chapter 4 .

A blueprint in the form of a list of learning targets is useful when the assessment will be fairly simple. Example 6.1 shows a blueprint for a written response assessment focusing on selected learning targets from the "Physics of Sound" unit described in Chapter 4 .

 **TABLE - Assessment Blueprint in List Format**                                                                        
**Unit on Physics of Sound**                                                                                           
| **Learning Target**                                                                                       | **Points** |  
|-----------------------------------------------------------------------------------------------------------|------------|   
| Learn that sound originates from a source that is vibrating                                               | 6          |
|  and is detected at a receiver such as the human ear                                                      |            |
| Understand the relationship between the pitch of a sound and the physical properties of the sound source: | 6          |
|  – length of vibrating object                                                                             |            |
|  – frequency of vibrations                                                                                |            |
|  – tension of vibrating string                                                                            |            |
| Use knowledge of the physics of sound                                                                     | 8          |
|  to solve simple sound challenges                                                                         |            |


 A blueprint in the form of a table is useful when the assessment is more complex and will include both knowledge and reasoning learning targets in combination. These plans are similar to those used with selected response assessments, but they differ in how numbers are assigned to each cell. With selected response assessments, the numbers represent how many items will be needed. With written response assessments, the numbers represent the total point value for each cell. The points may come from a single item or more than one item. Example 6.2 shows the table form of a blueprint for a written response test covering a unit on pollution. The teacher has translated the learning targets to be tested into categories of content students are to know and the patterns of reasoning they are to master. The categories of content are represented in the lefthand column and the patterns of reasoning are listed across the top. The numbers in each cell represent the relative emphasis assigned to each.

**TABLE-Assessment Blueprint in Table Format**
|                                      |      | Unit on Pollution    |          |       |
|--------------------------------------|------|----------------------|----------|-------|
| Content                              |      | Pattern of Reasoning |          |       |
|--------------------------------------|------|----------------------|----------|-------|
|                                      | Know | Compare              | Evaluate | Total |
| Concentrations                       | 10   | 0                    | 0        | 10    |
| Effects of Pollutants                | 7    | 8                    | 0        | 15    |
| How to Reduce Pollution              | 6    | 10                   | 9        | 25    |
| Total                                | 23   | 18                   | 9        | 50    |

Given 50 points for the entire exam, this plan emphasizes how to reduce pollution, requiring that students rely on that understanding to compare and evaluate.

### **THE DEVELOPMENT STAGE FOR A WRITTEN RESPONSE ASSESSMENT**

The Development Stage for a written response assessment has three steps: develop the items, prepare the scoring guide or guides, and critique the overall assessment for quality. As described in the following sections, written response items can take one of two forms: short answer or extended response. They can be designed to assess mastery of knowledge and reasoning proficiencies together or separately. Scoring guides can take one of three forms: a list, a task-specific rubric, or a general rubric. The items and scoring guides can be critiqued using the Quality Guidelines for Written Response Assessments ( Figure 6.4 ) and the Rubric for Rubrics (which will be introduced in Chapter 7 and presented in Figure 7.10 ) prior to use.

### **DEVELOPING THE ITEMS**

One of the advantages of written response tests relative to other test formats is that items are easier and less time consuming to develop. Keep in mind, however, that "easier to develop" does not mean they require little thought, as illustrated by

**FIGURE 6.2** Options for Item Design
Short Answer Items
- Require a brief response
- Have one or a limited range of possible right answers
- Can be used for knowledge and some reasoning targets
Extended Written Response Items
- Require a response that is at least several sentences in length
- Have a greater number of possible correct or acceptable answers
- Can be used for knowledge and reasoning targets
Interpretive Items
- Can be either short answer or extended written response in format
- Knowledge provided; students demonstrate reasoning
- Used for reasoning targets

the question about the farmer's crops. If we are not careful at this stage, students who know the material may not perform well, and students who have not mastered the material may be able to look as though they have. Poorly framed written response items can be a nightmare for students to answer and for teachers to score.

#### **Short Answer or Extended Response?**

Use short answer items when the learning target calls for demonstration of conceptual understanding and the concept is fairly narrow, such as the learning target "Understand that the Earth's rotation causes day and night." (A "narrow" concept is not necessarily easy to understand. It just has a straightforward explanation.) Some patterns of reasoning can also be assessed with short answer items. For example, students can summarize the main idea of a paragraph in a sentence or two.

If, however, they are summarizing a longer passage, the response could be one to several paragraphs in length, in which case, extended written response may be a better choice. For learning targets that are more complex and therefore require greater depth in an explanation or demonstration of reasoning capability, develop extended written response items.

#### **Devising Short Answer Items**

A short answer item should be short and clearly worded, while giving students enough information to frame an acceptable response. If you are looking for two examples, three instances, or five characteristics, put that information in the item. Here are some examples:

**Learning target:**

Understand that shapes in different categories (e.g., rhombuses, rectangles, and others) may share attributes (e.g., having four sides). (Grade 3 Mathematics) (CCSSI 2010c, p. 26 )

This:* 
Name four ways that rhombuses, rectangles, and squares are alike.
*Not this:* 
How are rhombuses, rectangles, and squares alike?
*Also not this:* 
What do rhombuses, rectangles, and squares have in common?

**Learning Target:**
Understand that human capital refers to the quality of labor resources, which can be improved through investments. (Grade 4 Social Studies) (Council for Economic Education, 2010, p. 3 )

*This:* 
Define *human capital* and give two examples of how you can improve your own human capital.
*Not this:*  What is human capital?
**Learning Target:**
Describe characters, settings, and major events in a story, using key details. (Grade 1 Reading) (CCSSI, 2010a, p. 11 )
*This:* 
Who is the main character of the story? What is the main character like? Give two details from the story that help you know this.
*Not this:* 
Describe the main character.

If you are working with students who do not write sentences yet, you can ask written response questions orally. You can also ask them to draw a picture to supplement the answer, if a picture will help show their thinking. If you are requiring a picture, make sure your item asks for a picture that will show their thinking and not just illustrate the topic.

**Learning Target:**
Understand that day and night are caused by the earth's rotation. (Grade 2 Science)

*This:* 
Everyone knows about day and night. Write what you think makes day and night. Draw a picture to show what you think.
Not this:* 
Explain day and night.
*And not this:* 
Everyone knows about day and night. Write what you think makes day and night. Draw a picture of day and night.

#### **Devising Extended Written Response Items**

We have all experienced "essay" questions on tests, some of which may have been painful. "Discuss photosynthesis." "Analyze *King Lear.* " "Explain the causes of the Civil War." High-quality extended written response items, in contrast to these, carefully frame the task so that students who have learned the material know how to tackle it.

**ITEMS ASSESSING KNOWLEDGE MASTERY.** Extended written response items that assess factual and conceptual knowledge do three things: (1) set a clear and specific context, (2) indicate what students are to describe or explain, and (3) point the way to an appropriate response without giving away the answer.

 To assess the learning target "Understand the importance of the carbon cycle and how it works," we could create this item:

*1. Set the Context*

We have been studying the importance of the carbon cycle and how it works.

In this example, the context is stated as a paraphrase of the learning target. With extended written response questions, especially if they are included in a test with other items, it helps students to be reminded of the specific body of knowledge they are to use when framing their responses.

*2. Tell What to Describe or Explain*

Based on your understanding of the carbon cycle, describe why we need to know about it and how it works.

The task they are to carry out is stated in the second part—with a knowledge or conceptual understanding learning target, students are generally explaining or  describing something. Notice this sentence doesn't just say "Describe the carbon cycle," for two reasons. First, that would not be sufficient to assess the learning target, and second, it doesn't give enough guidance—describe what? If you want students to describe how it works, make sure the item lays out that expectation.

*3. Point the Way to an Appropriate Response*

Be sure to include the following:

- Why it is important to understand the carbon cycle (5 points)
- The four major reservoirs where carbon is stored (4 points)
- At least six ways that carbon gets transferred from one place to another (6 points)

The third part helps students know what will be considered an appropriate and complete response; students who know the material will be able to answer well, and students who don't know it won't be able to bluff their way through.

**ITEMS COMBINING KNOWLEDGE MASTERY WITH REASONING.** Extended written response items that ask students to reason with knowledge they have learned are similar in structure to items that assess knowledge mastery. They also have three components, slightly altered to account for the addition of reasoning: (1) set a clear and specific context; (2) specify the kind of reasoning to be used; and (3) point the way to an appropriate response without giving away the answer.

For example, to assess the sixth-grade learning target, "Explain how an author develops the point of view of the narrator or speaker in a text," (CCSSI 2010a, p. 36 ), the Common Core State Standards document offers this extended written response task as a possibility: "Students explain how Sandra Cisnero's choice of words develops the point of view of the young speaker in her story 'Eleven'" (CCSSI, 2010b, p. 89 ).

Applying our item frame to the task it might look like this:

*1. Set the Context*

We have been studying "point of view"—what it means and how to identify it in a story.

*2. Describe the Reasoning Task*

After reading the story "Eleven," explain how the author uses word choice to show the point of view of the young speaker.

*3. Point the Way to an Appropriate Response*

Choose at least three examples. Make sure you explain what the young speaker's perspective is and how each example shows that.

(Written response items should also include information about how they will be evaluated, whether with a list of a number of points or with a rubric. Both options are described in the next section.)

As another example, to assess the learning target, "Evaluate opposing positions on humankind's role in global climate change," we could create this item:

*1. Set the Context*

There are those who contend that global climate change is a naturally occurring phenomenon and others who contend it is caused by the actions of humans.

*2. Describe the Reasoning Task*

Analyze the evidence we have studied to support each claim. Decide whom you think has the stronger argument. Defend your judgment with reasons.

*3. Point the Way to an Appropriate Response*

In doing so, consider the evidence from geologic history, the history and levels of emissions, and the political and economic interests of each side. Here are two simpler examples:

- Explain the mathematics formula we studied today in a memo to a student who was absent.
- Teach younger students how to read a contour map by creating a list of instructions accompanied by diagrams and/or illustrations.

**DEVISING INTERPRETIVE ITEMS.** Interpretive items allow you to assess mastery of specific patterns of reasoning disentangled from student mastery of the prerequisite content knowledge. As you will recall from Chapter 4 we do this by supplying a passage, table, chart, or map of background information about a given topic and then asking students to write a response demonstrating the targeted pattern of reasoning: describe certain relationships, draw comparisons, conduct analyses, or create and fill categories, for example.

Interpretive items also do three things: (1) set the context, (2) describe the reasoning task, and (3) point the way to an appropriate response without "over helping."

To assess the learning target "Summarize text," we could create this item:

*1. Set the Context*

We have been learning how to write a summary—a brief statement of the main ideas of a text.

*2. Describe the Reasoning Task*

After reading (the assignment provided), write a paragraph that summarizes the main ideas.

*3. Point the Way to an Appropriate Response*

In your paragraph, be sure to do the following:

- focus only on the main ideas (2 points)
- include enough information to cover all of the main ideas (2 points)

*or*

Your paragraph will be evaluated with the Summary Rubric, attached.

**FAQ 6.1 **Analyze This**
Question:

*Some learning targets beginning with the word* analyze *seem to require more than*  " examining the components or structure of something." *How do we know what to teach and assess?* 

Answer:

When a learning target begins with the word *analyze,* it also often requires students to do something with the components or structure. You will have to think through the whole target to determine the full extent of the reasoning required. Basically, you have to **analyze** an *analyze* target to know what to teach and assess.

For example, a sixth-grade Common Core State Standard in Reading states:

Analyze in detail how a key individual, event, or idea, is introduced, illustrated, and elaborated in a text (e.g., through examples or anecdotes). (CCSSI, 2010a, p. 39 )

In this case, we would suggest that to *analyze* here means this: *Explain how a key individual is introduced, illustrated, and elaborated and support the explanation with examples or anecdotes from the text.*

#### **Offering Choices**

Typically, we recommend that you don't offer choices where the choices don't all provide comparable evidence of the same learning target. Regardless of the assessment format being used, especially in summative assessment contexts, the question should always be, "Can you hit the agreed-on target(s)?" It should never be, "Which (or which part of the) target are you most confident that you can hit?" or "Which target are you most interested in?"

When students select their own sample of performance, it can be a biased one when it avoids revealing that which they have yet to master. If what they don't know is essential for mastering what comes next, we won't know it and they won't learn it. However, in a formative assessment environment, we might relent somewhat because,

**FIGURE 6.3** Scoring Guide Options for Written Response Items

Task-specific list—a list of possible correct answers or desired features of the response along with information about how points will be awarded

Task-specific rubric—a rubric describing features of quality as they appear in a single item or task

General rubric—a rubric describing the features of quality as they apply across items or tasks

hopefully, key gaps in learning will be revealed in recurring diagnostic assessments down the road.

### **PREPARING THE SCORING GUIDES**

Again not surprisingly, a key to successful use of written response assessment is having clear and appropriate criteria by which to judge the quality of student responses. We recommend that you don't use "floating standards," in which the evaluator waits to see what responses come in before deciding what will count and thus how to score. Floating standards destroy the validity and reliability of the assessment. Teachers and students alike need to be clear in advance regarding which aspects of the response are important—this is as essential to a quality assessment as is thoughtful item development. In this section, we explain three scoring guide options and describe how to create each.

#### **Scoring Guide Options**

A *scoring guide* is simply a way to assign points to specific features of a response. Three types of scoring guides are appropriate for assessing written response items: lists, task-specific rubrics, and general rubrics.

**LISTS.** A scoring guide in list form identifies the possible correct answers or desired features of the response and specifies how points will be awarded. Use a list when the desired answer has several parts and each one represents a specific category of knowledge or reasoning with knowledge.

For an item asking students to cite instances where Spanish literature and politics may have influenced each other in the twentieth century, the scoring guide might look like this:

3 points for each instance cited, maximum 9 points
Quality of inferences about prominent novelists
Quality of inferences about political satirists
Quality of inferences about prominent political figures of Spain

For an item asking students to explain the Krebs cycle, the scoring guide might look like this:

One point for each of the following, maximum five points:
Cycle describes the sequence of reactions by which cells generate energy
Cycle takes place in the mitochondria
Cycle consumes oxygen
Cycle produces carbon dioxide and water as waste products
Cycle converts ADP to energy-rich ATP
For an item requiring students to explain how carbon moves from one place to another, the scoring guide might look like this:

One point for any six of the following, maximum six points:

Carbon moves from the atmosphere to plants through photosynthesis.
Carbon moves from the atmosphere to oceans by dissolving in places it is cold.
Carbon moves from the oceans to the atmosphere by evaporation where it is hot.
Carbon moves from land to the atmosphere through fires.
Carbon moves from land to the atmosphere through volcanic eruptions.
Carbon moves from land to the atmosphere through burning fossil fuels.
Carbon moves from the land into the oceans through erosion.
Carbon moves from plants/animals to the ground/sediments through decay.

For Example 6.3 shows an extended written response item and its scoring list.

**Creating Lists.** Because a scoring list is simply a description of specific information along with its point value, it is fairly easy to create one. Notice in the last two examples the correct responses are propositions, which we explained in Chapter 5 . One good way to create a list of correct responses is to follow the instructions for proposition development found in that chapter. Another form of list useful for scoring is one that describes the characteristics of a correct response. For Example 6.3 shows a mathematics extended written response item scored by a task-specific list following this pattern.

**Sharing Lists.** Because the item should communicate to students what is most important to attend to in a quality response, it should include information about how their work will be scored. In the first "Spanish literature and politics" example, we suggest including the scoring list as written. However, in the second and third examples, the scoring list is the answer key, so we would not recommend including it in the item. Instead,

**For Example 6.3 Extended Written Response Item Scored by a List**
**"Label the Graph" (Grade 3)**
Content Standard: Draw a scaled bar graph to represent a data set with several categories (CCSSI 2010c, p. 25 ).
**Learning Targets:**
Know how to make equal-interval scales on axes Interpret data from a bar graph
**Extended Written Response Item:**
Bar graphs can be used to compare things. This graph has four bars. What might the bars be comparing?
- **1.** Put numbers and labels on the graph to show what you mean.
- **2.** Give your graph a title that helps us know what it is comparing.

![](_page_255_Picture_11.jpeg)

**3.** Write down four comparisons that are true for the data your graph shows. (Ten lines are provided.)
**Scoring List**
8 points total
- 1 point: Each bar on the X axis has label below it.
- 1 point: Labels identify categories that can be compared.
- 1 point: The Y axis is marked with an equal-interval scale.
- 1 point: The title accurately communicates the comparison.
- 1 point for each accurate comparison listed (4 points)

for the second "Krebs cycle" example, the item might conclude with the statement, "5 points possible, one for each key feature described correctly." For the third " carbon cycle" example it might look like this: "6 points possible, one for each correct explanation."

**RUBRICS.** A scoring guide in the form of a *rubric* is a detailed description of the features of work that constitute quality. In a rubric, the features of quality are described at different levels, representing a continuum of "novice" to "proficient" or "weak" to "strong." The levels can be labeled with words, symbols, or numbers. With a rubric, the level is the score.

There are two basic forms of rubrics: *task-specific* and *general* . A *task-specific* rubric describes the features of quality as they appear in a single item or task. For Example 6.4 shows a rubric used to evaluate a task given to assess the learning target "Interpret data from a graph." It can only be used to score one item, so it is called a *task-specific* rubric. A *general* rubric describes the features of quality as they apply across items or tasks. Example 6.5 shows a rubric that can be used to score any item that tests students'

ability to interpret information from a graph. It is *general* because it is *generalizable* across tasks.

We recommend the use of general rubrics over task-specific rubrics whenever practical for several reasons:

Task-specific rubrics can't be handed out to students in advance because they give away the "answer."
You have to create a new one for each assignment.

With task-specific rubrics, it's easy to award points to features that are idiosyncratic to the specific assignment and not essential to the accomplishment of the learning target.

However, there are times when a task-specific rubric makes sense to use.

**For Example 6.4 Task-specific Rubric for Interpreting a Graph**

This rubric is an example of a task-specific rubric. It can only be used on one task, the math problem "Going to School" (not included here).
**4 points:**
Has all five points on the graph labeled correctly. Indicates that Graham and Paul ride their bikes to school. Provides accurate and complete explanation of how those conclusions were drawn. Indicates that Susan walks, and Peter takes a car as a part of the explanation.
**3 points:**
Has four or five points on the graph labeled correctly. Indicates that Graham and Paul ride their bikes to school. Explanation of reasoning is correct but incomplete and requires interpretation. May indicate that Susan walks and Peter takes a car as a part of the explanation.
*Remember from Chapters 3 and 4 that when the characteristics of effective writing are evaluated, we would classify the learning as a product target and evaluate it with a performance assessment.* 
**2 points:**
Has three points on the graph labeled correctly. Indicates that Graham or Paul ride a bike. Chooses the incorrect mode of transportation for the other. Explanation of reasoning is partially correct, but also includes faulty reasoning. Explanation may indicate that information about Susan or about Peter was interpreted incorrectly.
**1 point:**
Has one or two points on the graph labeled correctly. Indicates that Graham or Paul ride a bike. Chooses the incorrect mode of transportation for the other. Explanation of reasoning, if offered, is incomplete and incorrect. Explanation may ignore information about Susan and Peter or have interpreted it incorrectly.
**0 points:**
Has no points on the graph labeled correctly. Indicates incorrect mode of transportation for Graham and Paul. Explanation of reasoning, if offered, is faulty.

**For Example 6.5 General Rubric for Interpreting a Graph**

This is an example of a general rubric. It can be of use to judge responses to all problems requiring interpretation of a graph.
**4 points:**
Interprets information from graph to provide correct answers. Provides accurate and complete explanation of how conclusions were drawn.
**3 points:**
Interprets information from graph to provide correct answers. Provides accurate explanation of how conclusions were drawn, but explanation is incomplete and requires interpretation.
**2 points:**
Interprets information from graph to provide partially correct answers. Provides partially accurate explanation of how conclusions were drawn, but includes faulty reasoning. Explanation may also be incomplete and require interpretation.
**1 point:**
Interprets information from graph to provide partially correct answers. Explanation, if offered, consists of faulty reasoning and does not support correct conclusions drawn.

### **0 points:**

Provides incorrect answers. Explanation, if offered, represents faulty reasoning.

#### **Creating Task-Specific Rubrics**

Task-specific rubrics are appropriate for assessing conceptual understanding. An example of a second-grade learning target for which we might create a task-specific rubric is, "Understand that the Earth's rotation causes night and day."

To write a task-specific rubric, we refer back to selected response methodology for the "bones." First we create a proposition—a sentence that accurately states the conceptual understanding—and from that we identify statements representing partial understanding, misunderstanding, and lack of understanding. These become our rubric levels.

Here is the process illustrated with the rubric for the item "Day and Night" shown in For Example 6.6 :

**1.** Create a proposition—a sentence that accurately states the conceptual understanding.
**Proposition:**
"Night and day happen because the earth turns so that the same side is not always facing the sun."
**2.** Identify typical characteristics of partial understanding. You can also include misconceptions that aren't egregious; that is, they don't contradict the central understanding.
**Statements of partial understanding:**
"Night and day happen because the moon and sun are on different sides of the Earth."

"The Earth rotates facing the sun and then the moon."

(These are partial understandings because the Earth's rotation does cause day and night, but facing the moon is not a factor. The misunderstanding does not contradict the explanation of Earth's rotation being the central cause.)

**3.** Identify typical characteristics of misunderstanding or lack of understanding. Also identify any egregious misconceptions that contradict the central understanding, which we call "fatal flaws."
**Statement of misunderstanding:**

"Night and day happen because the sun moves across the sky."
(This is also the "fatal flaw.")

**4.** Determine how many levels the rubric will have.
```
 Levels:
"2," "1," and "0"
```
**For Example 6.6 Short Answer Item Scored by a Task-specific Rubric "Day and Night" (Grade 2)**

**Learning Target:**
Understand that the Earth's rotation causes day and night.
**Item:**
Everyone knows about day and night. Write what you think makes day and night. (Four lines are provided.)
Draw a picture to show what you think. (A 5 -5-inch box is provided.)

**Scoring Guide:**
**2** The response indicates that the Earth turns so that the same face is not always facing the sun.
*Example: "The Earth turns every 24 hours and for 12 hours we are facing the sun."* 
**1** The response indicates that the moon and sun are on different sides of the Earth and the Earth rotates facing one and then the other. There is no implication that the sun moves.
*Example: "In the day we face the sun and in the night we turn to face the moon."* 
**0** The response indicates that the sun moves to cause night and day (possibly across the sky).
*Example: "The sun moves and makes way for the moon ."* 
Essay 3: "Day and Night" reprinted from *Exemplary Assessment Materials* – *Science* (p. 15), by Australian Council for Educational Research, Ltd., 1996, Hawthorn, Victoria, Australia.
For many task-specific rubrics, it makes sense to have three levels. You can use this formula to create one:

2 points = The response indicates _____________ *insert statement(s) showing complete understanding*
1 point = The response indicates _____________\_ *insert statement(s) showing partial understanding*
0 points = The response indicates _____________\_ *insert statement(s) showing lack of understanding, complete misunderstanding, or partial understanding with a "fatal flaw"*

Notice that in the "Day and Night" rubric in Figure 6.6 an explanation can demonstrate partial understanding and also include a misunderstanding and still earn partial credit, but if it includes reference to the sun moving, it earns no credit because that is considered a "fatal flaw" in understanding the concept. A fatal flaw wipes out partial credit.

A three-level ("2," "1," and "0") rubric is a formula for awarding two, one, or no points. It can be used for a short answer or an extended response item, if the conceptual understanding is not too complex.

For conceptual understanding with more variables, it may make sense to have four levels, following this formula:

3 points = The response indicates _____________\_ *insert statement(s) showing complete understanding.*
2 points = The response indicates _____________\_ *insert statement(s) showing partial understanding. You may also include a statement of simple misunderstanding at this level.*
1 point = The response indicates _____________\_ *insert statement(s) showing partial understanding with some misunderstandings, but no "fatal flaw."*
0 points = The response indicates _____________ *insert statement(s) showing lack of understanding, misunderstanding, or partial understanding with the inclusion of a "fatal flaw."*

A four-level ("3," "2," "1," and "0") rubric is a formula for awarding three, two, one, or no points. It is best used with extended written response items. However, if you find yourself stretched to differentiate clearly between the two-point level and the onepoint level, you may want to combine them into a three-level ("2," "1," and "0") rubric.

**FAQ 6.2 Rubric Levels** 

Question:

*Sometimes I see rubrics that have five levels but only the level 5, the level 3, and the level 1 are defined. What's the difference between that kind of rubric and the threelevel rubric defined here?* 

Answer:

The first difference is that the five-level rubrics do have five distinct levels and the three-level rubrics only have three distinct levels. Five-level rubrics are commonly used for more complex learning targets with multiple scales, such as the Six-trait Writing Rubric. It has a separate scoring scale for six different criteria, called *traits*: Ideas and Content, Organization, Voice, Word Choice, Sentence Fluency, and Conventions. Each of the traits has its own five-level scale. Within each trait, multiple facets are described at the "five" level, the "three" level, and the "one" level. For example, the trait of Organization includes phrases at each of the three levels that describe these things: the quality of the introduction, the sequence of ideas, transitions, pacing, and the conclusion. Students could be doing well on some of those and not others. So the rubric is designed to let you award a "four" or a "two" to those papers that have some of the characteristics of each of the adjacent levels. It keeps us from having to assign a rubric score that doesn't match the work in front of us. Chapter 7provides further explanation of these kinds of rubrics.

#### **Creating General Rubrics**

Although we often use task-specific scoring, either in the form of lists or rubrics, for assessing student understanding of content, it is not our only option. We can also use a general rubric designed to assign points to student understanding of any concepts within a body of knowledge.

To create a general rubric for conceptual understanding, we can use the task-specific rubric formula itself. Instead of including the content-related statements of understanding, partial understanding, misunderstanding, or lack of understanding, we leave the descriptions in general terms:

2 points = Evidence shows complete understanding
1 point = Evidence shows partial understanding, with no serious misunderstandings
0 points = Evidence shows lack of understanding, complete misunderstanding, or partial understanding with a "fatal flaw"

Or
3 points = Evidence shows complete understanding
2 points = Evidence shows partial understanding with few, if any, simple misunderstandings
1 point = Evidence shows partial understanding with some misunderstanding, but no "fatal flaw"
0 points = Evidence shows lack of understanding, misunderstanding, or partial understanding with the inclusion of a "fatal flaw"

If we are using interpretive items to assess reasoning alone, then all we need is a general rubric for the pattern of reasoning we are assessing, such as the Rubric for a Generalization shown in For Example 6.7 and the Rubric for Analysis shown in For Example 6.8. Additional rubrics designed to evaluate inference, classification, comparison, synthesis, and evaluation can be found on the CD in the Chapter 6 file.

**For Example 6.7 *Rubric for a Generalization**
**2 points**
• Statement is true for evidence presented and extends application logically to a broader array of instances.
**1 point**
• Statement is true for evidence presented, but application to other instances includes too broad an array of instances to be supported by evidence ( overgeneralization).
**0 points**
- Statement is true for evidence presented, but application to other instances is flawed.
- Statement is true for evidence presented, but no application to other instances is attempted.
- Statement is not true for evidence presented.

**For Example 6.8 Rubric for Analysis**

Definition: *Analysis* is the examination of the components or structure of something to determine how the parts relate to each other or how they come together to form a whole.
![](_page_263_Picture_1.jpeg)
*Source:* Adapted from *Introduction to Student-Involved Classroom Assessment*, 6th ed. (p. 53 ), by R. Stiggins and J. Chappuis, 2011, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

The situation is different if we're also scoring how well students reason with the knowledge given in the item. In this case, we need both a way to assess content knowledge and a way to assess the pattern of reasoning. For example, if the extended written response item calls for students to make a generalization based on content, you might use a task-specific list to assess content understanding, and then a general rubric to analyze the quality of the generalization, such as the one shown in For Example 6.7 . The process for starting from scratch to develop general rubrics is explained in Chapter 7 .

We recommend general rubrics for assessing the quality of various patterns of reasoning because they are versatile:

- You can give them to students in advance to help them understand what high quality looks like.
- You can use them to give students feedback on practice work, highlighting strengths and areas for improvement.
- Students can self-assess with them while practicing, evaluating their own strengths and areas for improvement.
- Students can offer peer feedback with them.
- You can use them again and again, for any task calling for the pattern of reasoning.

**My Classroom Then and Now 6.1**
**Kelly Dye**
I used to . . .

On tests and quizzes, I would give an extended response or short answer question and assess it based on my idea and criteria of what a 4-point, 3-point, 2 -point , 1-point, and 0-point response would be. I would score the students' work and give it back with an attached rubric. We would discuss it briefly and I would attach comments as to what was well done and what needed improvement in their answers.

Now I . . .

The students learn how to answer extended response and short answer questions by practicing, and by assessing each other's work. I show strong and weak models of the different answers and we discuss what criteria are needed to achieve the different point values. Students then have a chance to practice answering questions on their own. I scan them into the electronic whiteboard anonymously and we as a class assess them. This can be done in pairs, groups or a whole class.

Why I changed . . .

I feel that student involvement is a key piece in having them understand and focus on the learning target. I believe that students needed to have a more active role in their learning and academic growth, as well as their weaknesses. When they take ownership and responsibility, they are more likely to achieve and feel confident in their work.

What I notice as a result . . .

Students have a clearer picture of the learning target and have much more success when answering extended response and short answer questions. Frequent practice of scoring and repeated exposure to strong and weak examples allows them to better assess their own work. This has led them to more accurately answering these types of questions not only in my classroom, but also on the state test.

*Source:* Used with permission from Kelly Dye, 6th-grade mathematics teacher, Olmsted Falls City Schools, Olmsted Falls, OH, 2011.

### **CRITIQUING THE OVERALL ASSESSMENT FOR QUALITY**

An excellent way to check the quality of your items is to try to write or outline a high-quality response yourself. If you can, you probably have a properly focused item. If you cannot, it needs work. Or you can ask a colleague to write a response and discuss the item and its scoring guide to see if either needs revision.

If your scoring guide will take the form of a general rubric, you can use the Rubric for Rubrics ( Figure 7.10 in Chapter 7 ) to check it for quality.

Remember that there can be sources of bias specific to extended written response assessment. Figure 6.4 summarizes the factors to think about when devising extended written response items and scoring procedures. Answering these questions assists in constructing effective, high-quality items—those that avoid bias and distortion.

### **THE USE STAGE**

The Use Stage has two steps: conduct and score the assessment and revise as needed for future use.

#### **Conducting and Scoring the Assessment**

Even the best planning can't catch all problems with an assessment. Here are two things to watch for as you administer the test:

 • Students have enough time to complete their responses. If students don't have the opportunity to attempt each item, their scores will not reflect what they have learned. Watch for large numbers of incomplete items.

**FIGURE 6.4** Quality Guidelines for Written Response Assessments
**Quality of the Items**
- Is written response the best assessment method for this learning target?
- Do items call for focused responses?
- Is the knowledge to be used clearly indicated?
- Is the reasoning to be demonstrated (if any) clearly indicated?
- Is the item itself written at the lowest possible reading level—will all students understand what they are to do?
- Will students' level of writing proficiency in English be adequate to show you what they know and can do?
- Is there anything in the item that might put a group of students at a disadvantage regardless of their knowledge or reasoning level?
- Are there enough items to provide a defensible estimate of student learning on intended targets?

**Quality of the Scoring Guide(s)**
- For the knowledge aspect of the response, is it clear how points will be awarded? If a task-specific rubric is used, does the item clearly call for the features described in the highest level of the rubric?
- For the reasoning portion of the response (if any), does the rubric capture the essence of highquality thinking at the highest level? Does it identify flaws in reasoning at the lower levels?
- Does the scoring guide sufficiently represent the intent of the learning target(s)?

 **Scoring Considerations**
- Is the total number of items to be scored (number of items on the assessment times number of students responding) limited to how many the rater(s) can accurately assess within a reasonable time?
- If the scoring guide is to be used by more than one rater, have raters worked together to ensure consistent scoring?
- Students ask for clarification of an item. Make notes on the items for which more than one or two students ask for clarification. Clarify the directions or the item itself for the next time you use it.

#### **Revising for Future Use**

While correcting the tests, make note of items that caused unexpected difficulties. After you conduct, score, and interpret the assessment, if it has flaws you will see them clearly and can then correct them before future use. You also will notice where any instruction may have fallen short on particular targets, which allows you to reteach your current students and revise your plans for instruction next year.

**My Classroom Then and Now 6.2 Michele Buck**
I used to . . .
I used to show students examples of work after a summative assessment.
Now I . . .
Now I do an activity we call "Scoring Camp for Kids." First, the students complete math problems relating to a specific learning target using real-life problems. Upon completion of the pretest the student and teacher use student models to create a rubric to score the student answers. Finally, the kids rate the models while discussing the strong and weak points for each math problem. When the "Scoring Camp for Kids" lesson is complete the students attempt and score additional practice problems.
Why I changed . . .
I changed my teaching focus to include formative assessments and clear learning targets. I found that many of my students questioned why I was taking off points on the extended response questions on the chapter tests. A few students did not even attempt the essay questions because they were not able to make a connection between a math skill and how it relates to a real-life math story.
What I notice as a result . . .
Now my students understand how to read a question and determine what they need to include in their answer in order to get a perfect score. Most importantly, the students now know why points are taken off of their score. Clear learning target instruction directly impacts student achievement, because my students are earning higher scores on summative assessments.

*Source:* Used with permission from Michele Buck, 6th-grade mathematics teacher, Olmsted Falls City Schools, Olmsted Falls, OH, 2011.

### **WRITTEN RESPONSE AS ASSESSMENT** *FOR* **LEARNING**

As we have seen, student motivation and achievement both improve when we use the assessment process to help students answer the following three questions: "Where am I going?"; "Where am I now?"; and "How can I close the gap?" Here are some suggestions for assessment *for* learning practices with written response formats.

• Engage students in devising practice items like those that will appear on a future test. This will help them learn to center on important content and to become
sufficiently comfortable with patterns of reasoning that they can build them into practice items. If they write practice items, trade with classmates, and write practice responses, both we and they gain access to useful information on what parts of the standards they are and are not mastering.
- Provide students with practice items and see if they can place them in the proper cells of the test plan. Then have them defend their placement.
- Design written response assessments so they function as feedback to students. Let students self-assess on the basis of results. Have them set goals for further learning based on their assessments of what they have mastered and what they still need to learn.
- Make use of general, but not task-specific, rubrics as instructional tools. Students can't use task-specific lists or rubrics while developing their responses because they outline the exact answers. However, they can use general rubrics, such as those for patterns of reasoning, to guide their responses. We offer more suggestions for using rubrics formatively at the end of Chapter 7 .
- Provide sample items and let students practice scoring each other's responses to those items. By repeating this process as they proceed through a unit of study, students can watch themselves improve.
- Give students a list of misconceptions and design instruction to address the misconceptions. As students can correct each misconception, they date the list and write the correct understanding (see Example 6.9 ).

**My Classroom Then and Now 6.3 Jeff Overbay**

I used to . . .
In the past I would use the combined curriculum document as a guide for teaching. I never felt confident that I was covering the necessary content or that I was being effective in the classroom. It was more of a topical approach to teaching content. Having objectives has always been a part of my teaching but having clear learning targets for both myself and my students was something that seemed to always be out of reach.
Now I . . .
I now use the deconstructed standards to design a student- friendly self-assessment of the learning targets. These are broken down into knowledge, reasoning, and skills targets.

| Knowledge Targets: "What do I need to know?                                                                                            |
| Yes 1 . ______ | No ______ | 1. I can give examplesof adaptations that allow organisms to survive their environment.                             |
| Reasoning Targets: "What can I do with what I know"                                                                          |
| 1.______       | ______    | 1. I can use modelsto show how energy flows through an ecosystem (food chains, food webs, and energy pyramids) . |

**Why I changed . . .**
The use of learning targets ensures assessment accuracy. The targets are much clearer and guide the day-to-day learning.
**What I notice as a result . . .**
Students now know where we are going and can self-assess along the way. This process helps students to quickly recognize areas that they are struggling with. These templates can also be used as a guide to create more accurate assessments. They actually become the guide for designing the assessments.
*Source :* Used with permission from 7th/8th-grade science teacher Jeff Overbay, Bell County School District, Pineville, KY, 2011.

**For Example 6.9 Correcting Misconceptions**

| Misconception | Date | Correction |
|---------------|------|------------|
| 1.            |      |            |
| 2.            |      |            |
| 3.            |      |            |

*Source:* Reprinted from *Seven Strategies of Assessment* for *Learning* (p. 133 ) by J. Chappuis, 2009, Upper Saddle River, NJ: Pearson Education. Reprinted by permission.

 For more examples of how each of these strategies can be used with written response assessment across grade levels and subject areas, see Chappuis (2009).

### **Summary**

Written response assessments are excellent for assessing conceptual understanding, extended bodies of knowledge, and reasoning learning targets. We followed the creation of these assessments through eight steps, with an in-depth focus on the development stage. Items need to specify what knowledge and patterns of reasoning, if any, students are to use in developing their response. They also need to indicate what features of performance will count, by pointing the way to the correct answer without giving away the answer. Items must avoid other potential sources of bias and distortion such as unclearly written instructions, instructions at too high a reading level, and features that might disadvantage any group.

Scoring procedures and guides must be developed along with the items. We explored three options: the list, the taskspecific scoring guide, and the general rubric. The first two are most typically used to call out content knowledge that must be present in a correct response, while the third is useful for evaluating patterns of reasoning. We offered examples of general rubrics, as well.

We concluded with suggestions for strategies that use written response items as assessment *for* learning, where students share the item development and scoring responsibility. These strategies connect assessment to teaching and learning in ways that can maximize both students' motivation to learn and their actual achievement.

### **CHAPTER 6 ACTIVITIES**

End-of-chapter activities are intended to help you master the chapter's learning targets. They are designed to deepen your understanding of the chapter content, provide discussion topics for learning team meetings, and guide implementation of the practices taught in the chapter.

Forms for completing each activity appear in editable Microsoft Word format in the Chapter 6 CD file. Documents on the CD are marked with this symbol:

#### **Chapter 6 Learning Targets**

At the end of Chapter 6 , you will know how to do the following:

- **1.** Develop short answer items and scoring guides.
- **2.** Develop extended written response items and scoring guides.
- **3.** Use written response assessments formatively, as teaching tools.
- **4.** Structure written response assessments so that students can use the results to self-assess and set goals for further learning.

Activity 6.1 Keep a Reflective Journal Activity 6.2 Evaluate a Written Response Assessment for Quality Activity 6.3 Create a Short Answer Item and Scoring Guide Activity 6.4 Create an Extended Written Response Item and Scoring Guide Activity 6.5 Apply an Assessment *for* Learning Strategy Activity 6.6 Reflect on Your Own Learning Activity 6.7 Select Portfolio Artifacts

##### **Activity 6.1 Keep a Reflective Journal**

Keep a record of your thoughts, questions, and any implementation activities you tried while reading Chapter 6 .

Reflective Journal Form

#### **Activity 6.2 Evaluate a Written Response Assessment for Quality**

Work independently, with a partner, or with a team to carry out this activity. It helps if those you are working with are familiar with the content being assessed.
- **1.** Select an already-developed assignment or assessment that includes short answer and extended written response items from a unit you have taught or will teach. It can be one that you or another teacher created, or one that comes from district or published materials.
- **2.** Write out the answers to each short answer and extended written response on the assignment or assessment yourself. Or trade assessments with a colleague and answer each others' assessment items. Score your answers with whatever scoring guide is provided. Note any difficulties either in answering items or in using the scoring guide.
- **3.** Review the guidelines for developing short answer and extended written response items. Then use the checklist version of Figure 6.4 , "Quality Guidelines for Written Response Assessments," to check each item and scoring guide for quality.
- **4.** If this activity reveals flaws in one or more items, revise them and rewrite them.
- **5.** If this activity reveals problems with one or more scoring guides, revise them and rewrite them.

Evaluate a Written Response Assessment for Quality

Checklist of Quality Guidelines for Written Response Assessments

##### **Activity 6.3 Create a Short Answer Item and Scoring Guide**

Work independently, with a partner, or with a team to complete this activity. It helps if those you are working with are familiar with the content being assessed.
- **1.** After reading the section, "Short Answer or Extended Response?" select one or more learning targets for which short answer items are a good match.
- **2.** Follow the guidelines in the section, "Devising Short Answer Items" to write one or more short answer items to assess the learning target(s) you selected.
- **3.** Follow the recommendations for when to use which type of scoring guide option in the section, "Scoring Guide Options" to select a scoring guide option (list, task-specific rubric, or general rubric) for each item.
- **4.** Follow the instructions for creating the type(s) of scoring guide(s) you selected.
- **5.** Use the checklist version of Figure 6.4 , "Quality Guidelines for Written Response Assessments," to check your item(s) for quality. Revise as needed.
- **6.** Ask a colleague to answer your item(s) and then score the response(s) using the scoring guide(s) you have created. Discuss any difficulties in responding or scoring.
- **7.** Revise your item(s) and scoring guide(s) again as needed.

Template for a Short Answer Item and Scoring Guide

Checklist of Quality Guidelines for Written Response Assessments

##### **Activity 6.4 Create an Extended Written Response Item and Scoring Guide**

Work independently, with a partner, or with a team to complete this activity. It helps if those you are working with are familiar with the content being assessed.
- **1.** After reading the section, "Short Answer or Extended Response?" select a learning target for which an extended written response item is a good choice.
- **2.** Follow the guidelines in the section, "Devising Extended Written Response Items" to write the item.
- **3.** Follow the recommendations for when to use which type of scoring guide option in the section, "Scoring Guide Options" to select a scoring guide option (list, task-specific rubric, or general rubric) for each item.
- **4.** Follow the instructions for creating the type(s) of scoring guide(s) you selected.
- **5.** Use the checklist version of Figure 6.4 , "Quality Guidelines for Written Response Assessments," to check your item(s) for quality. Revise as needed.
- **6.** Ask a colleague to answer your item(s) and then score the response(s) using the scoring guide(s) you have created. Discuss any difficulties in responding or scoring.
- **7.** Revise your item and scoring guide(s) again, as needed.

Templates for Extended Written Response Items and Scoring Guides
Checklist of Quality Guidelines for Written Response Assessments

##### ** Activity 6.5 Apply an Assessment** *for* **Learning Strategy**

Work independently, with a partner, or with your team to complete this activity.

- **1.** After reading the section, "Written Response as Assessment *for* Learning," choose one of the suggestions to try in your classroom.
- **2.** Create the materials needed to carry out the suggestions.
- **3.** Try the suggestion with your students.
- **4.** If you are working with a partner or team, discuss one or more of the following:
	- What did you do? What did you notice happening with students as a result?
	- How might the activity benefit student learning?
	- Would you conduct the activity again?
	- What, if anything, would you change?

Debrief the AFL Strategy You Tried

##### **Activity 6.6 Reflect on Your Own Learning**

Review the Chapter 6learning targets and select one or more that represented new learning for you or struck you as most significant from this chapter. If you are working individually, write a short reflection that captures your current understanding. If you are working with a partner or a team, either discuss what you have written or use this as a discussion prompt for a team meeting.

Reflect on Chapter 6 Learning

##### **Activity 6.7 Select Portfolio Artifacts**

Any of the activities from this chapter can be used as portfolio entries. Select activities you have completed or artifacts you have created that will illustrate your competence at the Chapter 6learning targets:

- **1.** Develop short answer items and scoring guides.
- **2.** Develop extended written response items and scoring guides.
- **3.** Use written response assessments formatively, as teaching tools.
- **4.** Structure written response assessments so that students can use the results to self-assess and set goals for further learning.

If you are keeping a reflective journal, you may want to include Chapter 6 's entry in your portfolio.

![](_page_275_Picture_9.jpeg)

Chapter 6 Portfolio Entry Cover Sheet

### **CD RESOURCES**
- 1. Activity 6.1 Reflective Journal Form
- 2. Activity 6.2 Evaluate a Written Response Assessment for Quality
- 3. Activity 6.2 Checklist of Quality Guidelines for Written Response Assessments
- 4. Activity 6.3 Template for a Short-answer Item and Scoring Guide
- 5. Activity 6.3 Checklist of Quality Guidelines for Written Response Assessments
- 6. Activity 6.4 Templates for Extended Written Response Items and Scoring Guides
- 7. Activity 6.4 Checklist of Quality Guidelines for Written Response Assessments
- 8. Activity 6.5 Debrief the AFL Strategy You Tried
- 9. Activity 6.6 Reflect on Chapter 6 Learning
- 10. Activity 6.7 Chapter 6 Portfolio Entry Cover Sheet
- 11. Figure: Rubric for Inductive/Deductive Inference
- 12. Figure: Rubric for Comparison
- 13. Figure: Rubric for Classification
- 14. Figure: Rubric for Synthesis
- 15. Figure: Rubric for Evaluation