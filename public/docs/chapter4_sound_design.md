## Chapter 4: Sound Design
*Varying assessment methods to give students practice or to accommodate learning styles is a thoughtful consideration. However, assessment methods are not interchangeable. To ensure accurate assessment results, the overriding criterion for selection of method is consideration of the type of learning targets to be assessed.* 

S o far, we have examined two keys to assessment quality, clear purpose and clear targets. The first key, *Clear Purpose* , asks that we identify at the outset who will use assessment results and how they will use them. The second key, *Clear Targets* , asks that we identify the knowledge, reasoning, skill, and product learning targets that will be the focus of instruction. Now we consider the third key to classroom assessment quality—how to design assessments that align with our targets and serve our purposes.

In this chapter we describe four assessment methods, explain how to choose which method to use for any given learning target, and outline the steps in assessment planning and development. We explain each of the four assessment methods in depth in Chapters 5 through 8 ; in this chapter we offer an overview with an emphasis on selecting the proper method and on thoughtful assessment planning.

### **Chapter 4 Learning Targets**

At the end of this chapter you will know how to do the following:

- Select the appropriate method(s) to assess specific learning targets.
- Follow the steps in the Assessment Development Cycle.
- Create an assessment blueprint.
- Use an assessment blueprint with students as assessment *for* learning.

From Chapter 4 of *Strategies Classroom Assessment for Student Learning: Doing It Right – Using It Well*, Second Edition. Jan Chappuis, Rick Stiggins, Steve Chappuis, Judith Arter. Copyright © 2012 by Pearson Education. All rights reserved.

**FIGURE 4.1** Keys to Quality Classroom Assessment

![](_page_96_Figure_2.jpeg)

### **ASSESSMENT METHODS—A SET OF FOUR OPTIONS**

Throughout your school career, both as a student and as a teacher, you have encountered thousands of different assessments. Although the variations are endless, all of the assessments you have experienced and give today fall into one of four basic categories of methods:

- **1.** Selected response
- **2.** Written response
- **3.** Performance assessment
- **4.** Personal communication

All four methods are legitimate options, but only when their use is closely matched with the kind of learning target to be assessed and the intended use of the information.

#### **Selected Response**

*Selected response assessments* are those in which students select the correct or best response from a list provided. Formats include the following:

- Multiple choice
- True/false
- Matching
- Fill-in-the-blank questions

 Students' scores on selected response assessments are usually figured as the number or proportion of questions answered correctly.

 How to develop and use selected response items is the focus of Chapter 5 , "Selected Response Assessment."

**FIGURE 4.2** Assessment Methods
**Selected Response**

- Multiple choice
- True/false
- Matching
- Fill in-the-blank questions

**Written Response**

- Short answer items
- Extended written response items

**Performance Assessment**

- Performance task
- Performance criteria

**Personal Communication**

- Questions during instruction
- Interviews and conferences
- Participation
- Oral exams
- Student journals and logs

#### **Written Response**

*Written response assessments* require students to construct an answer in response to a question or task rather than to select the answer from a list. They include *short answer* items and *extended written response* items. Short answer items call for a very brief response having one or a limited range of possible right answers. Extended written response items require a response that is at least several sentences in length. They generally have a greater number of possible correct or acceptable answers.

Examples of short answer items:

- Describe two differences between fruits and vegetables.
- List three causes of the Spanish-American War.
- What will happen if this compound is heated? Why will that happen?

Examples of extended written response items:

- Evaluate two solutions to an environmental problem. Choose which is better and explain your choice.
- What motivates (the lead character) in (a piece of literature)?
- Interpret polling data and defend your conclusions.
- Describe a given scientific, mathematical, or economics process or principle.

We judge correctness or quality of written response items by applying one of two types of predetermined scoring criteria. One type gives points for specific pieces of information that are present. The other type takes the form of a rubric, which describes levels of quality for the intended answer.

Example of the "points" approach: When students in a biology class are asked to describe the Krebs cycle, points might be awarded for including the following information:

- The cycle describes the sequence of reactions by which cells generate energy.
- It takes place in the mitochondria.
- It consumes oxygen.
- It produces carbon dioxide and water as waste products.
- It converts ADP to energy-rich ATP.

Example of the "rubric" approach: when students in an environmental science class are asked to evaluate two solutions to an environmental problem, their responses might be judged using these three dimensions: the criteria used for comparison, the accuracy of evidence brought to bear, and the strength of the argument for the supremacy of one over the other.

How to develop and use short answer and extended written response items and scoring procedures is the focus of Chapter 6 , "Written Response Assessment."

#### **Performance Assessment**

*Performance assessment* is assessment based on observation and judgment. Even though it is called *performance* assessment, this method is used to judge both

real-time performances, also called *demonstrations* , and products, or *artifacts,* that students create. It has two parts: the task and the criteria for judging quality of the response. Students complete a task—give a demonstration or create a product—that is evaluated by judging the level of quality using a rubric.

Examples of demonstrations (reflecting *skill* targets) include the following:

- Playing a musical instrument
- Carrying out the steps in a scientific experiment
- Speaking a foreign language
- Reading aloud with fluency
- Repairing an engine
- Working productively in a group

Examples of products (reflecting *product* targets) include:

- Term paper
- Lab report
- Work of art
- Wood shop creation
- Geometric solid

The criteria used to judge the demonstration or product can award points for specific features that are present, or it can describe levels of quality. For example, to assess the ability to carry out a process such as threading a sewing machine, doing long division, or safely operating a band saw, points might be awarded for each step done correctly and in the correct order. Level of achievement will be reported by the number or percent of points earned.

For more complex processes or products, you might have a scoring rubric for judging quality that has several criteria. In the case of evaluating an oral presentation, your rubric might cover four criteria: content, organization, presentation, and use of language. For a task requiring mathematical problem solving, the rubric might consist of these four criteria: analysis of the problem, reasoning processes and strategies used, communication, and accuracy. Level of achievement will be reported in terms of rubric levels (generally a number).

How to develop and use performance tasks and rubrics is the focus of Chapter 7 , "Performance Assessment."

#### **Personal Communication**

Gathering information about students through *personal communication* is just what it sounds like—we find out what students have learned through structured and unstructured interactions with them. Examples include the following:

- Asking questions during instruction
- Interviewing students in conferences
- Listening to students as they participate or perform in class
- Giving examinations orally
- Having students keep journals and logs

Because these kinds of classroom assessments lead to immediate insights about student learning, they can reveal misunderstandings and trigger timely corrective action. This is why we usually think of them as formative, rather than summative assessments. As long as the learning target and criteria for judging response quality are clear, information gathered via personal communication can be used either way. It can serve as the basis for instructional planning, for feedback to students to guide next steps, and for student self-assessment and goal setting. If the event is planned well and recorded systematically, the information can also be used as part of the final grade.

Student responses in personal communication assessments are evaluated in one of two ways. Sometimes the questions we ask require students to provide a simple, short answer, and all we're looking for is whether the answer is correct or incorrect. This is parallel to scoring for short answer written response questions. Other times, our questions generate longer and more complex responses, which we can evaluate with scoring criteria. This is parallel to scoring for extended written response questions.

 **FAQ 4.1 Assessment Methods**

Question

*What about portfolios? I notice they aren't listed as a method. Where do they fit in?* 

Answer

Portfolios can be a powerful aid to learning and we devote Chapter 11to their use. However, a portfolio is not an assessment method, but a vehicle for teachers and students to use to track, reflect on, and communicate about achievement. Typically, a portfolio contains a collection of evaluated work, each piece of which is the result of an assignment or task. The individual pieces represent responses to some form of assessment—selected response, written response, performance, or personal communication—but the portfolio itself is the *repository* of evidence, not the *stimulus* that produced its ingredients. So although a portfolio includes an array of assessments and plays a valuable role in assessment *for* learning, it is not an assessment method itself.

Question

*What about exhibitions of mastery, group projects, worksheets, posters, brochures, PowerPoint® presentations, and the other ways that students show their achievement? Aren't they methods of assessment?* 

Answer

All of these performances and artifacts can be classified within one of the four basic assessment methods described. Exhibitions of mastery and group projects usually take the form of extended written response, performance assessment, or personal communication depending on how they are carried out. Worksheets contain various types of items, most frequently selected response or written response questions. Posters and brochures are generally tasks assigned in the context of performance assessment. If a content standard calls for students to develop and use a PowerPoint® presentation, there are two separate learning targets at work—the creation of the slides themselves and the skillful use of them in a presentation. Both would be assessed using a performance assessment.

### **MATCHING ASSESSMENT METHODS TO LEARNING TARGETS**

The accuracy of any classroom assessment depends on selecting the appropriate assessment method that matches the achievement target to be assessed. Acceptable matches result in accurate information gathered as efficiently as possible. Mismatches occur when the assessment method is not capable of yielding *accurate* information about the learning target. Figure 4.3 summarizes when to use each assessment method. (Dispositional learning targets are not addressed because in this book, we focus our assessment information only on academic achievement targets.)

As you read through Figure 4.3 , note that the descriptions of the matches are described as *Strong* , *Good* , *Partial* , and *Poor* . Here is what each means.

**Strong:** The method works for all learning targets of this type.

**Good:** The method works for many of the learning targets of this type.

**Partial:** The method works in some instances for learning targets of this type.

**Poor:** The method never works for learning targets of this type.

#### **Assessing Knowledge Targets**

Knowledge targets represent the factual information, procedural knowledge, and conceptual understandings that underpin each discipline.

**SELECTED RESPONSE.** This is labeled a **good match** in Figure 4.3 because selected response options do a good job of assessing mastery of discrete elements of knowledge,
**TABLE-Target Method Match**
|                | Selected Response        | Written Response           | Performance Assessment       | Personal Communication                    |
|----------------|--------------------------|----------------------------|------------------------------|-------------------------------------------|          
| Knowledge      | Good                     | Strong                     | Partial                      | Strong                                    | 
|                | Can assess isolated elem-| Can assess elements of     | Can assess elements of know- | Can assess elements of knowledge and      |
|                | nts of knowledge and some| knowledge and relationships| ledge and relationships among| relationships among them                  |
|                | relationships among them | among them                 | them in certain contexts     |                                           |
| Reasoning      | Good                     | Strong                     | Partial                      | Strong                                    |
|                | Can assess many but not  | Can assess all reasoning   | Can assess reasoning targets | can assess all reasoning targets          | 
|                | all reasoning targets    | targets                    | in the context of certain    |                                           |
|                |                          |                            | tasks in certain             |                                           | 
| Skill          | Partial                  | Poor                       | Strong                       | Partial                                   |
|                | Good match for some meas-| Cannot assess skill level; | Can observe and assess skills| Strong match for some oral communication  |
|                | urement skill targets;not| can only assessprerequisite| as they are being performed  | proficiencies; not a good match otherwise |
|                | a good match otherwise   | knowledge and reasoning    |                              |                                           |
| Product        | Poor                     | Poor                       | Strong                       | Poor                                      |
|                | Cannot assess the quality| Cannot assess the quality  | Can directly assess the      | Cannot assess the quality of a product;   |
|                | of a product; can only   | of a product; can only ass-| attributes of quality of     | can only assess prerequisite knowledge    |
|                | assess prerequisite      | ess prerequisite knowledge | products                     | and reasoning                             |
|                | knowledge and reasoning  | and reasoning              |                              |                                           |

J. Chappuis, 2011, Upper Saddle River, NJ: Pearson Education. Adapted by permission.

such as important history facts, spelling words, foreign language vocabulary, and parts of plants. These assessments are efficient in that we can administer large numbers of questions per unit of testing time and so can cover a lot of material relatively quickly. It is easy to obtain a sufficient sample of student knowledge from which to draw a confident conclusion about level of overall knowledge acquisition.

**WRITTEN RESPONSE.** Written response is a **strong match** for knowledge targets. It is especially useful for assessing blocks of interrelated knowledge and conceptual understanding, such as causes of environmental disasters, the carbon cycle in the atmosphere, how one mathematical formula can be derived from another, or the concept of checks and balances in government. Not only can we determine if students know the correct answer, but we can also get at how students know, thus minimizing the chances of a right answer for the wrong reasons. Written response assessment is not as efficient as selected response in sampling broad domains of content because response time is longer. So, if time is limited or fixed, the assessment will include few exercises. But the tradeoff is the potential to get at deeper levels of knowledge and conceptual understanding.

**PERFORMANCE ASSESSMENT.** Performance assessment is a **partial match** for assessing knowledge targets. First we'll consider when it can be a good match. Then we'll explore the potential problems that make it a partial match at best.

It is a good match with primary students and with students who cannot read or write. To assess their acquisition of knowledge targets, we rely heavily on observation and judgment—performance assessment—as well as personal communication. Selected response and written response are obviously not viable choices for knowledge learning targets if students cannot yet read or write at a level that would allow them to show what they know.

In all other instances, it is a good match only if the student performs well. If we pose a performance task that asks a student to rely on the knowledge and reasoning to display a skill or create a product that meets certain standards of quality and the student does well, then we can draw the strong conclusion that the student was, in fact, a master of the prerequisite knowledge needed to be successful. However, because we can't be sure of the outcome in advance, we do not recommend that you use performance when the object is to assess solely mastery of knowledge. Three major barriers—accuracy, efficiency, and practicality—contribute to our recommendation.

**Accuracy.** A poor performance may not be the result of lack of knowledge. The key question is, Why did the student not perform well? Was it due to the lack of prerequisite knowledge? Failure to reason well using that knowledge? If it was a demonstration-based performance assessment, was the problem inadequate skills? If it was a product-based performance assessment, was the poor performance due to a problem with creating the product? For example, let's say we assign a complex performance, such as writing and executing a computer program, and let's say our learning target is student mastery of prerequisite knowledge. When a student's program works well, we can conclude she possesses the prerequisite knowledge. The problem comes in when the program does not run successfully. Because of factors beyond the prerequisite knowledge that could have contributed to the failure, we can't know that lack of prerequisite knowledge is the reason for failure. We will

have to do some followup probing to find out if the prerequisite knowledge was there to start with. If our objective is to assess mastery of specific knowledge, to save time and increase accuracy, we are better off using selected response or written response assessments.

**Efficiency.** It is an extravagant use of time to rely on performance assessment to assess all content knowledge. A single performance task does require some subset of knowledge, and you can assess its presence with a particular performance task, but how many performance tasks would you have to create, administer, and score to cover all the knowledge you want students to acquire?

**Practicality.** It isn't always practical, or in some cases safe, to conduct certain performance assessments to assess knowledge. For example, if you want to assess students' ability to read bus schedules, although it would be most "authentic" to ask students to get around town on the bus, it would be highly inefficient and perhaps dangerous. Asking students to answer multiple-choice or short answer questions requiring understanding of a bus schedule would be a more efficient and safer way to get the information needed.

 For these reasons we recommend as a general rule of thumb that you assess knowledge with a simpler method, when possible, and reserve performance assessment for those learning targets that really require it.

**PERSONAL COMMUNICATION.** Personal communication is a **strong match** with knowledge targets for most students at all grade levels. While for summative uses it tends to be inefficient if a lot of knowledge is to be assessed, recorded, and reported for lots of students, it works well in formative applications, such as real-time sampling of student understanding during instruction. Additionally, for some students such as those with special needs, English language learners, or younger students, it may be the only way to gather accurate information.

#### **Assessing Reasoning Targets**

Reasoning targets specify thought processes students are to learn to do well within a range of subjects—solve problems, make inferences, draw conclusions, form judgments.

**SELECTED RESPONSE.** Selected response is a **good match** for reasoning targets. A common misunderstanding is that selected response questions can tap only knowledge targets and can't assess reasoning proficiency. Selected response is not a good choice for all patterns of reasoning, but can be effective for some. For example:

- Which of the following statements best describes how dogs in real life are different from the dog in the story? (Comparative reasoning)
- What generalization can you make from this selection about how these plants lure their prey? (Inference—generalizing)

- Which answer best explains the author's purpose in writing this story? (Inference—determining author's purpose)
- Choose the sentence that best tells what the story is about. (Inference—identifying main idea)
- Which problem-solving strategy is the best choice for this problem? (Evaluation)

There are limits to selected response formats when assessing reasoning. If you want to assess how well students can select a strategy and work it through to completion to solve a problem requiring several steps, how well they can explain their choice or reasoning process, or how well they can defend an opinion, you must use another assessment method. For example, you might ask students to solve the following problem in mathematics: "Estimate the number of hours of TV advertising the typical U.S. fifth grader watches in a year. Describe the process you used to determine your answer." This is an extended written response question. If the learning target you want to assess falls into the category of student reasoning, a single number as the right answer is not the focus of the assessment—competence with the reasoning process is. So in instances such as these, you will need the deeper evidence of thinking that written response reveals.

**WRITTEN RESPONSE.** Written response represents a **strong match** for assessing reasoning targets. The trick here is to pose good questions, ones that require students to analyze, compare, contrast, synthesize, draw inferences, and to make an evaluative judgment. The criteria used to determine student scores must include the quality of each student's application of the pattern of reasoning in questions as well as the accuracy and appropriateness of the information or evidence brought to bear. These criteria were mentioned in Chapter 3 and will be detailed in Chapter 6 .

Also, remember from Chapter 3 that to assess a student's ability to reason well, the question has to pose a novel problem (new to the student) to be solved at the time of the assessment. If students worked on the answer to the question during instruction, and that very question appears on a subsequent assessment, their answers are likely to represent a piece of remembered knowledge, which does not require reasoning.

**PERFORMANCE ASSESSMENT.** This is a **partial match** for assessing reasoning targets, for the same reasons as with performance assessment and knowledge targets. We can, for example, observe students carrying out science laboratory procedures and draw strong conclusions about their reasoning based on our observations if they succeed at the performance assessment. However, if they don't do well, it could be due to lack of prerequisite knowledge, lack of technique (skills), or to imprecise reasoning. In situations such as these, without engaging in additional assessment, we remain unable to judge level of achievement on reasoning targets.

As another example, students are sometimes asked to create a diorama in a shoebox as a reading assignment. This is a task we would assign only if it elicits evidence of specific reading learning targets. We have to be careful of assessing reading

comprehension with a shoebox for the reason that it's fun. If the diorama project can be made to yield solid evidence of identifying main idea and supporting details, summarizing, determining cause and effect, or whatever reading comprehension targets are the focus of instruction, then it can be a match. If not, it's not good assessment.

**PERSONAL COMMUNICATION.** For gathering accurate information, personal communication is a **strong match** to reasoning targets. Teachers can ask students questions to probe more deeply into a response. Or, students can demonstrate their solution to a problem, explaining their reasoning out loud as they go. The drawbacks with using personal communication to assess reasoning proficiency are the amount of time it takes and the record-keeping challenge it poses.

 **FAQ 4.2 Target–Method Match**
Question

*To accommodate student learning styles, and/or to adhere to the call for " multiple measures," shouldn't I be using the widest variety of assessment methods possible?* 

Answer

In all contexts and cases, the driving force behind the selection of an assessment method must be matched to the learning target. If more than one method can work and you wish to have other factors such as student preference or learning style come into play, that's fine. But our goal always is to generate accurate results so we and our students can make sound decisions that advance learning.

Question

*Shouldn't we only be using "authentic" assessments—performance assessments—to judge student progress?* 

Answer

It is somewhat of a misnomer to label one assessment method as "authentic," and thereby imply that the others are "inauthentic" and therefore inferior. None of these methods is inherently superior to any other, and all are viable if used well. Good assessment means clearly knowing what it is you want to assess and then choosing the best method to get the job done, which depends foremost on the kinds of learning targets being assessed. That is the point of the target–method matrix, which is grounded in principles of sound measurement. We would never advocate giving up accuracy to achieve authenticity, which happens when performance is the only acceptable method to use. However, within performance assessment methodology, authenticity is to be desired. We discuss the attribute of authenticity of performance tasks in Chapter 7 .

#### **Assessing Skill Targets**

Skill targets are those where a demonstration or physical skill-based performance is at the heart of the learning.

**SELECTED RESPONSE.** Selected response is a **partial match** for skill targets. It is a **good match** only in a very limited number of cases. When the learning target calls for measuring with tools, there is a degree of manual dexterity involved and, although technically it is a skill target, we can evaluate it through selected response methodology. For example, we can construct a multiple-choice item to test whether a student measures with a ruler or a protractor correctly. We can present a drawing of a measuring cup containing liquid and showing the meniscus and ask students to determine the correct measurement.

Beyond those limited cases, selected response is a poor match for skill targets. We can use it to determine if students possess the prerequisite knowledge required to perform skillfully, but it cannot be used to judge the level of performance. As an example, assessing with a multiple-choice test whether a student can play his cornet clearly will not work. Also, in the measurement examples, we cannot diagnose problems easily, so if our intent is formative, we may want to watch students while they are measuring and correct their procedures as needed, which is performance assessment.

**WRITTEN RESPONSE.** Written response is also a **poor match** for skill targets, for the same reasons. Assessing with a written response whether a student can pronounce words correctly in Japanese will not yield accurate information about the student's proficiency. No one would think of doing that, yet if we only use selected response and written response methodology and our curriculum includes skill targets, we are not capturing the whole picture of student achievement.

**PERFORMANCE ASSESSMENT.** There is really only one assessment method that is a **strong match** for skill targets, and that is performance assessment. For example, we can determine whether students know how to conduct themselves during a job interview using another assessment method, but the only way to evaluate how well they can do it is to watch and listen to them during a simulated job interview and then judge their level of competence.

**PERSONAL COMMUNICATION.** Personal communication is a **partial match** for assessing skill targets. It is a good choice when the skills in question fall into the category of oral proficiency, such as speaking a foreign language or giving an oral presentation. In these instances, personal communication *is* the focus of the performance assessment. When the skill target in question is *not* related to oral proficiency, such as "dribbles a basketball to keep it away from an opponent," personal communication won't do.

#### **Assessing Product Targets**

Product targets describe learning in terms of artifacts where creation of a product is the focus of the learning target. With product targets, the specifications for quality of the product itself are the focus of teaching and assessment.

**SELECTED RESPONSE.** Selected response is a **poor match** for product targets. We can use it only to determine if students possess the prerequisite knowledge required to create the product, which is not the same as demonstrating the ability to create the product itself. If the learning target specifies that students will write opinion pieces on a topic or text supporting a point of view with reasons and information (CCSSI, 2010a, p. 20 ), no form of selected response assessment will provide accurate evidence.

**WRITTEN RESPONSE.** Written response is a **poor match** for product targets. When the

 *If the target requires creation of a product, only creating the product will give accurate evidence of achievement.* 

learning target specifies the creation of a written product, such as an essay or a research report, the appropriate assessment method is performance assessment. Remember, by definition, *written response* is a short or extended answer to a question or task, and by definition, we limit it to assessing knowledge and reasoning targets. If the learning target states that students will construct scatter plots for bivariant measurement data, providing a written answer to a question does not go far enough to assess the intent of the target.

**PERFORMANCE ASSESSMENT.** Performance assessment is a **strong match** for determining whether students can create a specified product: Assign a task that calls for the creation of the product and then judge its quality using a rubric.

**PERSONAL COMMUNICATION.** Personal communication is a **poor match** for assessing product targets. We can use it only to determine if students possess the prerequisite knowledge required to create the product.

**For Example 4.1**
**TABLE-Examples of Target–Method Match—English Language Arts**
| Learning Target                                                                        | Target Type | Assessment Method(s)                     |
|----------------------------------------------------------------------------------------|-------------|------------------------------------------|
| Understand the meaning of the terms connotation (associations) and denotation          | Knowledge   | Selected Response, Written Response, or  |
| (definitions). (KY 1)                                                                  |             | Personal Communication                   |
| Recognize and correct inappropriate shifts in pronoun number and person.               | Knowledge   | Selected Response, Written Response,     |
| (CCSSI, 2010a, p. 52 )                                                                 |             | Performance Assessment, or Personal      | 
|                                                                                        |             | Communication                            |
| Analyze text to locate figures of speech (e.g., personification) and interpret meanings| Reasoning   | Selected Response, Written Response, or  |
| in context. (KY 2)                                                                     |             | Personal Communication                   |
| Make strategic use of digital media (e.g., textual, graphical, audio, visual, and      | Skill       | Performance Assessment                   |
| interactive elements) in presentations to enhance understanding of findings, reasoning,|             |                                          |
| and evidence and to add interest. (CCSSI, 2010a, p. 50 )                               |             |                                          |
| Tell a story or recount an experience with appropriate facts and relevant, descriptive | Skill       | Performance Assessment or Personal       |
| details, speaking audibly in coherent sentences. (CCSSI, 2010a, p. 23 )                |             | Communication                            |
| Write informative/explanatory texts to examine and convey complex ideas and information| Product     | Performance Assessment                   |
| clearly and accurately through the effective selection, organization, and analysis of  |             |                                          | 
| content. (CCSSI, 2010a, p. 45 )                     

*Sources:* CCSSI, 2010a, pp. 23 , 45 , 50 , & 52KY 1 is from http://www.education.ky.gov/users/otl/KLN/ELA/Language%20 St5%20Grade%207.docKY 2 is from http://www.education.ky.gov/users/otl/KLN/ELA/Language%20St5%20 Grade%208.doc

**For Example 4.2**
**TABLE-Examples of Target–Method Match—Mathematics** 
| Learning Target                   | Target Type | Assessment Method(s)       |
|-----------------------------------|-------------|----------------------------|
| Recognize that analog and         | Knowledge   | Selected Response, Written |
| digital clocks are objects that   |             | Response, Performance      |
| measure time. (KY 3)              |             | Assessment, or Personal    |
|                                   |             | Communication              |
| Distinguish between situations    | Knowledge   | Selected Response, Written |
| that can be modeled with linear   |             | Response, or Personal      |
| functions and with exponential    |             | Communication              |
| functions. (CCSSI, 2010c, p. 70 ) |             |                            |
| Learning Target                   | Target Type | Assessment Method(s)       |
|-----------------------------------|-------------|----------------------------|
| Use ratio and rate reasoning to   | Reasoning   | Selected Response, Written |
| solve real-world and mathematical |             | Response, Performance      |
| problems., e.g., by reasoning     |             | Assessment, or Personal    |
| about tables of equivalent ratios,|             | Communication              |
| tape diagrams, double number line |             |                            |
| diagrams, or equations.           |             |                            |
| (CCSSI, 2010c, p. 42 )            |             |                            |
| Given a two-digit number, mentally| Reasoning   | Personal Communication     |
| find 10 more or 10 less than the  |             |                            |
| number without having to count;   |             |                            |
| explain the reasoning used.       |             |                            |
| (CCSSI, 2010c, p 16)              |             |                            |
| Use a protractor correctly.       | Skill       | Performance Assessment     |
| (KY 3)                            |             |                            |
| Draw (freehand, with ruler and    | Product     | Performance Assessment     |
| protractor, and with technology)  |             |                            |
| geometric shapes with given       |             |                            |
| conditions. Focus on constructing |             |                            |
| triangles from three measures of  |             |                            |
| angles or sides, noticing when the|             |                            |
| conditions determine a unique     |             |                            |
| triangle, more than one triangle, |             |                            |
| or no triangle. (CCSSI, 2010c,    |             |                            |
| p. 50 )                           |             |                            |
| Construct two-way frequency       | Product     | Performance Assessment     |
| tables of data when two categories|             |                            |
| are associated with each object   |             |                            |
| being classified. (CCSSI, 2010c,  |             |                            |
| p. 82 )                           |             |                            |


*Sources:* CCSSI, 2010c, pp. 16 , 42 , 50 , 70 , & 82KY 3 is from http://www.education.ky.gov/KDE/Instructional+Resources/ Curriculum+Documents+and+Resources/Mathematics+DRAFT+Deconstructed+Standards.htm

### **ASSESSMENT DEVELOPMENT CYCLE**

 All assessments, regardless of method selected, need to go through the same development steps to ensure quality. Although the list (also shown in Figure 4.4 ) may look imposing at first, it outlines a commonsense process with several familiar steps. With a bit of practice, the process will become second nature.

**FIGURE 4.4** Assessment Development Cycle

All assessments, regardless of method selected, need to go through the same development steps to ensure quality.
**Planning Stage**

- **1.** Determine who will use the assessment results and how they will use them.
- **2.** Identify the learning targets to be assessed.
- **3.** Select the appropriate assessment method or methods.
- **4.** Determine sample size

**Development Stage**

- **5.** Develop or select items, exercises, tasks, and scoring procedures.
- **6.** Review and critique the overall assessment for quality before use.

**Use Stage**

- **7.** Conduct and score the assessment.
- **8.** Revise as needed for future use.

**Planning**

- **1.** Determine who will use the assessment results and how they will use them.
- **2.** Identify the learning targets to be assessed.
- **3.** Select the appropriate assessment method or methods.
- **4.** Determine sample size.

**Development**

- **5.** Develop or select items, exercises, tasks, and scoring procedures.
- **6.** Review and critique the overall assessment for quality before use.

**Use**

- **7.** Conduct and score the assessment.
- **8.** Revise as needed for future use.

*All assessments, regardless of intended use or method selected, need to go through the same development steps to ensure quality.* 

So far in Chapters 2 through 4 , we have introduced the first three steps in the *Planning* stage: determining the intended users and uses of an assessment, identifying the learning targets to be assessed, and selecting the proper assessment method(s). Now we'll look at how to apply those first three steps when actually developing an assessment. We'll also address the fourth step, determining the appropriate sample size.

In Chapters 5 through 8 , we'll work by method through the steps of selecting or creating items, tasks, and scoring procedures and checking them for adherence to the guidelines to quality. Also in those chapters, we'll offer suggestions for administering the assessment, noting any problems, and revising the assessment as needed.

The Assessment Development Cycle applies to any type of assessment intended for classroom use: formative or summative; practice assessments, quizzes, tests, projects; short-cycle, common, interim, or end-of-course assessments; developed or selected by individual teachers or by grade-level teams, content area departments, or district subject area teams. All need to adhere to standards of quality—all should follow the steps of the assessment development cycle.

#### **Step 1: Determining Users and Uses**

In a balanced classroom assessment system, each assessment represents a part of a long-term assessment map that parallels the curriculum map for the reporting period. Each assessment contributes to an accumulating body of evidence of each student's level of achievement. Some assessments will be used formatively—to guide further instruction and learning—while others will be used summatively to report level of achievement. Our first planning decision is to determine the purpose for each:

- Who will use the information?
- How will they use it?
- Is the use formative or summative?

The answers to these questions guide design decisions. Summative assessments can function formatively and formative assessments can function summatively, but not without carefully thought-out design.

 **My Classroom Then and Now 4.1**
Amy James
I used to . . .

When I began teaching, I would teach a concept over one to two weeks assuming that my students were "getting it" along the way and then would quiz students over that concept. The quiz did very little to inform my instruction, but instead rewarded those students who "got it" and punished those who didn't. Then we moved on and any misunderstandings only accumulated over the course of the unit. At the end of the unit, I was faced with trying to remediate several students over large amounts of material, while other students were ready to move on to the next unit.

Now I . . .

Now I am probing constantly for understanding with both formal and informal styles of formative assessment. I give both verbal and written feedback, as opposed to simply grades, so that both the student and I know what they understand and where they are struggling. I have broken units of study into manageable chunks, or specific learning targets, and assess students in a progressive manner on each learning target. Remediation occurs immediately, allowing students to revisit learning targets that they struggle with. This allows for easy differentiation and grouping, focusing on what each individual student needs help with and allowing those students who have reached understanding to delve deeper into the concepts.

Why I changed . . .

I felt as though I wasn't meeting the needs for all of my students, and there just wasn't enough of "me" to go around.

What I notice as a result . . .

I have found that by structuring instruction and assessment in this way, students take more ownership of their learning. Grades become less punitive and intimidating and instead are more of a gauge of the learning progression. Students who often struggle can more easily see their successes and can manage areas that they need to focus their attention on, which raises their confidence. I find that the classroom environment is more positive and focused. Students help each other and work together. I am more able to confer with individual students and groups, meeting more students' needs. And as a whole, I feel that I am more intentionally working to ensure that *all* students reach proficiency for each learning target in each unit. And isn't that the goal for every teacher?!

*Source*: Used with permission from high school science teacher Amy James, Oldham School District, Crestwood, KY, 2011.

**CONDITIONS FOR FORMATIVE USE.** As we saw in Chapter 2 , if you have determined to use the assessment results formatively, certain conditions must be met. Because some of these conditions are often not met by assessments whose primary purpose is summative, you will need to consider them carefully if you are retrofitting a summative assessment for a formative purpose. The conditions are as follows:

- **1.** The assessment instrument or event is designed so that it aligns directly with the content standards to be learned.
- **2.** All of the instrument or event's items or tasks match what has been or will be taught.
- **3.** The instrument or event provides information of sufficient detail to pinpoint specific problems, such as misunderstandings, so that teachers can make good decisions about what actions to take, and with whom.
- **4.** The results are available in time to take action with the students who generated them.
- **5.** Teachers and students do indeed take action based on the results. (Chappuis, 2009, p. 6 )

#### **Step 2: Specifying the Intended Learning Targets**

List the learning targets to be assessed along with their classification (knowledge, reasoning, skill, or product). If the target is complex or unclear, clarify it or deconstruct it first, following the processes outlined in Chapter 3 . Specifying the intended learning targets is important because the breadth and depth of a learning target will affect how much coverage it will need on the assessment and in instruction. Classifying the targets is important because different target types require different assessment methods.

 **My Classroom Then and Now 4.2**
**Christine Heilman** 
I used to . . .

I used to be unclear on what I was supposed to teach, what kids were supposed to learn, and what I should do with students who didn't demonstrate understanding. I carefully planned what I was teaching, but didn't pay much attention to what students were learning. I used textbook assessments and recorded percentages in my grade book, but did no reteaching. Tests were for grading purposes. There was no structure for collaboration or discussion of results. I taught what I thought was important and then "hoped" students would perform well on standardized tests.

Now I . . .

I now have essential learnings planned out for each content area. I focus instruction on student-friendly learning targets that are clearly posted. I write assessments based on state benchmarks with five to eight questions per learning target. I assess student understanding to fine-tune my instruction for the whole class and for individual students. I plan reteaching, interventions, and enrichment as necessary to ensure all students demonstrate understanding of learning targets. I include student reflection pieces, rubrics for student feedback, and error analysis opportunities. Student homework is aligned to learning targets. I send an essential learnings update to parents each month filled with suggestions for how they can follow up at home.

Our team now plans six-week instructional goals, action steps, and instructional strategies that support each goal. We use a variety of written response assessments, checklists, multiple-choice assessments, and student reflections to gather information about student understanding of targets. We share our goals and instructional plans with our resource support teachers so we are all working together to support goals. At the end of each six-week goal, our team comes together to share data. We display our data graphically, analyze student performance on each target, and make plans for intervention. We discuss instructional strategies that worked well in our individual classrooms so we all have an opportunity to learn from one another.

As a team, we are now very clear in what we are teaching, what we expect students to learn, and how they will demonstrate their learning. We use data to make plans for intervention and ensure remediation opportunities for those who need it. "Hope" is no longer our strategy for test preparation. We have data throughout the year that helps us measure progress on learning targets on an ongoing basis.

*Source* : Used with permission from 2nd-grade classroom teacher Christine Heilman, ISD 196, Rosemount, MN, 2011.

#### **Step 3: Selecting the Appropriate Assessment Method(s)**

 This is fairly straightforward. Once you have classified learning targets by type, decide which assessment method or methods to select by referring to the guidelines described in the section "Matching Assessment Methods to Learning Targets" and summarized in Figure 4.3 .

#### **Step 4: Determining the Appropriate Sample Size**

 Realistically, any test can include only a subset of all the questions we could have asked if testing time were unlimited. It never is unlimited, so we include a sam-

 *Sampling means answering the question, "How much evidence is enough?"* 

ple of the possibilities and then generalize from that sample to student mastery of the domain represented (Stiggins & Chappuis, 2011). Our sampling challenge is to answer two questions: *What will be the scope of coverage of this assessment? What will be the relative importance of the standards or learning targets to be assessed?* When we define the relative importance

of each of the learning targets listed, we are mapping out how we will *sample* student learning.

Sample size is in large part determined by teacher judgment. In all cases, the assessment must include enough questions or tasks to lead us to a confident conclusion about how each student did in mastering each relevant standard or target. So we must decide how much evidence is enough for this target. How many multiple-choice test items, written response items, or performance tasks will we need for each learning target? The guiding principles for sampling are these:

- **1.** The broader in scope the learning target, the larger the sample required to cover it thoroughly.
- **2.** The more important the learning target is as a foundation of later learning—that is, the more confident you want and need to be about student mastery—the larger should be the sample.
- **3.** The more important the decision to be made on the basis of results (formative or summative), the larger and more precise the sample must be.

 Each assessment method brings with it specific rules of evidence for sampling within these guidelines. We will address those in more detail in Chapters 5 through 8 as we discuss assessment development with each method.

 **FAQ 4.3 Sampling**

Question

*How much evidence is enough?* 

Answer

Whenever we build an assessment, we face the practical issue of how many items to include. As a general rule, assuming an assessment built of quality items, the longer it is, the more dependable its results will be. But after that guideline, the matter of test length is a function of the context of that particular assessment. You have to rely on your own professional judgment. The assessor's challenge is to gather enough evidence to lead to a confident conclusion about student achievement without wasting time gathering too much.

How much is enough in any particular classroom situation is a function of several factors:

- **1.** The assessment purpose—the decision to be informed by the results
- **2.** The nature of the learning target to be assessed
- **3.** The assessment method to be used
- **4.** The students to be involved in the assessment

The matter of sampling student achievement in classroom assessment is as much about the art of assessment as it is the science. We suggest guidelines for each of these four factors individually. Just know that in the classroom they play out as a constellation of influences that can interact in complex ways—thus the need for your thoughtful choices.

- **1.** *The Assessment Purpose* With respect to the instructional decision to be made based on assessment results, the more important the decision, the surer you must be about achievement status and, in general, the larger (more dependable) must be your sample. So, for example, when designing a formative assessment, the results of which will guide immediate action, you might be willing to limit sample size. If results lead to an incorrect decision about any student's current level of achievement, that fact will be revealed immediately and corrective action can be taken. On the other hand, in a summative context where, for example, a student's report card grade hangs in the balance, a larger sample might be indicated because it will be more difficult to reverse the decision later if the assessment is inaccurate.
- **2.** *The Nature of the Learning Target* The broader the scope of the target or the greater its complexity, the larger should be the sample. For example, if we want to determine if a student has mastered a key piece of factual or procedural knowledge, we don't have to ask a number of times—once will probably be enough. But if we want to find out if a student is a competent writer, we will need to sample a number of different kinds of writing to evaluate properly.

This sampling challenge is simplified to a certain extent by adopting clear content standards that identify and thus limit the scope of what students are responsible for learning. Teachers used to need to sample broad, vaguely defined domains of achievement such as a semester's worth of history. When content is defined in terms of a more focused set of achievement standards, the sampling challenge is to determine how each student did in mastering each standard.

- **3.** *The Assessment Method* The more information provided by one assessment item, task, or exercise, the fewer items needed to cover the domain. For example, a multiple-choice test item typically provides one specific piece of evidence. Its focus and coverage is narrow. So typically we use several of them to cover the range of content. A written response or performance task, on the other hand, tends to provide relatively more evidence and thus samples broader targets in students' responses. So we typically need fewer of them.
- **4.** *The Students* This factor comes into play more on the formative side than the summative; that is, during the learning. And, in this case, your professional judgment becomes very important. The key variable is where any given student is on the achievement continuum. If the student is clearly a master of the standard or clearly not, then you probably need few items. So, for example, if you ask a couple of questions and a particular student gets them all right or all wrong, then a trend is clear and you can decide based on relatively fewer items. But if the student gets some right and some wrong it may be necessary to keep asking

until that trend becomes apparent. Here is the guideline for thoughtful use: you probably have gathered enough evidence if, based on that evidence, you can guess with a degree of certainty how that student would do if you offered one more chance.

In summary, in high-stakes decision contexts with broader or more complex learning targets using a multiple-choice test, you will want to think about using relatively larger samples of items. But while the learning is under way in the classroom with narrower, more focused targets or when relying on assessment methods that yield more information per item, smaller samples may suffice.

**COMBINING PLANNING DECISIONS INTO A TEST BLUEPRINT.** Creating or selecting an assessment without having a blueprint can result in mismatches between instruction and assessment. Without such a plan, an assessment may not measure

*Creating a good assessment blueprint requires starting with clear targets.*

what you intend it to measure, which is a *validity* problem. From an assessment quality point of view, this is a bad thing. If you yourself have ever taken an exam that did not match what you understood to be the important aspects of the course, you know what this problem feels like to students.

 A *test blueprint* is simply a record of the decisions made in Steps 2, 3, and 4: what learning targets the assessment should

cover, which assessment method or methods to use, and how much weight each learning target will receive in the overall score. Make a blueprint whether you intend to create the assessment from scratch, revise an existing one, or use an already-developed assessment.

- **1.** List the major learning targets that will be the focus of instruction, being clear about target classification (knowledge, reasoning, skill, or product).
- **2.** Write the learning targets into the appropriate spaces in the test blueprint format you select. (Which form you use depends on whether the test will include multiple assessment methods.)
- **3.** If you will be using more than one assessment method, identify the appropriate method for each learning target.
- **4.** Determine the relative importance of each target (weight it will receive) if the assessment is to include more than one.

 Figure 4.5 is an example of a blueprint for a test employing multiple assessment methods: in this case, selected response items, written response items, and a performance task. Figure 4.6 is an example of a blueprint for a test comprised solely of selected response items.

**FIGURE 4.5** 
**4th Grade Unit: The Physics of Sound (Selected Targets)** 
**TABLE-Blueprint for a Unit Test with Multiple Assessment Methods**
| Type of Target    | Assessment Method      | Percent Importance    |
|-------------------|------------------------|-----------------------|
| Knowledge         | Selected Response      | 20                    |
| Knowledge         | Written Response       | 10                    |
| Knowledge         | Written Response       | 20                    |
| Reasoning         | Written Response       | 10                    |
| Reasoning & Skill | Performance Assessment | 40                    |
|                   |                        |                       |

*Source:* Reprinted from the *FOSS* ® *Physics of Sound Teacher Guide* , © The Regents of the University of California, 2005, developed by Lawrence Hall of Science and published by Delta Education, LLC. Reprinted by permission.

**TABLE-Blueprint for a Selected Response Quiz 5th Grade Reading**
| FIGURE 4.6 Blueprint for a Selected Response Quiz 5th Grade Reading                         |          |              |
|---------------------------------------------------------------------------------------------|----------|--------------|
| Learning Target                                                                             | Problems | Total Points |
| Uses prefixes and knowledge of root words to determine meaning of unfamiliar words          | 1–6      | 6            |
| Uses context to determine meaning of unfamiliar words                                       | 7–10     | 4            |
| Summarizes text                                                                             | 11–15    | 5            |

#### **The Steps in Test Development**

So far, our planning has yielded an understanding of intended use, a list of learning targets or important concepts, a determination of appropriate method(s) to use, and an indication of each target's relative importance. At the *Development* stage, we adhere to guidelines for quality specific to each assessment method. This stage is comprised of Steps 5 and 6 of the Assessment Development Cycle.

Step 5 is to develop or select items, exercises, tasks, and scoring instruments adhering to guidelines for quality specific to each method. These are described in detail in each of the methods chapters, Chapters 5 through 8 .

Step 6 is to review and critique the assessment before using it. Regardless of how carefully we plan, things can still go wrong that result in inaccurate measures of achievement. These are called *sources of bias and distortion* . A list of potential sources of bias and distortion common to all assessment methods is shown in Figure 4.7 . Note that some problems, such as unclear targets, inappropriate assessment method, and

**FIGURE 4.7** Potential Sources of Bias and Distortion Common to All Assessment Methods
**Barriers that can occur within the student**

- Language barriers
- Emotional upset
- Poor health
- Physical handicap
- Peer pressure to mislead assessor
- Lack of motivation at time of assessment
- Lack of testwiseness (understanding how to take tests)
- Lack of personal confidence leading to evaluation anxiety

**Barriers that can occur within the assessment context**

- Insufficient time allotted
- Noise distractions
- Poor lighting
- Discomfort
- Lack of rapport with assessor
- Cultural insensitivity in assessor or assessment
- Lack of proper equipment

**Barriers that arise from the assessment itself**

- Directions lacking or vague
- Poorly worded questions
- Misleading layout
- Poor reproduction of test questions
- Missing information

improper sampling, would be solved by adhering to the planning steps of the Assessment Development Cycle. Others are issues that can be hard to anticipate. Sources of bias and distortion specific to each method are discussed fully in Chapters 5 though 8 .

#### **Use and Refinement**

Step 7 is to administer and score the test. Even if you are using an already-developed test, we recommend that you review it with each of the first six steps in mind. When we rely on textbook test writers to do the planning and development, we may have high-quality items, but we may have a partial or poor match to what we taught or to the relative balance of importance of each learning target in our curriculum. In addition, the test may not give students specific and detailed feedback regarding their strengths and areas of need.

Step 8, the last step, is to double check that the test did indeed do what we wanted it to. Were we able to use the results for all the decisions we intended to make? Were students able to use the results to understand their strengths and identify areas needing more work? How about how well it matched the intended learning? Consider asking students to help in this. Were parts of the test a surprise to them? Did it seem out of balance with what they thought it was most important to learn?

Did a source of bias or distortion creep in and affect the results? Were some questions or tasks confusing—students knew the material, but didn't know how to respond? It is almost impossible to eliminate *all* sources of bias and distortion up front. Some only become apparent when you give students the assessment. In any case,

- Do the best you can prior to administering the assessment.
- Watch for possible sources of mismeasurement during and after the assessment.
- If something goes wrong, either (1) don't use the results from the items or tasks in question, or (2) interpret the results with possible bias in mind.

Remember that our goal in the classroom is to get accurate information about student achievement, and if we know the information is not accurate, we have an obligation to discard it and to revise the assessment as needed before future use.

 **My Classroom Then and Now 4.3**
**Ken Mattingly** 
I used to . . .

I always believed I was a good teacher. My instructional activities engaged the students, confronting their preconceptions and misconceptions. Everything focused on the big idea of the unit and getting students to mastery of the standards.

Students were periodically assessed throughout the unit to determine how they were doing. I used the results to make adjustments in my teaching and hopefully fill in any gaps that appeared in student understanding. At the end of the unit students took an assessment that contained a mix of essay and multiple-choice questions.

The end-of-unit assessment was designed to address the big ideas of the unit. The multiple-choice questions had answers that attended to student misconceptions and identified specific problems. The essay questions were written to determine the depth of student understanding of key points. After taking the assessment, students received feedback on their performance in the form of a percentage grade.

Now I . . .

I begin my instructional design process long before I start a unit. I take the standards for the unit and deconstruct them into the knowledge, reasoning, skill, and product targets that make up each standard. I then decide how I will assess each target *during* the unit and at the *end* of the unit. A test plan for the end-of-unit assessment is created, paying attention to match the target to the correct assessment method and determining the proper question sample size.

With my targets developed and unit assessment determined, I now turn my attention to selecting instructional strategies that will enable my students to reach the targets. Whereas before I would pick activities that tied in to the big idea, I now select those that attend to a specific target or group of targets. Any activity, lesson, or strategy that doesn't move students toward mastery of a target is weeded out of my instructional plans.

Throughout the unit, students receive feedback on their performance on targets along with a discussion of how they can close the gap to mastery. Then on the unit assessment student performance is broken out by target so that students can see how they did on each individual target. This diagnosis allows for continued, focused work on gaining target mastery.

Why I changed . . .

I changed because it didn't make sense not to. Years of getting roughly the same results from group after group of students left me searching for a way to do things differently. After being exposed to assessment *for* learning practices and comprehending the classroom responsibility shift that would occur with its implementation, I slowly began to incorporate it into my teaching.

What I notice as a result . . .

Everything in my classroom now is more transparent than before. Students no longer have to guess about what they are supposed to learn. I am clearer on my instructional

goals, and my students and their parents know what they are expected to learn and do. Involving students as partners in their learning, through feedback and selfanalysis, encourages them to continue to try and improve. Student accountability and subject interest has improved, as has overall performance.

*Source* : Used with permission from 7th-grade science teacher Ken Mattingly, Rockcastle County School District, Mt. Vernon, KY, 2011.

### **ASSESSMENT** *FOR* **LEARNING USING ASSESSMENT BLUEPRINTS**

Assessment *for* learning and student involvement activities (formative applications) can spin directly off the assessment blueprint. With the information provided by the blueprint you can do the following:

- Differentiate subsequent instruction after giving a quiz or test, by grouping students according to which learning targets they had trouble with.
- Share the blueprint with students at the outset to make the learning targets clearer.
- Share the blueprint with students and ask them to identify where each day's instruction fits.
- Share the blueprint with students and have them write practice test questions periodically for each cell, as a form of focused review.

 We will offer more ideas for using test blueprints as instructional tools in Chapter 5 and 6 .

### **Summary**

None of the available assessment methods is inherently superior to others. Each brings its own unique strengths and limitations. Selected response, written response, performance assessment, and personal communication are all viable options. To ensure accuracy of results, we first consider the kind of learning target to be assessed and then take into account any special student characteristics such as age, English language proficiency, or specific learning disabilities that might compromise accuracy.

The Assessment Development Cycle proceeds through a series of commonsense stages: Planning, Development,

and Use. There are eight steps in the cycle: (1) identify the purpose, (2) specify the targets, (3) select appropriate methods, (4) decide on relative importance of the targets and sample well, (5) write or select the items, exercises, tasks, and scoring instruments using guidelines for quality, (6) review and critique the assessment for quality, (7) administer and score the assessment; and (8) examine the results and revise as needed.

 Complete these steps and you can have confidence that your formative and summative assessments are yielding accurate and usable results.

### **CHAPTER 4 ACTIVITIES**

End-of-chapter activities are intended to help you master the chapter's learning targets. They are designed to deepen your understanding of the chapter content, provide discussion topics for learning team meetings, and guide implementation of the practices taught in the chapter.

Forms and materials for completing each activity appear in editable Microsoft Word format in the Chapter 4 CD file. Documents on the CD are marked with this symbol:

#### **Chapter 4 Learning Targets**

At the end of this chapter you will know how to do the following:

- **1.** Select the appropriate method(s) to assess specific learning targets.
- **2.** Follow the steps in the Assessment Development Cycle.
- **3.** Create an assessment blueprint.
- **4.** Use an assessment blueprint with students as assessment *for* learning.

Activity 4.1 Keep a Reflective Journal
Activity 4.2 Practice with Target-method Match 
Activity 4.3 Audit an Assessment for Clear Purpose
Activity 4.4 Audit an Assessment for Clear Learning Targets
Activity 4.5 Make a Test Blueprint
Activity 4.6 Try an Assessment *for* Learning Application
Activity 4.7 Reflect on Your Own Learning
Activity 4.8 Select Portfolio Artifacts

#####  **Activity 4.1 Keep a Reflective Journal** 

Keep a record of your thoughts, questions, and any implementation activities you tried while reading Chapter 4 .

![](_page_125_Picture_4.jpeg)

Reflective Journal Form

##### **Activity 4.2 Practice with Target–Method Match**

After reading the section, "Matching Assessment Methods to Learning Targets," work independently, with a partner, or with your learning team to carry out this activity.

- **1.** Select a short unit that you are currently teaching or will teach this year.
- **2.** List the learning targets that will be the focus of the unit.
- **3.** Classify each target as Knowledge, Reasoning, Skill, or Product (KRSP).
- **4.** Using the information from "Matching Assessment Methods to Learning Targets," determine which assessment method to use for each.

![](_page_125_Picture_13.jpeg)

Target–method Match Template

##### **Activity 4.3 Audit an Assessment for Clear Purpose**

After reading the section, "Step 1: Determining Users and Uses," work independently, with a partner, or with your learning team to complete the following activity.

- **1.** Select an assessment to audit for clear purpose.
- **2.** Answer the following questions:
	- Who will use the information?
	- How will they use it?
	- Is the use formative or summative?
- **3.** If any of the answers indicate a need for revision, identify the problem and the revision needed.

![](_page_126_Picture_10.jpeg)

##### **Activity 4.4 Audit an Assessment for Clear Learning Targets**

After reading the section, "Step 2: Specifying the Intended Learning Targets," work independently, with a partner, or with your learning team to carry out the following activity.

First, select an assessment that you have not personally developed. Then follow these steps.

**1.** *Analyze the assessment item by item or task by task.*

Identify and write down what learning each item or task assesses. Describe the learning in whatever terms you want. If two or more items or tasks address the same learning, use the same terms to describe that learning. Note the number of points each item is worth.

- **2.** *Organize the learning targets into a test blueprint*. Transfer the information from step one to a test blueprint chart.
- **3.** *Question the blueprint. Does this match what you taught and what you expected students to learn?*
	- Are some learning targets overrepresented? If so, which one(s)?
	- Are some learning targets underrepresented? If so, which one(s)?
	- Are any important learning targets you taught left out? If so, which one(s)?
	- Do all items on the test align directly with the content standards you have taught?

*Does the sample represent the learning appropriately?*

- Does the number of points for each learning target represent the amount of time you spent on it relative to the whole? If not, which ones are out of balance?
- Does the number of points for each learning target represent its relative importance within the whole? If not, which ones are out of balance?
- **4.** *Adjust the blueprint, as needed.*
	- Add or delete learning targets to reflect what you taught and what you deemed most important to learn and assess.
	- Adjust the number of points each target receives to reflect the amount of time you spent teaching each learning target and each target's relative importance to the content as a whole.

![](_page_127_Picture_19.jpeg)

Audit an Assessment for Clear Learning Targets

##### ** Activity 4.5 Make a Test Blueprint**

After reading the sections titled "Step 3: Selecting the Appropriate Assessment Methods" and "Step 4: Determining the Appropriate Sample Size," work independently, with a partner, or with a team to carry out this activity.

- **1.** Select a short unit that you are currently teaching or will teach this year.
- **2.** List the major learning targets that will be the focus of the unit. Be clear about the classification of each target (knowledge, reasoning, skill, or product).
- **3.** Select or modify one of the test blueprint forms in Figures 4.4and 4.5 . Write your learning targets on the test blueprint.
- **4.** If the learning targets will be assessed with multiple assessment methods, identify which method(s) you will use for each target.
- **5.** Determine the relative importance of each target (the weight it will receive) and add that information to the test blueprint form.
- **6.** If a test for the unit already exists, compare its content to the specifications in your test blueprint. Are there any discrepancies? Describe them.
- **7.** Revise either the test blueprint or the test itself to accurately reflect achievement on the learning targets as needed.

Test Blueprint Form A Test Blueprint Form B

##### **Activity 4.6 Try an Assessment** *for* **Learning Application**

After reading the section, "Assessment *for* Learning Using Assessment Blueprints," work independently, with a partner, or with your learning team to carry out this activity.

- **1.** Select a unit that you are currently teaching or will teach this year.
- **2.** Create a test blueprint for the unit, following the instructions in Activity 4.5 .
- **3.** Choose one or more of the ideas described in the section "Assessment *for* Learning Using Test Blueprints." Try the idea(s) with your students.
- **4.** Briefly describe what you did, either in writing or as a discussion with a partner or your team.
- **5.** Also describe the effect you noticed the activity had on students and their learning.

![](_page_128_Picture_21.jpeg)

Debrief the AFL Application You Tried

##### **Activity 4.7 Reflect on Your Own Learning**

Review the Chapter 4learning targets and select one or more that represented new learning for you or struck you as most significant from this chapter. If you are working individually, write a short reflection that captures your current understanding. If you are working with a partner or a team, either discuss what you have written or use this as a discussion prompt for a team meeting.

![](_page_129_Picture_4.jpeg)

Reflect on Chapter 4 Learning

##### **Activity 4.8 Select Portfolio Artifacts**

Any of the activities from this chapter can be used as portfolio entries. Select any activity you have completed or artifacts you have created that will illustrate your competence at the Chapter 4 learning targets:

- **1.** Know how to select the appropriate method(s) to assess specific learning targets.
- **2.** Know how to follow the steps in the assessment development cycle.
- **3.** Be able to create an assessment blueprint.
- **4.** Know how to use an assessment blueprint with students as assessment *for* learning.

If you are keeping a reflective journal, you may want to include Chapter 4 's entry in your portfolio.

![](_page_129_Picture_14.jpeg)

Chapter 4 Portfolio Entry Cover Sheet

#### **CD RESOURCES**

- 1. Activity 4.1 Reflective Journal Form
- 2. Activity 4.2 Target–Method Match Template
- 3. Activity 4.3 Audit an Assessment for Clear Purpose
- 4. Activity 4.4 Audit an Assessment for Clear Learning Targets
- 5. Activity 4.5 Test Blueprint Form A
- 6. Activity 4.5 Test Blueprint Form B
- 7. Activity 4.6 Debrief the AFL Application You Tried
- 8. Activity 4.7 Reflect on Chapter 4 Learning
- 9. Activity 4.8 Chapter 4 Portfolio Entry Cover Sheet